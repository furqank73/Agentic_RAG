{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbb4651e",
   "metadata": {},
   "source": [
    "### Answer Synthesis from Multiple Sources\n",
    "\n",
    "**What Is It?**\n",
    "Answer synthesis from multiple sources is the process by which an AI agent gathers information from **different retrieval tools, knowledge bases, or documents** and merges it into a **single, coherent, and contextually rich answer**.\n",
    "\n",
    "In **Agentic RAG**, this is a core capability ‚Äî the system doesn‚Äôt just retrieve information. It **plans**, **retrieves**, and then **synthesizes** an answer that draws insights from **multiple sources**.\n",
    "\n",
    "---\n",
    "\n",
    "**Why It‚Äôs Important**\n",
    "Real-world queries are often:\n",
    "\n",
    "* **Multifaceted** ‚Äì requiring multiple types of information\n",
    "* **Ambiguous or incomplete** ‚Äì needing refinement\n",
    "* **Open-ended** ‚Äì not answerable from a single document\n",
    "\n",
    "Relying on a single vector database or source is often insufficient.\n",
    "\n",
    "---\n",
    "\n",
    "**What an Agent Does**\n",
    "A multi-source agent can:\n",
    "\n",
    "* **Plan retrieval** ‚Äì decide what to fetch and from where\n",
    "* **Retrieve content** ‚Äì gather data from multiple tools (e.g., Wikipedia, PDFs, APIs, SQL)\n",
    "* **Evaluate and merge context** ‚Äì consolidate relevant information\n",
    "* **Generate a unified response** ‚Äì produce a single, human-like answer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ce4b8",
   "metadata": {},
   "source": [
    "So in short:\n",
    "\n",
    "Answer Synthesis from Multiple Sources = Combining info from multiple sources into one accurate, clear, and context-rich answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72a3e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader,WebBaseLoader\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders.youtube import YoutubeLoader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6b678c",
   "metadata": {},
   "source": [
    "# making the gemini as llm as to be used as an llm directly which is simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c6cbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Agents represent a significant evolution in AI, combining the powerful language understanding and generation capabilities of large language models (LLMs) with the autonomy and decision-making framework of traditional AI agents.\n",
      "\n",
      "## What are Transformer Agents?\n",
      "\n",
      "At their core, Transformer Agents are **autonomous AI systems driven by one or more large language models (LLMs)**, typically based on the Transformer architecture. Unlike simple LLMs that respond to a single prompt, agents operate in an **iterative loop**:\n",
      "\n",
      "1.  **Perceive:** Observe their environment (read text, interpret code, receive sensor data, get API responses).\n",
      "2.  **Reason/Plan:** Use the LLM's capabilities (understanding, world knowledge, logical inference) to formulate a plan, break down tasks, or decide the next action. This often involves techniques like Chain-of-Thought (CoT) or Tree-of-Thought (ToT).\n",
      "3.  **Act:** Execute an action based on the plan. This could be writing code, calling an external API/tool, generating a natural language instruction, controlling a robot, or searching the internet.\n",
      "4.  **Reflect/Learn:** Observe the outcome of the action, evaluate its success, and potentially update the plan or learn for future decisions.\n",
      "\n",
      "Essentially, they equip LLMs with:\n",
      "*   **Memory:** To recall past interactions and information.\n",
      "*   **Tool Use:** To interact with the external world beyond just text generation (e.g., search engines, calculators, code interpreters, APIs).\n",
      "*   **Planning & Self-Correction:** To pursue long-term goals and recover from errors.\n",
      "\n",
      "## Why are they Significant?\n",
      "\n",
      "1.  **Generalization:** LLMs bring vast pre-trained knowledge, allowing agents to tackle a wide range of tasks with little to no specific training (zero-shot/few-shot learning).\n",
      "2.  **Natural Language Interface:** Users can instruct and interact with agents using natural language, making them more accessible.\n",
      "3.  **Complex Reasoning:** LLMs excel at understanding context, making inferences, and generating coherent plans, which is crucial for tackling multi-step problems.\n",
      "4.  **Adaptability:** They can adapt their behavior based on new information or environmental changes.\n",
      "5.  **Human-like Explanation:** They can often explain their reasoning and actions in a human-understandable way.\n",
      "\n",
      "## Recent Evolution in AI Research\n",
      "\n",
      "The concept of agents has been around for decades in AI (e.g., in robotics, game AI). The \"recent evolution\" of Transformer Agents specifically refers to their emergence and rapid advancement *since the widespread adoption of powerful LLMs (like GPT-3, GPT-4, Llama)* starting around late 2022/early 2023.\n",
      "\n",
      "Here are the key aspects of their recent evolution:\n",
      "\n",
      "1.  **Reasoning & Planning Enhancements (CoT, ToT, Self-Reflection):**\n",
      "    *   **Chain-of-Thought (CoT):** Early breakthroughs showed that simply prompting LLMs to \"think step-by-step\" significantly improved their reasoning on complex tasks.\n",
      "    *   **Tree-of-Thought (ToT):** Extends CoT by exploring multiple reasoning paths, allowing the agent to backtrack and refine its plan if a path proves unproductive, mimicking human-like problem-solving.\n",
      "    *   **Self-Reflection/Self-Correction:** Agents are increasingly designed to evaluate their own outputs, identify errors, and iteratively improve their responses or plans. Projects like AutoGPT and BabyAGI, while often simple, popularized this idea by demonstrating autonomous goal pursuit and self-correction.\n",
      "\n",
      "2.  **Advanced Tool Use & API Integration (ReAct, Function Calling):**\n",
      "    *   **ReAct (Reasoning and Acting):** A seminal paper that showed LLMs could interleave natural language *reasoning* with specific *actions* (like using a search engine or calculator) to solve complex problems. This was a blueprint for many subsequent agentic systems.\n",
      "    *   **Standardized Function Calling:** OpenAI's \"Function Calling\" (and similar capabilities in other LLMs) made it much easier and more reliable for LLMs to generate structured calls to external tools and APIs, effectively transforming them into controllers for vast ecosystems of software.\n",
      "    *   **Tool Libraries & Frameworks:** Projects like LangChain, LlamaIndex, and Marvin emerged to simplify the creation of agents by providing modular components for memory, tool integration, planning, and orchestration. These frameworks allow agents to use web search, code interpreters, databases, image generation APIs, and more.\n",
      "\n",
      "3.  **Long-Term Memory & Context Management (RAG, Vector Databases):**\n",
      "    *   **Retrieval Augmented Generation (RAG):** To overcome the limited context window of LLMs and provide agents with up-to-date or domain-specific knowledge, RAG systems integrate external knowledge bases (often vector databases). Agents can query these bases to retrieve relevant information before generating a response or making a decision. This allows agents to \"remember\" vast amounts of information.\n",
      "    *   **Episodic Memory:** Some agents are being developed with more sophisticated memory systems that can store past experiences, observations, and decisions, allowing them to learn and adapt over longer periods.\n",
      "\n",
      "4.  **Multi-Agent Systems & Collaboration:**\n",
      "    *   Research has moved towards creating systems where multiple LLM agents collaborate to solve complex problems, each potentially specializing in a different role (e.g., a \"planner\" agent, a \"coder\" agent, a \"debugger\" agent, a \"critic\" agent).\n",
      "    *   This mimics human team dynamics and has shown promise in tasks like software development, scientific discovery, and creative writing.\n",
      "\n",
      "5.  **Embodied Agents & Robotics:**\n",
      "    *   Bridging the gap between language and the physical world. LLMs are increasingly used to translate high-level natural language instructions into low-level robot actions.\n",
      "    *   Projects like SayCan (Google) and PaLM-E showed how LLMs could ground language in real-world actions for robotic manipulation, allowing robots to understand and execute complex, multi-step commands.\n",
      "    *   Simulated environments (e.g., Minecraft, robotic simulators) are crucial for training and testing these embodied agents without real-world constraints.\n",
      "\n",
      "6.  **Autonomous Code Generation & Development:**\n",
      "    *   Agents are becoming adept at not just writing code but also planning entire software projects, writing unit tests, debugging, and iteratively improving codebases. This pushes the boundaries of AI-assisted software engineering.\n",
      "\n",
      "## Challenges and Future Directions\n",
      "\n",
      "Despite rapid progress, Transformer Agents face significant challenges:\n",
      "\n",
      "*   **Reliability & Hallucinations:** LLMs can still \"hallucinate\" incorrect information or generate nonsensical plans, which can be critical for autonomous agents.\n",
      "*   **Cost & Efficiency:** Running complex agentic loops with powerful LLMs can be computationally expensive and slow.\n",
      "*   **Safety & Alignment:** Ensuring agents operate within ethical boundaries and don't pursue unintended or harmful goals is paramount.\n",
      "*   **Long-Term Memory & Forgetting:** Maintaining consistent, relevant memory over extended periods without \"forgetting\" crucial details remains a challenge.\n",
      "*   **Scalability:** While good at individual tasks, building truly robust, general-purpose agents that can handle highly open-ended, long-horizon tasks is difficult.\n",
      "*   **Interpretability:** Understanding the internal reasoning process of an LLM-driven agent can be opaque, making debugging and trust difficult.\n",
      "*   **Proactive Learning:** Moving beyond reactive task completion to agents that can proactively explore, discover, and set their own goals.\n",
      "\n",
      "The recent evolution of Transformer Agents marks a shift from LLMs as mere text interfaces to powerful, autonomous decision-makers capable of interacting with and shaping the real world. This area continues to be one of the most dynamic and impactful fields in AI research.\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# --- IMPORTANT ---\n",
    "# Make sure you configure your API key *before* running\n",
    "genai.configure(api_key=\"your_api_key\") \n",
    "\n",
    "class GeminiLLM:\n",
    "    def __init__(self, model: str):\n",
    "        # 1. Initialize the GenerativeModel object\n",
    "        try:\n",
    "            self.model = genai.GenerativeModel(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model: {e}\")\n",
    "            print(\"Please ensure your API key is set and the model name is correct.\")\n",
    "            raise\n",
    "\n",
    "    def invoke(self, prompt: str):\n",
    "        # 2. Use the .generate_content() method\n",
    "        response = self.model.generate_content(prompt)\n",
    "        \n",
    "        # 3. Access the text using response.text\n",
    "        \n",
    "        # Mimic LangChain's .content\n",
    "        class Result:\n",
    "            def __init__(self, content):\n",
    "                self.content = content\n",
    "        \n",
    "        # 4. Pass the text content to your Result class\n",
    "        return Result(content=response.text)\n",
    "\n",
    "# --- Usage ---\n",
    "\n",
    "# Note: \"gemini-2.5\" might not be a valid model name.\n",
    "# Using \"gemini-1.5-flash\" as a common, valid example.\n",
    "try:\n",
    "    # Try this model name if 'gemini-1.5-flash' still fails after upgrading\n",
    "    llm = GeminiLLM(model=\"gemini-2.5-flash\")\n",
    "    prompt = \"Explain transformer agents and their recent evolution in AI research.\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    print(answer)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during invocation: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "284b3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data_science_interview_cleaned.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f79ff",
   "metadata": {},
   "source": [
    "# Defines the agent's tools. These are four functions for retrieving information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b80f0b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load text retriever \n",
    "def load_text_retriever(file_path):\n",
    "    docs = TextLoader(file_path, encoding=\"utf-8\").load()\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    vs = FAISS.from_documents(chunks, embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "    return vs.as_retriever()\n",
    "\n",
    "# 2. Load youtube retriever\n",
    "def load_youtube_retriever():\n",
    "    # Mocked YouTube transcript text\n",
    "    content = \"\"\"\n",
    "    This video explains \n",
    "    how agentic AI systems rely on feedback loops, memory, and tool use.\n",
    "    It compares them to traditional pipeline-based LLMs. \n",
    "    Temporal reasoning and autonomous tasking are emphasized.\n",
    "    \"\"\"\n",
    "    doc = Document(page_content=content, metadata={\"source\": \"youtube\"})\n",
    "    vectorstore = FAISS.from_documents([doc], embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\"))\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# 3. Load Wikipedia retriever\n",
    "def wikipedia_search(query: str) -> str:\n",
    "    print(\"üåê Searching Wikipedia...\")\n",
    "    return WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper()).invoke(query)\n",
    "\n",
    "# 4. Load Arxiv retriever\n",
    "def arxiv_search(query: str) -> str:\n",
    "    print(\"üìÑ Searching ArXiv...\")\n",
    "    results = ArxivLoader(query).load()\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in results[:2]) or \"No relevant papers found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9298e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_retriever = load_text_retriever(\"data_science_interview_cleaned.txt\")\n",
    "youtube_retriever = load_youtube_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7aea0c",
   "metadata": {},
   "source": [
    "# State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bf9ddb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydantic model It ensures each field has the correct type and allows automatic data validation.\n",
    "# For example, if someone passes an integer instead of a string for question, Pydantic will raise an error.\n",
    "class MultiSourceRAGState(BaseModel):\n",
    "    question: str\n",
    "    text_docs: List[Document] = [] # text_docs should be a list of Document objects, and by default, it starts as an empty list (= []).\n",
    "    yt_docs: List[Document] = []\n",
    "    wiki_context: str = \"\"\n",
    "    arxiv_context: str = \"\"\n",
    "    final_answer: str = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f6f98",
   "metadata": {},
   "source": [
    "# ‚ÄúRetrieval Nodes‚Äù\n",
    "\n",
    "These functions are **independent modules (nodes)** that:\n",
    "\n",
    "* Take in the current **state** (the `MultiSourceRAGState` object)\n",
    "* Retrieve data from a specific **information source** (like text files, YouTube, Wikipedia, or ArXiv)\n",
    "* Then return an **updated copy** of the state that includes the new retrieved data\n",
    "\n",
    "This structure allows **LangGraph** or other workflow frameworks to connect them in a graph sequence ‚Äî each node modifies the shared ‚Äústate‚Äù object and passes it on.\n",
    "\n",
    "\n",
    "## `state.model_copy(update={...})` ‚Äî Why Used?\n",
    "\n",
    "Instead of modifying `state` directly, this method:\n",
    "\n",
    "* Creates a **new immutable copy** of the Pydantic model,\n",
    "* Applies the update cleanly.\n",
    "\n",
    "This keeps your data flow **functional and safe**, avoiding shared-state mutation bugs.\n",
    "Think of it as:\n",
    "\n",
    "> ‚ÄúCopy the state, update just this one field, and pass the new state forward.‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## **How They Fit in the Workflow**\n",
    "\n",
    "\n",
    "```\n",
    "User Question\n",
    "     ‚îÇ\n",
    "     ‚ñº\n",
    "[retrieve_text] ‚îÄ‚Üí Adds text_docs\n",
    "[retrieve_yt]   ‚îÄ‚Üí Adds yt_docs\n",
    "[retrieve_wikipedia] ‚îÄ‚Üí Adds wiki_context\n",
    "[retrieve_arxiv] ‚îÄ‚Üí Adds arxiv_context\n",
    "     ‚îÇ\n",
    "     ‚ñº\n",
    "   Synthesizer ‚Üí Combines all info into final_answer\n",
    "```\n",
    "\n",
    "Each node updates a different part of the `MultiSourceRAGState`, and by the end of all retrievals, the state is **fully enriched** with diverse context ready for synthesis.\n",
    "\n",
    "---\n",
    "### In short:\n",
    "\n",
    "Each function is a **retrieval node** that:\n",
    "\n",
    "* takes the shared `state`,\n",
    "* retrieves info from a unique source,\n",
    "* updates that part of the state,\n",
    "* and passes it forward ‚Äî ready for **multi-source answer synthesis** later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d6782a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieval Nodes\n",
    "def retrieve_text(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    docs = text_retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"text_docs\": docs})\n",
    "\n",
    "def retrieve_yt(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    docs = youtube_retriever.invoke(state.question)\n",
    "    return state.model_copy(update={\"yt_docs\": docs})\n",
    "\n",
    "def retrieve_wikipedia(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    result = wikipedia_search(state.question)\n",
    "    return state.model_copy(update={\"wiki_context\": result})\n",
    "\n",
    "def retrieve_arxiv(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    result = arxiv_search(state.question)\n",
    "    return state.model_copy(update={\"arxiv_context\": result})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65464ee3",
   "metadata": {},
   "source": [
    "### Combine Context from Multiple Sources\n",
    "- This section gathers and merges information from all retrieved sources\n",
    "- (internal documents, YouTube transcripts, Wikipedia, and ArXiv).\n",
    "- It structures them into labeled sections, making it easier for the LLM\n",
    "- to understand which content came from where.\n",
    "### Process Internal Documents\n",
    "- For each document in state.text_docs:\n",
    "-   Extract its main text using `doc.page_content`\n",
    "-   Join all document texts together into one string\n",
    "-   Add a clear header [Internal Docs] to indicate the source section\n",
    "### Model Response Handling\n",
    "- After sending the combined context and question to the LLM:\n",
    "-   `.content` extracts the textual output (model‚Äôs generated answer)\n",
    "-   `.strip()` cleans up any leading or trailing spaces or line breaks\n",
    "### Update State\n",
    "- Create a new state object identical to the previous one,\n",
    "- but with the `final_answer` field updated to include\n",
    "- the synthesized response generated by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "369df84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## synthesize\n",
    "def synthesize_answer(state: MultiSourceRAGState) -> MultiSourceRAGState:\n",
    "    \n",
    "    context = \"\" \n",
    "    context += \"\\n\\n[Internal Docs]\\n\" + \"\\n\".join([doc.page_content for doc in state.text_docs]) \n",
    "    context += \"\\n\\n[YouTube Transcript]\\n\" + \"\\n\".join([doc.page_content for doc in state.yt_docs])\n",
    "    context += \"\\n\\n[Wikipedia]\\n\" + state.wiki_context\n",
    "    context += \"\\n\\n[ArXiv]\\n\" + state.arxiv_context\n",
    "\n",
    "    prompt = f\"\"\"You have retrieved relevant context from multiple sources. Now synthesize a complete and coherent answer.\n",
    "\n",
    "Question: {state.question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "\n",
    "    answer = llm.invoke(prompt).content.strip() \n",
    "    return state.model_copy(update={\"final_answer\": answer}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b25544a",
   "metadata": {},
   "source": [
    "# StateGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f516ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAI4AAAJ2CAIAAAAGwS0CAAAQAElEQVR4nOydB1wUR/vHZ6/Qm4J0EcWKDRSUqH9EEWvsvcUaY9co9hbbm2iM+sYSY9T4WogxmqhJjL2isWDDrohdQAWkc233/9wtHAdcXW4Pdp3vB8+92Zm52f3tzDw7O/uMiKIohOECIoThCFgqzoCl4gxYKs6ApeIMWCrOUD5S3buS8exuTma6XJZPkQrl3QIhICiSEggIkqRgG0Io1Yb6E0JgL6IIkiI1Q5TxCUIgJBRyks68MBBRyj1FNyOqcGXGyp9T7i0Kh68KRbGbFiFkWDxEJCbgz7GSuFoD2/rNXJDFISx5XxV76N3ja1m52SScGpE1IRYRAjGcP6UwhAC0QYSQoBSgjTKy8qug6FMVB2JSJUJUWiKlVLKCA6H3wn5CJQkqPD51Kqp4OEUgoQCRimJFFQhLhgjFlFRCKeSUNJ+EtDaOglpB9v/XwwNZCgtJdWZ/yoMrWXD1elSzbtbR1bu6LeIySc/yrhxNTXkmgWpap6lDm36WEMwSUm1bmCiXUUERLs06uCJ+EXfi/fWTGUIxMWpJDcQy7EqV/Dxn//dJfnVtu37ug/jL4Z/fPL2T2328l2+APWINFqXKz1Nsmfu012Rv7+p2iO+kvMr77bvXo5ZWt3UQInZgS6pXD7MP/ZQ8flVN9DGxMTqh8wh3//pOiAUEiB0O/pg8eE5V9JExfIHf39veInZgRaot8xNrNLJzdrVGHxl2zla1mtjD4SMWML9U/2xPUijITsO90UdJ+8FecNf1z89JyNyYX6rE2zn/190NfcT8Xx/XxDs5yNyYWapjMckiEQoMK4dxl4pDnWBnkRVxdKeZK5aZpXp2O7taA/6b5gap0dDu+b1cZFbMLJU0H3UcauleKioq6vXr18hEnjx58umnnyJ2iBrkBQOGMokMmQ9zShV7IEUkRhYmKSkpPT0dmc69e/cQm4itiAt/pSHzYU6pkp9LbezZuleHW/WYmJhBgwa1bNlyyJAh69evVygUcXFxXbt2hb3du3efPn06UtWVFStW9OnTp0WLFhBt3759dPKEhISQkJDY2NiOHTsOHDhw06ZNixcvTk5OhsDdu3cjFoBhi3cvpch8mPN5VW6Wwt6JLan27Nmzbdu2qVOnglRnzpzZsGGDvb39iBEj1q5dC4EHDx708VEOM3733Xdv3ryZN28ejOI/e/YMZPPy8oIkYrGyvm/ZsmXo0KFBQUH169eXSqXHjh3766+/EDvYOQpysuTIfJhTKniWI7JhS6rr168HBgbSvUvPnj1DQ0Nzc7X0219//XVOTo63t7K/hBpz6NChixcvglTKp4cIhYWFDR48GFkEsY1QkU4i82Hep8CEELE1+Nu4ceN169YtWbIkODg4PDzc19dXazRoJ6H+Xbhw4fnz53QIXdto6tWrhywFPO9EpDnPhjmlIoSURGLO60gT6KWgxTt79iz0MSKRCKy+yZMnV6lSRTMOSZJTpkyBlm3ixIlQpRwdHUeNGqUZwdracmNdknw5nBBkPswplZ29MC9bgdhBIBD0VJGYmHjlypXNmzdnZ2evWbNGM86DBw/u3r27cePGZs2a0SFZWVnu7u6oPIBTYWtWI8ucFqCbr3VeFlu1Cvp/sO5go0aNGgMGDAAr7uHDhyXifPjwAT7V2iSqQOVEbpbczccKmQ9zStWsfSWZlK2+6siRIzNmzDh37lxGRgbY3KdOnYLeC8L9/f3h8/jx43fu3AEVoW3cuXNnZmYmmH/ffvst2BFw46U1Qz8/v/fv34Mxqe7VzItMgoLaOCPzYU6pHFyshGJ0dj8rD2zmz58PSkybNi0yMnLp0qWtW7cGixzCwb6AWyu4TwKjw9PTc9myZbdv327btu2XX345YcIEuMECCeGzdIatWrUCqz06Ovro0aPI3Jw/8A4RyM3TnGNsZn4KfGDjq7cvJWO+DkAfNz/NTXT1tuo10ReZDzOPAfYY7yvNp5Ke5qGPmPdvJJI80rw6ITZm13r4W/29LWn0Uu2TrZ4+fQpDDFp3FZsKW5wePXrAkARiB8j55s2bWnc5OztD16h116xZszp16qR114GNL6tUNf9gKCvTYDbOSAiJqtSsvZZZfzBwp3WUAcjLy7O11T6VE4aFbGxsEDtAeaBUWnfJZDJ6RKo0UB6tu66dTLt0OG3Cd+af/8PKnPXuY30ObHytVSqhUAh3plpT6QpnGzs7c3b+lw+ndf3CC7EAK9NgfAJsG7Z03Dw7AX1kbJ77pN4njn61WZm4yeKUzRcPc/78MWnC6o9lKuD66QmfjvLyD2Rrgi27E6FjD76/dfbDJ90qN4mojPjLzbPvLxz6AA1JeC8W3zNg/fWCV09y//zxDTzH6jnex7GyOQdaKgI5GdL9617nZCg6j/KsVtcBsYmFXtrZ998XKS+k9s7CwOaOzTrwYerZleOp9//NzPqgcPcV9Zvmj9jHoq/C7V/38t0rKamgrGwJO0eRjZ3A2l5EP/QrVa6CV9XUrxeqbrqUL8PBMyDNdw7VcYpi0t+Kx6GhkxcloT80X5dTBamjaUIpKEmePDdHkZdJSvNJoVA5PN1nsuUme1tUKppXT3LuXsh4/0aan0PCg2OFkQ+1VSe0UJVixS4ZCCdaAadSUPrIivQrkEdDpWLRtJwW0E8gRvBcw83XqmELF5+alp5DVw5SWQB4rhgXF4f4BQ/fsJfL5XCjjXgHP6WCp1aId2CpOAMPD0nPGCunwbWKM2CpOAOWijPgvooz4FrFGbBUnAFLxRmwVJwBmxWcAdcqzoCl4gxYKs6ApeIMWCrOgKXiDFgqzoCl4gz4Fpgz4FrFGXh4SFCl7O1Z9HdeXvBQKoVCkZWVhXgHHxsKkQjaQMQ7sFScgYdSCYVCXa9hcxpcqzgDloozYKk4A5aKM2CpOAOWijNgqTgDP6XC91XcAG6Bca3iBrgB5AxYKs6ApeIMfJWKP95gJk+efP78eYFAoD4igiBIkrxx4wbiBWytC2x5Jk2a5OXlBfIICgHNGjZsiPgCf6SqVatWWFiYZoijo2Pfvn0RX+CPVMCwYcM0F0uCSkavGccPeCVVtWrVWrVqRW+DcdGrVy/EI3glFTBkyBC6YsFnjx49EI8wzQJ88Sjn8fUsSX5RiIBQek1U5yFAFKm0vFQuMQmVW8RCr5ZKKIIqdJUoECJKDgGF5SjcoKjiIVSRb0U6Q1WJiaIQCgkEiNRYiykxMSEx8WnNgJr+1atrFh6iUYqiX9TMU+tadoWeODUKoDo8DSiV80dU2qMnXSStnj5prKxRzSAH/3omuLs1QaqtC59IcimxNSGTFCsTZFCUh4BCJFFQUAGUlKRU54YQINqnqdrRqFBEKBSUpidS+qwUSSVQJlQWTzOEKghTp1KeTbheNByYwlew0ZUWYHGvpgJ6Qb3ih6ssGK0/WdIvK52tpsdNTakIougyKiVhYdriBdNEbEXJZMjalhi1xNilboyVatPshCq+4vZDqyGM+Tge8/ztM9nYFTWNiWyUVD/NTfCrZ9uimw/CmJvLR5Of3Mj+4mvDahk2K2L/TIHWDOvEEs07eEJlOXcw2WBMw2OArx7l2znxcKiw4uDgIn79yPC63IZrlTS3yOLCsIQ03/Bja8PVBew0gq1lSTFKSAVByg3XGdyylT+UCoPRDEsFNxDa14LAmAkB3H4JzCGVSiuEYQ/lfb8RZxg3gOUPoRyaM6yVYamUQyO4VrEJnGCSNFNfhaViFZU1YDiaEbUKVyqWIehHDobAfVVFAIbmzdFXYdim2FMk3RiWSiAk8H0Vq1CIMo9UhDHtKKZMaF9HsASGh54UYEiS5SNW956RO3ZuQXyHUD2hNojhKMoGUMBWA/j06ZMBgz7Vtbd/v6GNGgYjC6K/PMbQs3fUm6TXJiWBBpA0YkDccANIKij2uqqHj+7p2Tto4HBkWfSXxyDJyUkfPqQjEyFU84YMRjNcqwjTrQpouPbv/2XKl5+3iQzJzMqEkCNH/xw/cXinLq3gc9/+GLob/Xn7phUrF6ekJEO03/btTkxMgI1Ll2L79Os4esxAVLwBvHs3fuasid26txk6rNfGH9bk5ORA4NW4S5Dkzp1b6p++/+CuMpPLF3Ql0UOJ8kBIWlrqsuXzoJ716NVu+dcLXr58jlRO7CDDhYtmqBNOjx4HBY67dnngYOUM0cFDui9YGI2MxriBdSOkEokJgci06YJisfivw3/UrFnn25Ub7GztTpw8Aqegdq26MbsOjR41AaRav/E7iDZi+NgB/T/z8PA8fTKub5/BtL/FHbu2QLs3fdp8zQxfvX4ZPXN8viR//bqfly5elZj4+MtpY+CUNQkOdXRwPHf+lDpmbOxpCAkNCdOVRE+xS5RHoVB8Of2Lm7eufTl17rYtv1ZyqTx+wrDXb16JRKLZM786H3satIFUZ8+djL99Y/7c5SFNm3+9fC2E7N51cOmSVchohAKBwIgVVw1rIJOSCplpzxahGjo5OU+aEA2lhwM7fPhAo0bBU6fMrlSpMpzcEcPGHjiwNz09rXQq+ISzDKepXt36mrtOnPhHLBLDGffz8/f3rxE9fcHjhIexF84IhcI2bdqfO39SHRNki4zsCOG6kiCjuX375osXz+bOWdq8WYvKlV3HjZ3q5Oyyf38M7Kpfv1H3bn3WrPlPbm7uxh9Wg8bwE4gpYLiRRry7zNbs2jq1A+kNKMadu7dCQz5R7woODoVAuBK1Jqxdq17pwLt3b9WtW9/Z2YX+6unp5e3tS+cQEREFTdajxw+Qyih49epFZNuO+pMYye07N6Giw7VFf4UrKahx01vx1+mvYz6fLJFKxo4f6ubmDnURlQHlOt5G6GDMcC2TW2ArKyt6QyqVymSyrds2wp9mhNK1qiChtXXpwOzsrAcP70EXUiyHtFT4hNMHlfXcuZPQwEKjVKWKe4MGjfUnMRLIAUpeIgcXl0r0hp2dXY/u/eCgoEoJBGW64lUj64ajGTNcW6abKhsbGziq9lFdwsMjNcO9vXyNz6Syq1vDhkFwUjQDnZ2UNQYuI2gDoWWDXhA6qqh2nQ0mMRJXVzdbW9vly9ZoBgoLe5WMjA9/HPi1TUTUL3u2R0V19vL0RixjWCqRWFDGG+CAgNpZ2VnBQQWXJ1yqSUmv3d09TMihRq1jx/9u3KiJ+vp99izR19eP3m4b0f733/eA6Qi9EXQtxiQxsth5eXnu7p4+3gVXFdwwuTgX1Kr1G1ZV86u+cMHXEyePXL16ORhQiCnwwB6JzGGsy2UkKS+TWJ+PmnjhwpnD/xyELgr66iVL50yLHgsNI1K+r+GXmvo+NvYMbQfrok+fwZAW7Mb8/HyI+ePm70eO7p/4NIHeC508CA+mdo0aNdXdu/4kutAsT9MmzZo1a7Fq1VLoC6EOHTj429hxQ48cOQTR4LIAw2/6dKWZOjN6IViJR4/+BdtV/fzh88yZ4/fu30FGA8NBCiPeXTbmvqqsjxahIdq8aXd8/A24kwcD/rzBKwAAEABJREFUOicne9nS1daqPimseauGDYIWLIo+eeqonhycHJ22bvnV1sb2i3FDPhveG07NjOgF0DmpI0S0jgLLom2bDsYn0UqJ8oDx3bp1uyXL5sB91e9/7GnXrlOvXgOys7NXfLt44IBhdG0DC7N3r4EbN60BOSGkY4eucNFs2bIemRvDc9b/t+QZSNVrsj/CsMOB9c8VMmr4V/76oxkxsKR6cQVhWMNsFqCRT/65QtduEbp2zZr1VauWEcjyUIgwywN7Qv3BCzZvjtG1C4aOUHmgnLJJmGVyWeFLmPzAAjdApkIhMz2wV+mN+yoWMXJk3UizAmHYg0Dm6qsEeNI6u6gaQHP0VUqd8EQYNoG6YJ43QWg/BgjDGiRlprkVCjmJb4FZxWzvV+G+im2Us/fMM1qBEO6rKgJ4DJAzGJbKyhbaPyMm1GCYIrYihGaZsWTvJJTkGHaAgWFMfo7M1tFwNMNStennlpeD+yoWyc+hIvq6G4xmWCpnV1tPf6vdXycgDAvs/ibBvaqVq4etwZjGOpn79/C7W2czvALsfGrZ2thYaY1DFQxnlRwnpqiCOdlU8XtpSulHsdAJY7HXVrS4RCxwBlkYszCrElkqvwtUUUvmSVGaj92U+6mSv65ZHm2JCkMEBfkWHG+pV3DpfAp+nSh8f6q4bZYvkb1+mPMmMa9RK+eWXasgIzDBdePVY+9uX8iW5CkUMr05lnYtqcvbJKV9GIQq4+iItvRaTrq5xmBKZWRMzgIhsrEX1Auz/6STsTO3+OMSX5OQkJC4uDjEL3j4LrBcLhcKeXh3wU+pRCI+rnWCeAeWijNgqTgDloozYKk4A5aKM8hkMvq1Yp6BaxVnwFJxBiwVZ8BScQZsVnAGXKs4A5aKM2CpOAPuqzgDrlWcAUvFGbBUnAFLxRmwVJwBS8UZeHhINjY2ZfSlWDHhoVT5+fkZGRmId/CxoRCJ9Ht+5ihYKs6ApeIMWCrOgKXiDFgqzoCl4gxYKs7AQ6mEQqFCYcRyKFwD1yrOgKXiDFgqzoCl4gxYKs6ALUDOgGsVZ+CPN5hBgwY9fPiw9OFcv34d8QL+PNieOnWqm5uboDjVq1dHfIE/UjVr1iwwMFAzBDqtrl27Ir7Aq+kiI0aMqFSpkvpr1apVe/fujfgCr6Rq1KhRkyZN6G2CICIiIhwdjfA0yhH4Nglr9OjR7u5K56I+Pj79+/dHPMIMxjrcxDy7natnwegib5eEygGlrr2MKJFcgLxbBfeKi7vesknLzCS7jKTsIoeXOlIVOsvUUoxCT5laiq0JpVx9QKcveoWC9K9vrV7TmjFlMtaledKd37zKyyaFQqTf9SaN2RxbauRlQGkdP1na76cuT6B6MjESoVh5cmwdBf1nejk4GPZRqwvmUknzFJvnP61W1y6iX4VbZ60Ccnrvmxf3c0cvr25jy9CtJHOpNk5P6DbFx9mZ+WXysZGXJ9278sXE1TURIxiaFXtWPXNyE2GdTMLW1srFw+qXlc8RIxhKlZEq961rhzAmUq2ebUaqEb26NhhagAo5sne2RhgTcapsTSkYmigMpaLkiFCYzZr7eCBJQq4wYq0qbfDwIUiFhmJu+DOtVQivv8gEqgwnjaFUPFvY3mKUZXk9pg0gDLUwbHI/aghCwPgKx32VZSlDtSpDX4UxHYpkPuTKtK9CuK9igmodL8tagAghPi6mxDqqNdAYnjjcV1keLBUnEAgIi98CU3gJbiaQJMW0VjEcWVfdAZeDXZGYmNAmMiQ+/gbiMAzPG/NpMOyZFX8c2Pv1ikVad7m4VPps6Gh3d09UAdBTTl0UrsHKhIrYVz18eE/XrsqVXUcMH4sqBnrKqQuqYM1dJlhIKmi4Rn0+4Ovla1etXgY1Y8vmX+Ry+dZtGy9djn37NrlBg6Ce3fuFhbWCmFOnjbl1SznL/Nixv3/ctGv37m1CodDDw2vPrzsWf7XS18cP8vnvmp8aNQqGOEeO/nnoz/1PnyZUr16zbZv2vXsNJAhi0pRRtja2K1esV//6nHlTMzI+bFy/XdeP6mHKl59bW1lr5rZgYXRq2nsrKyt1OWN2H/LyNGqCiYBgfjvKsAGkCNPqMe2ieceuLf37DZ0+bT5sf79u5b79MT179I/Z/Wfr8MhFi2eePXcSwteu3lyvXoP27bucPhlXu1ZdSJj4NAH+li9d3ahhsGaeJ04eWbFyMcSJ2XVo9KgJkNv6jd9BeJvWUdeuX8nJyaGj5efnx8Vdate2o54f1UPnjt0ht7S0VHVuoHT7qC6a5TRSJ1S2XoOpWWHiT9IXU2hIWN8+g+vVrS+RSI4e+2vQwOHduvZ2dnLu3Kl7ZNuOO3b+pDVhcvKbxYtWtmgRDtVRc9fhwwegbk2dMrtSpcpNgkNHDBt74MDe9PS01q3bkSR5PvYUHS32whn4GhERZfyPatKmTXs7O7tTp4+qc4PPtm07IEZQZRCrDLNrTf/N2rXq0RuPHt2XSqWhIZ+odwU1bgqNZEamFj9+1fyq29jYlAiEs3/n7i3NHIKDQyEw/vYNV1c3yO187Gk6/MKFM02bNINOzqQfVQMNXbvITidO/EN/PX/+VMsWrZ0cnRAzymA3W9SssLIumI6RnZ0Fn9CplIiQnpYK17uuVJrASZfJZNDxwF+xHNLT4BPq0PoNq6Cxgn7u30vnJ0+aaeqPavJpl14HDv72+s0r18pul69cWDDvP4gxFLL0cG0ZcXWrAp/Tp83z8amqGW68FQ71DNol6DPCwyM1w729fJFKKuiWLv57DuqEsvVrHVWWHw0IqAXd0j//HKxVq66trV3z5i0RYwjC0qMViEBluQMGQ85aVVeCg0LoEKgNFEXB2Tc+k4CA2lnZWeocoJIlJb12d/eAbagl0OhduXJRIsmH9orOtiw/Ch0bmKCvXr2AxrAsPoyVI+uEZUcrlONKiDlwdoYP+wK69Nu3b0JTBmZY9Mzxa//7Db0Xrvr79+9cv3GVbs108fmoidAPHf7nINQbyGfJ0jnTosdCbvReMC7i469fu3YZapgxP6qftm06pKa+g9YPNFMHqsuZnZ2NjAMenTO2K8rtIciA/p9BtYjZs/369Sv29g71AxtNnz6f3tW1Sy8wAWbMnLDim3V6cmjYMGjzpt27Y37+cfP3+fl5kMOypautCzs2aPRWr/kPfIVaZcyP6gdkbtq0+bu3KdWrB6gD1eX86ccYBweG05uNh+Gc9Q1fJjTv7F6nGVNDiGtALezbv9OYzyd16dwDlYGEW1kXfk+ZuJaJrvghiAGSk5Nev3n5+x97qlWrrtn6MUPZ3xA8GgO0MNB1zZ03VdfeHj3679q9tW7d+l8tXEGUeZJCOYwBwk8SfHk3VdnnbY7RtRcGjUaNHIcqAMynbPJpHqDxg3hlpCwDS7gB5AxYKotCIOYLy2CpLIzFG0AKT9lkRDn0VWV0NoFhQBkaQKyV6QjK8LIT7qssShnugLFUloXi3KNFDAMYSiUQIqEV7qxMRihkvmInQ6mEYuLDewnCmEj6u3yhGDGDocQu7uKX93MQxkSe38usVIVpS4YY0e9Lv5wMxaObqQhjNM8ffMhKJ/tP90eMKJM/wB9mJLj5Wod2dHX1xP6W9PE+OS/u6Pt3LyXjv2X+XL+sftb/t/RZ9gc53NbpWS9Aj5NKPaMeulIpS1zqNtKQH8wClI/ZCJ2pShSGUO03Ip+C13tVvh+J0jkLVf7/7J2FwxZUR2XAPC7x01Kkepd2KHQsWtq3pcbZIQoPuiCE0jF/jSJKz88qkc/IkSO3bvu59GVQogAlxCAoQtOtKfQNpA7fm5pKKKNpybyoRGDyuXqW1RsqMtd9VWUPMxTFXCgUiuS0R1W8mVpaFRUe3gLLZDL6xROewUOp5HI5loobgFRlmatcYeFnA4il4ga4r+IMuAHkDFgqzoCl4gxYKs6ApeIMWCrOgI11zoBrFWfAUnEGLBVnwFJxBiwVZ8BScQYsFWfAUnEGLBVnwFJxBmtr60qVKiHewUOpJBJJRkYG4h18bChEImgDEe/AUnEGLBVn4KFUQqFQofdlB46CaxVnwFJxBiwVZ8BScQYsFWfAFiBnwLWKM2CpOAOWijNgqTgDloozYAuQM5jHG0xFYNCgQenp6SRJSqXSzMxMGxsbmYobNzi9NHcRfFnYA6EhQ4ZkZ2enpqZmZWURBAHPgkG26tXL5NaoQsEfqTp37lyzZk3NEGgwwsPDEV/gj1TAsGHDnJyKVj/z9fXt27cv4gu8kioiIqJOnTrqr2FhYT4+Pogv8EoqpHIvV7lyZdjw8vLq378/4hF8kyo0NDQwMBA2mjRpUqNGDcQjTDDWY1Y8zUxTKOSI1FhkDFILiBLOKfV6fS/lkFHlNLNYUOkctOSpzbGjkb429aCz8Dr8SGp1+VmQgtK3pgQhUHrfdKokGjzHHxlZNiOl+mFGgrO7qHYTZzdfG0QINdKriqVRJgFJkITGssGU2scmQkXnQiUvUcwpJSo8TQXuNlXnnVAvzlB8m45F+8XUOL8EXRYKFTnjLNSPKFiVXP1LhBYB1IdT0jkq/TPF3YDS4UpRCO2XFx1dh/aK1Nf5D65lZryVGenQ1iipQKfmXZ1rNa6CMObm+aO083vTxhmhluG+aveKZ85uIqwTS1SrXdnZQ7zrm0SDMQ1LlZUqr9vcGWFYI7C5S1aa4UVGDUsFI59e1T6WVbXLBe8aDpTCsDlkeGSdUiCSJyO6FReFwvApxosicQYsFWfAUpU/hFBgzAKMWKryh1KQxoxDGGFWIP6NFHISw1Ipq6Zhox/DHAIZNXCJG8Dyx8iVgrFU5Q9p3Ig5loozYKnKH3P2VQRer55NjFyB2yipKONkxzBDQBhlWBh1x0SUR6Xa//ueyKhmqIKx6KuZ06PH0dvde0bu2LmldBxd4bogjXsSb1ytYq1SLV4yOzT0k86dupfeFVivwdAho1EFIzw8UiaT6o/Tv9/QwHoNkbkpZ7Pi4cN7IJXWXfXqNYA/VMGIbNvBYJxBA4cjUzDyvsq4BhCZADRcvft2iL1wBpqvdRtWIZWDvh83fz9iVL8uXcNnzZl86VIsHbNNZEhS8ptvVy3t2j0CqdqWJUvnQEwIP3f+lGYDqCuHSVNGzZw1UfPX58ybOn7icD1JdPHixTP43Vu3rtNfT5w8Al//OLBXc++9+3c0G0BNbt68FtUh7MDB35DpDSBBGaWCUZFM6qqsrKxyc3MOHdo3Z/aSnt37Qcj361bu2x/Ts0f/mN1/tg6PXLR45tlzJyH8yOEL8DkjesGfB8/AhlgsTnyaAH/Ll65u1DBYM09dObRpHXXt+pWcnBw6Wn5+flzcpXZtO+pJogs/P393d4+79+Lpr3fu3PTw8LxX+PX2nZsO9g516wRqTfv8+dP5C6d169anR3cm865J4wbuzD8QC+YMnLIBA4a1i+zo6+snkUiOHvsL2oRuXXs7OzlDtxTZtuOOnT9pTZic/GbxoiBwikEAABAASURBVJUtWoS7uBT5XtSTQ+vW7UiSPB97io4JVRm+RkREGf+jmgQHhd6/f4fevhV/vWOHrvBJf719+2ZISJhAoOV0paa+j545vmHD4AnjpiE2YWvMvG6d+vTGo0f3pVJpaEhRhxTUuGliYkJGphbvitX8qtvY2JQI1JODq6sbbJ+PPU2HX7hwpmmTZpUru5r0o2qaBIfG31a+jJWR8eHZs8RuXfuADCkpyUhVq5o0KWmOql4Nyp85e6KTk/OiBd9oFdIYynkMEJpBeiM7OwupOpUSEdLTUr28Ss79t7K2Lp2VnhygxkAdWr9hFdRjoVD476XzkyfNNJgE6aBp0+aZmRnQLUEjXKtmHZA8MLBhfPz1Zs1avHnzqlloixLxwcbe+9su6BQhmvp4GWDk/GYjpCLKdAfs6qacQDh92jwfn6qa4e7unmbJAaSCbuniv+fgZClbv9ZRjH8U6mj16gHQXSU8edSwkbKzhC4TvgqEQm8vH+i6SiepVavumNGTZs+dDK3r8GFfIGYYVx2NkIoq07iSr4+ftaquBAeF0CHp6WlwPdrZ2UGPUsYcYBtqCTR6V65chLaoZYvWdKD+JHoIDg4FIzAx8fGQIcoa2bBB0OYt66DeQEelNX5Y81ZBQU3HfjEVLheodlC9EANI85kVZbkDhrMDlxtcdNAzQ/8BZhh0wmv/+w1SuW6uUsUdbLYbN+P0vBOvJwcaMC6gmbp27TLUMCOT6KJJEEh1TVmrGgTB1wYNgsC6g5xLd1SagOHXvHnLxUtnq21RNjByDLBMDOj/WUBA7Zg9269fv2Jv71A/sNH06fPpXYMHjfx5+6YrVy/+EvMXsxwAaPRWr/kPCA+1ysgkugBJklOSwHCvVEn5kpaDg4O/fw2wR6C26U84e9bikaP6rfx28eKvViJ2MPx6wfqpCT0nV3dyFSIMO+TlkL+uTJy01sAbBsZNg8GwidmMdT49/4Cua+68qbr27tp5wNnZBVkcisQP7EvRsGFQTMyfuvY6OjiiCsxH98C+guuhByP6KoJfjSBnMaKvorBpwTIEnrLJEQQUloojGPm8ypjhWoSpCBgzXFs+M5YwJSjnGUsY48F9FWcw/BCEECBKwENXsBUKY8x1I6QSotyMfIRhjeyMfMKI54aGo9jYC+79m4UwrHH3QoaNg2EhDMcI61opKTEPYVjjdUJuaHvDI/pGeS5LvJ31z/9Smka51A9zQxjzcf/K+7ijH9oPda/Z2LBrJGP9Ad48m3r5cDo8WBEIkFxW1Alq+sEr8LpXarYU9Jl0SOkNNZCtxmSQQpd/unNT50BHLeGOj3YHWRCz0C0fVZinOrnaa6T6h9TR1IGl334SEoRC9dsljqLYIVAqc6yonMof0owsslI+poKA0PaVQyIrIyMwzSX+vbj0tJcyktRlr1CFLWqJs6v2fkloumcs9M1oKnTawpTavFmeOnWmTZvWtAyUDv+YxTw5FjsEolg0oqRWhT4htWepUShC7eqx9JFSQqqKt1VgMxOeZPJn9QI1JEk2b9786tWriF/w8BZYLpeLRHxclgbxDiwVZ5DJZGKxGPEOXKs4A5aKM2CpOAPuqzgDrlWcAUvFGbBUnAFLxRmwVJwBS8UZsFScAUvFGfAtMGfAtYozYKk4A5aKM2CpOAM2KzgDrlWcgYeHBFXKxaUcPIWwDQ+lkkgkWVk8fB+Cjw2FSKTHZR13wVJxBiwVZ+ChVEKhUKHg4RuxuFZxBiwVZ8BScQYsFWfAUnEGbAFyBlyrOAOWijNgqTgDloozYKk4A5aKM/DHxciwYcPevXtHEIRMJktPT/fw8IBtqVR69OhRxAvYWmvR8nTu3PnDhw8pKSlpacpVxZKTk5OSkhivf1gB4c+R9OvXz8/PTzOEJMkmTZogvsAfqaC5Gzx4sOYSfW5uboMGDUJ8gT9SAV27dtWsWI0aNapfvz7iC7ySCqmMC7piQZWCSoZ4BN+kioqKCggIQMplYGsFBwcjHmHAWH/5KPfc7+9yM+XSUuvClvB2iZCW7dLeNAs9VxYgICiSIozJCpXyaqnrJyB7hUIuFIg0XTrSHkBLH6sxjjw1AilVn6h90eUCT42Eltw0f4IotXCR2IqydxaFdXMJCNQ301SfVA+vZZ745W0lDyv3qtaIEugom360xdHhppI+OeoviNCbib5wpMW3JqVrcROtmRCF3jtNuek0xv2mlq+IpKjUN/kfUqThfarUb65zMXCdoxXHf0l+dC37swU1EcZS7Fqe8PpRTvuh3lr36uyrQKfBc6sjjAUZMq/m45u5up6Lapfq762vbG0JeJyKMJbFxgH9/dMbrbu0N4BZ6QqxHV7ZpRywdRBnf9Beq7TrIcmjKBKvhFQOyCRIIdW+C1edioVQSOjSBEtVsSBJRCq03yHokAo3fuUERem80dUhFd9WNOAMQqEAibUvaIobwIqFQkEqZKY0gASBW8DyQSAAs0K7VDpGK/DysuUESVKkjknc2msVReLeqnzQ05rx7XkV59Hd92CzooJB6WzRtNcq6NywYVE+EIRARyuoXSro3Hi3WBw3gNNO6tils68qr0rVvWfkjp1bEKfY//ueyKhmyDxQus58OTSAT58+GTDoU117+/cb2qghx6avBNZrMHTIaGQO9Jx27WaFqgFkS6uHj+7p2Tto4HDENerVawB/yByolsDVvstsxjo0XPv3/zLly8/bRIZkZmVCyJGjf46fOLxTl1bwuW9/DN37/bx904qVi1NSkiHab/t2JyYmwMalS7F9+nUcPWYgKt4A3r0bP3PWxG7d2wwd1mvjD2tycnIg8GrcJUhy584t9U/ff3BXmcnlC7qS6Cc7OxtKNW7CMCjqkKE9IFV+fj69a9FXM5csnfPj5u8h/9NnjkOeCxfNUCecHj0OyiyXy9UN4KQpo+DXNTOfM28qHD4yGngIIhSZYlZAAygwsQUUi8V/Hf6jZs06367cYGdrd+LkEZCkdq26MbsOjR41AaRav/E7iDZi+NgB/T/z8PA8fTKub5/BtDvMHbu2QLs3fdp8zQxfvX4ZPXN8viR//bqfly5elZj4+MtpY+C8NAkOdXRwPHf+lDpmbOxpCAkNCdOVRH/Jf/9jT8wv26EA/1m+9osvppw5e/x/OzarDyrxaQL8LV+6OjgoZPbMr87Hno67dhl2nT13Mv72jflzl2u6iWzTOura9Svq6wMkj4u71K5tR2Q0CgWlkJs0sGQ60Lk5OTlPmhAd0rQ5lP7w4QONGgVPnTK7UqXKcHJHDBt74MDe9PS00qngE84yyFavbrFJyydO/CMWieGM+/n5+/vXiJ6+4HHCw9gLZ4RCYZs27c+dP6mOCbJFRnaEcF1J9Je8X98hWzb/EtG6HYjxf63atIlof+XqRXXxkpPfLF60skWLcBeXSvXrN+rerc+aNf/Jzc3d+MNquOzgVzSzat26HUmS52MLLiP4afgaERGFTDmNul5e0SUVk6GlOrUD6Q0o3527t0JDPlHvCg4OhUC4DLUmrF2rXunAu3dv1a1b39m5YBajp6eXt7cvnQMcPDShjx4/QCoj5dWrF5GqK1dPEj1A1bka9++48Z9FdQiDhm7vb7s0L6lqftVtbGzUX8d8PlkilYwdP9TNzR2ahxJZubq6BTVuCjWP/nrhwpmmTZpVruyKjAa6CdKk51Wq2YommxVWVlb0hlQqlclkW7dthD/NCKVrVUFCa+vSgdnZWQ8e3oNzVyyHtFT4hNMBlfXcuZPQwMJ5qVLFvUGDxvqT6GHzT+ugDYCmD64taJm3bN1w+J+DuspmZ2fXo3s/OC6oUlpf3oLLaP2GVdD0QS3/99L5yZNmIlOArociTHkIQpXtFhguQzik9lFdwsMjNcO9vXyNz6Syq1vDhkFwRjQDnZ2UNQZaCWgDoXmBXhA6qqh2nQ0m0QUc559/7e/Te9CnXXrSIaC3nvgZGR/+OPBrm4ioX/Zsj4rq7OVZcnolSPX9upUX/z0HF66y9WttQuunOjRECLVXEh3PqwRlfRAcEFA7KzsLWn/6K1SypKTX7u4eJuRQo9ax4383btREffE+e5bo61vwTk7biPa//74HTEfojebOWWpMEq1AwfLy8qA1o79CewBnWU98qDHQJC5c8PXEySNXr14ONlSJCM5OztDoXblyUSLJb9miteb7XsagNCtkpj2vKusd8OejJkJLDS0JXFm3b98Ek3da9Fg4EbALzl1q6vvY2DMvXz7Xk0OfPoMhLdiN0JhATLCYR47uD8YYvRd6eBAejOwaNWqq+3b9SbQC1z7YIP8cOfT6zSuoMStXLWnYICgrK1OrlQ9XBhh+06crLdWZ0Qtv3rp29OhfpaOBcREff/3atcsmGRQG0S4VpdDZuRkJNESbN+2Oj7/Rs3cUGNA5OdnLlq62VrX7Yc1bwelYsCj65Cl9L1Q7OTpt3fKrrY3tF+OGfDa8N5yXGdELoHNSR4C2BSyLtm06GJ9EKwvm/cfG2mb4iD5DPusBFWL06InwtWfvdknJxWa5wu3Xim8XDxwwzMdb2YyDwL17Ddy4aQ0IXCJDKFjK22S5Qg61CpkI3FeJxDreg9DaKe1Y9oxSEL2mVkMYy/L7uucKKTVyiX/pXTrMCgo/Ba5waJcKxjZIHjnp6NotQteuWbO+atUyAlUYVLNrTTHWYWyDT3PWt/+8T9cuR0cnVJFQDSxp36XLWCf41ALCIALiCsrRB1OMdQo/BS4vlK8SmziyjqetlwsC3cO1OmcsYaXKBWjOSB2TK/A0GM6gvVYJhASWqlxQPa8y6f0qlSWCMBaH0t0A6pgGo8ANYIUDz67lDLonl5EIY3kIAUkITbkFtrEnhFYIY3nAqrB10GHraQ31rG6dl8VDp8oVn9xMmXtVU6Rq3dMTzIr48+8RxoLcvZgKlnfbfia6wxq9pPqtMx+uncJqWYhrp1KunUz/bJHOmSD6/AFKpdLtX70gScLalpDLdFqEAtVQsHKSeymvhbCL1Agq5pCxcKINWJtk4ewAlYPFoiIRKs+OBVkJCbXvDaIwTqG3xIKv6qxUjvkoGMgkqYJtzVSlc6D/o5Ord1H0oVEFaWnHjeqfUG6onsCqQ4QCgULznkj5w6piaBwgnRWBis2HEIiRLF8BWY1c5Gdlq9NGMOwS/+rx9y8f5eXn6IxGT29SOaAsOc8JdmlakponS0ccSiWPll3FpCqebZFCheHv371zdXMTCAVKH56FhSrcqwwo6UFTtavgU+3UEz6FBRkS9Ms0VNFPKJ8TqR5BqEPgqaBCw5FL6VKpjg8RQlXeGlrZOAh8a9o2a29gZid/Vi9QA0cUGhoaFxeH+AUP3wWWy+WaU/55A5aKM/DwkGQyGf0uEM/AtYozYKk4A5aKM+C+ijPgWsUZsFScAUvFGbBUnIGfUmGzghuAVLxczQQ3gJwBS8UZ8C0wZ8C1ijNgqTgDlooz4L6KM+BaxRmwVJyBh4dEkqSms0XVnh3HAAAQAElEQVTewE+pDPqr5SJ8bChEIiwVN8BScQYsFWfAUnEGeFilUCgQ78C1ijNgqTgDloozYKk4A5aKM2ALkDPgWsUZsFScAUvFGbBUnAFLxRl45WKkd+/eBEHk5eW9e/fO3V25JFV+fv6JEycQL+BPrdqwYcPz50ULYiUnJyPlugUmrHNYwTHbCqblzoABA6pVq6YZAk/u69evj/gCf6SCCtS1a1fN13WcnZ379euH+AJ/pAL69u1btWpVehv64Nq1a4eFhSG+wCup7O3te/bsSa95C9uDBg1CPIJXUgEDBw6EigW9FPRbrVu3RjyirMb6yb1vkp/KZBJSLlNlp+GcUdPZokCA1A4oNb1OFo+jdGioLk0JP4+0y0xNSnmvVCaBEIk0PzcnD2oVXb2KefEs9KJYIm1RnsXLoxmndBKtjkJLIxQSVrbIu4Z1m75eqAyUSarNcxMgtWMlKzhHCjntmrLI0aamd0lTt1Fx76klctYVX7n4E4m0etAs3C5wb1kirfbIJfIp5e5VV0lKIFAaOlR2uvJaHvN1TcQU5lJtmpngH2zXsrM3whjH5SOvE67ljV3JUC2GfdXWhU+8A6yxTibRvKNP1Xq2WxYkIEYwkepNYp4kj2ozoCrCmEh4Lx9ZPnr5KIdBWiZSPbyRJeThMK+FEIoFj69nI9NhcsoVEkohxQvxMEQho/LzmNgHuHZYGpXbfywVF1AtjMCkTWIilXIlWtz+MYViWKkYSUVSCC/vxxghXOaMbpGYSIWrVFmgmC45z0QqXKXKBoX7Km5AUQSzioX7qnLBUlIRuFaVBYqZUkzvq3CtYg7B0DBjZFYgTJlg1iQxqlVYqzJAPwJlALfnVvz19x9tIkPMNe150Vczp0ePQyxDMTXKuDcG+MeBvQ8e3p0zazEyN+HhkTKZFFmCj2O49uHDe4gdItt2QBbCUmYFA/slKzvr5+2bLl+KTf+QVqd2YLt2nbp07gEhv+3bfejAabX7vv37f9m0+b/79x1bs+Y/BEG0i+z0zcqv8vJyAwMbjh0zpV69BlOnjbl16zrEPHbs7x837aJTpaa+X7p87t278b6+fgP6fwY50+EQ8r8dmx88uOvsUumTsP8b9tkYe3t7XYVBqgYwOzvru1U/XLhwdv7C6SUOYef/fof8obHdum3jpcuxb98mN2gQ1LN7v7CwVsgUBCIBswezTPoqBk3typWL792Nnzp1zvZt++CMr1n7NZzHrp/2zsvLOx97Wh3t7PmTrVpGODk6gXh378UfP3F40w87//k71trK+usViyDC2tWbIXn79l1On4yrXasuUr2i8/36lUOHjF793aa6deuv/e83KSnKFwtevX4ZPXN8viR//bqfly5elZj4+MtpY+heTWthNEvboEFjyE39FxBQy9PDy9W1Cuz6ft3KfftjevboH7P7z9bhkYsWzzx77iQyBVJOKhj1rRYyK27FX4eeIDQkzN3dY8znkzas3w5H7uZWBUJOnTpKx4HKcfv2zfZRXeivebm5M6IXenv5gBiRbTu+fPk8Nze3dM5w9rt17dO8WYvgoJDhw76Ar/cf3IHwEyf+EYvEIJKfn7+/f43o6QseJzyMvXBGV2E083R2doHc6L8XL569fv1y2dLVtra2Eonk6LG/Bg0c3q1rb2cn586dukPBduz8CZkExfC2lIlUDBrAhg2D9v6264dNay9ePCeTyerUrufpqZy/2LlzD2hMMjIzYPvM2RNwjpo1a0Enqernb2dnR287ODjCZ1ZWptbMGzdqQm+4OFeCT0l+PlK2fregkkGG9C74OW9v3/jbN/QUpjQJCY/Wb1g1a+ZXULHg66NH96VSaWjIJ+oIQY2bJiYm0OU3FkveAjMQGI720KF9p04fhXPkYO/Qs2f/z4Z+DtUFmjt7e4ezZ0/AdXru/EmoUupXOQQCY39F3dURGqcAep0HD++BKa8ZMz0tVU9hSmSbmZU5f+G07t36RrRup84TPidNGVUiJmQLlQyxDLOHIKSpSaD7GTJ45OBBI+7cuQWd085dW6Gi9Os7BE5Qp47doE+Cdj8+/saUSbOQmajs6ga1Z8TwsZqBzk4uegpTIodly+Z6eHiNGztVHeLqpmwnp0+b5+NTbGKdu7snMhqVWWHB4VqTyM7OPnb8b2jZbWxs4PTBX0LCw0ePH9B7u3TpuefXHXCBg5lQowbzecIlCKhRC34U2kZ17Xz2LBFMOGisTp48oqswamJ+2Z74NGHrT3s0X9jy9fGztraGDejD6JD09DSKotQNtTGozAomDaAlLECoOmA0f7VkFlzFaWmpYGc/TnjQsEEQvdfXpyq0+Pt//6VD+0+NyQ2u6Pv371y/cRVOk55offoMJkly/cbv8vPzwST5cfP3I0f3h7MvEuorDA3cD/y0ZT3Y/RD/xs04+u/t2xSQBCwXsCPA/IFOC2w/MDLB5kQWwRK3wHD9Lvnq23UbvqVb+erVA8Z+MRXaPXWEFi3C79y9FRnZ0ZjcunbpBd37jJkTVnyzTk80aOW2bvl1z57/fTFuCFhxYGLMiF5A2/f6CwOAmQefGzau1gycOCG6d68BoF9AQO2YPduvX78CvWz9wEbTp89HJsH0ERKT1wtO/ZLy4Fr20AUByEzMmTfV0dFp7uwl6CNg17In1QLtO48woXujYfgU2CyD69CHQeNz48bVu3dubdu6F30cMH4wW55PgZ8/T5w2fWyVKu6LF3/r5lYFfRwwnu3AcMaSWZ4C16/fCMaH0McGRVjwgT2eWFEeMGoAEaYcwFM2LQ3ckQstZlZgygJpyQf2eB5gmaCQyUOoKpjVKgK3gZaHWV+FhWIONEi4r+IGcJ0rPpLJZR8tWCrOwEQqkRUlssImIEOEYsraylLTYAIaOcrlzAxODFLIULVG9sh0mEhVtba9tRU690cSwphI7KFksTWq2dAJmQ7DeYCjltd8cSfnVixWywQeXkt7Gp89YnF1xIgy+QP8cXaCUEw4VrYSiYSUjrVtCAFFkarX/0s0mSVC1F9VG1qc76kuKoqERwhEqVSUegyZECJlSQiNV6PpOBruBAUiRMqLfAMWRihMQscUwKACUbRXs3hUYeEIVSqSKH1EBUdNb4souYzMSpXI5ejz5dU159WYRFm9bB7d+ebtC0l+HknqmIVDuzUsfepLeGJURyjYKO2oUfVaEqmgNCf7FUYuEkYoJBQKSiFXCEVCzTia+QlFSCEv8uKp9RfVbkELkhc6ZyRUelCloqHivh01w5VeNu2IKr6iTsN8URng1eoFakJCQuLi+PbQkof3VXK5nHEjU5HhoVQymUwsFiPewc9aVXoCOg/AUnEGLBVnwFJxBmxWcAZcqzgDloozYKk4A+6rOAOuVZwBS8UZsFScAUvFGbBUnAFLxRmwVJwBS8UZ8C0wZ8C1ijNgqTgDlooz8PGQRCJbW1vEO3goFUmSOTlMFt6t4PCzVplrPYMKBZaKM2CpOAMPpRIKhQqFAvEOXKs4A5aKM2CpOAOWijNgqTgDlooz8FMqbKxzA7ivwrWKG+AGkDNgqTgDlooz8FUq/niDmTlz5vHjx+mFxeDpIu3fBzZu3ryJeAG3F0bXZPz48T4+PoQKMAJBM9gIDg5GfIE/Uvn7+4eHh2uGODg49O/fH/EF/kgFDB06FCqW+qu3t3fHjkatNMcJeCWVl5dXREQEvQ3GRb9+/RCP4JVUwPDhw+mKBVWqZ8+eiEeUswUYH5v+7qUkL1shA+ua1PDJSPtR1PDJWOQYkVL9U1l4xYtOqbwxojdv3iQlJfn4+nh6eAoIRBZGUnpgJBCEKL1kkho/VDwj1TcNt50ERQgoWweRVw2bhi1cUPlRDlLlZUsP/5yS+loilShPiEBIqAzsQvekBeUq5RuzSCtlmQmiWEiBT1SiYMUmBUkqXQJCNEFRtiqlVHIijYWdiEJp1D+m8r6pedWQysVxVCkoJBITVapadfjMw8HZClkWi0oFA947lr3I+aAQ2wjsK9t61KwktuHSKxsyqezt44yctFxpnsKhkmjgNC9rB2tkKSwn1cFNr14+zLdxFNf8pEwuXCsICZde52dKfWvb9BhnocOxkFQ/zXsik1GBbRj6ra6wPDj7HBHk2K9rIvaxhFQ/zU0U2AgDQvlQmUqTcPWlPEc+dgXrarFurG+a9UTIX52AmqFVre2tfpiRgFiG3Vq1dX6iyF5cLcgb8Z0XN5MlOZLPl9VArMFirfpj40uZgvwYdAL8gjwpBfHHhpeINdiSKjtb+vqxpG443+wIPdQO93uTKEl/l4/YgS2pflv12trJ0jeJ5Y6Nk/WBDWwtacOKVB/S8nIyFbXCfNBHRkAzbzjwtBQJYgFWpDqx853IquIu9ZCdkx69oPnN2ycQC4ithSdjUhALsCLVu9cyJw8evjhtDI4e9qnJMsQCrEilkFPedaugjxLvOq5yGSXNlSJzY/4ZS9dPpxJs3lhnZqX++c/aZy/jpdL8OrXC2rUe6V6lGoRfuPTb8bPbxo38YceeOSlvE708aoa3GBja5FM61Y34Y0dO/piXlxlY9/9atxyM2AQG62/FZoa2d0Nmxfwn9d0riYDZctJGAGPzm7aNf/Lseu+us6dPjHGwr/z95pHvU18h5QJi4ry8rAN/r+rXY+63Sy41atB274Fl6R+SYVdSSkLMvoUhwZ1nT90fEtTl4N/fITYRiATv35i/VplfqtxMkp7hxQZPX9x8+/7ZwD6L69b+xMnRtWvHyfZ2Luf/3UPvVShkUW1GV6vaEB5KgSQwEPM66RGEX7y838XZMypilJ2dU80aTZuH9EBsIhAQOVnmX+HV/A0gxeZLGM+e3xIKxbVqhNBfQZKA6k0Sn91QR/DzqU9v2NkqVwnNy8+Cz/dpLz09ioZ8qvoEIjaBS4RSmH+4zvxSWdmxOK6Yl58NVQdMbc1AB/tK6m3NlRjV5OZmurlWVX+1smLXOiUVpJiFJa7NL5WTq5h8wNbgiqODK5zokYOLdTYG21to92SyoiJJJOy69SHlyLmK+U+s+XOs2dgh/lwmYgcfr9pSaZ6Li4db5YKnKqlprzVrlVYquXjde3CeJAs60XsPYxHLBDRmskizfszf/3vXsIPPtNdZiAVqBYTWrfXJbweWg2mXnfPhwuV9/900/Mr1P/Wnaly/HYxQHPj7O2iZExKvXby8D7HGh2RllfWrbYfMDStvgtg5Eu+ff6js44hYYOSQ1f9e/X3X3vnPX96u4latSeOO//eJgdnOdWo1/7TDpH+v/D5jYRiYgoP7Lt6w5YuSU9PMxPunH2wdWDGAWTEBLh99f/XIhwbtP6InIGrunnjaJNLlk85mvv9FLA0sNe/gBgMWSY/fo4+M5Mdp8MmGToi9V+Hqf+J452KWVy3thYZhheWrtd+H2lo75Emyte7yrFJj4pifkPmYvzxS1y6FQi4Uajk5lV28p03YqStV6ovMus0cEDuweA/0w8wEayfrGk216FKHUgAAAa1JREFUPLAHYyw7O01rKrlcKhJpfyYJ587e3pxTkTMzddZ7mUIqFmophkAo1GVwPr3+Ji9dMn4VW1OX2J0Gs/7LhFrhvtacmkLLDIVUcf/Mi4lrWJxixu7ksogBbgnnX6GPgPvnXkT0cUVswvqUzWf3c/76KalBFJ+twTvHn3YZ6VW9gT1iE0vMrn10I+vYjpQqNZw9alZG/OLtk7S3TzLaDqgS2NwZsYyF5qy/T8rZ+12SyEpYo7m3yIoPHhjgydmTi69kErLvFE93P7asPk0s+tLOb2tfpjyXWNsJK1V1cqtWnq+VlYX3Lz6kvciU5io8/Kz6fumHLEU5vAq3d/XL90kSUoGgkomshdb2YpGNAJ7hasZRvwNHmD7+U+z9uVLJC96aK3yLrvAluqLP0pByuSxfkZeZr5CRCiklECI3H6t+FhSJptxeML17OePh1cy0ZJkkjyw4a5oF0aNVYQhFUMr3FIvLonzVUVCUpOSLjeoN9ZunJVRSP2YqDITcKNU23OxV8bWu29QxMKx82gP+eIPhPTz0scRXsFScAUvFGbBUnAFLxRmwVJzh/wEAAP//e2aEcQAAAAZJREFUAwCieQXuMOXSBQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000024D83337160>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = StateGraph(MultiSourceRAGState)\n",
    "\n",
    "builder.add_node(\"retrieve_text\", retrieve_text)\n",
    "builder.add_node(\"retrieve_yt\", retrieve_yt)\n",
    "builder.add_node(\"retrieve_wiki\", retrieve_wikipedia)\n",
    "builder.add_node(\"retrieve_arxiv\", retrieve_arxiv)\n",
    "builder.add_node(\"synthesize\", synthesize_answer)\n",
    "\n",
    "builder.set_entry_point(\"retrieve_text\")\n",
    "builder.add_edge(\"retrieve_text\", \"retrieve_yt\")\n",
    "builder.add_edge(\"retrieve_yt\", \"retrieve_wiki\")\n",
    "builder.add_edge(\"retrieve_wiki\", \"retrieve_arxiv\")\n",
    "builder.add_edge(\"retrieve_arxiv\", \"synthesize\")\n",
    "builder.add_edge(\"synthesize\", END)\n",
    "\n",
    "graph = builder.compile()\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ad91e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Searching Wikipedia...\n",
      "üìÑ Searching ArXiv...\n",
      "‚úÖ Final Answer:\n",
      "\n",
      "Data Science is an interdisciplinary field that leverages a variety of scientific procedures, algorithms, tools, and machine learning techniques to extract valuable insights and create meaningful patterns from data. It integrates knowledge and methods from several foundational areas, including statistics, computer science, machine learning, deep learning, data analysis, and data visualization [Internal Docs].\n",
      "\n",
      "Here's a detailed explanation incorporating insights from the provided sources:\n",
      "\n",
      "1.  **Core Definition and Purpose (Internal Documents):**\n",
      "    At its heart, Data Science is about making sense of vast amounts of information. It is defined as \"an interdisciplinary discipline that encompasses a variety of scientific procedures, algorithms, tools, and machine learning algorithms... and creates patterns from which valuable insights may be extracted.\" Its primary goal is to \"make connections and solve challenges in the future.\" A key distinction is made between data analytics and data science: while **data analytics** focuses on \"removing current meaning from past context,\" **data science** is predominantly \"concerned with predictive modeling.\" This forward-looking aspect underscores its role in informing future decisions and strategies. The rise in its popularity is attributed to the increasing importance of data, often referred to as \"the new oil,\" which, when properly analyzed and utilized, can be immensely beneficial.\n",
      "\n",
      "2.  **Scientific Foundation (Wikipedia: Scientific Method):**\n",
      "    The \"scientific procedures\" aspect of Data Science is deeply rooted in the scientific method. This involves a systematic approach to knowledge acquisition, much like in traditional sciences. As described in Wikipedia, the scientific method encompasses:\n",
      "    *   **Careful observation** coupled with rigorous skepticism to prevent cognitive biases.\n",
      "    *   **Creating a testable hypothesis** through inductive reasoning.\n",
      "    *   **Testing this hypothesis** through experiments and statistical analysis.\n",
      "    *   **Adjusting or discarding the hypothesis** based on empirical results.\n",
      "    Data science adopts this empirical framework to ensure that its insights are robust, evidence-based, and subject to verification and refinement, rather than mere speculation.\n",
      "\n",
      "3.  **Methodologies and Practical Application (ArXiv):**\n",
      "    The ArXiv paper, while focusing on software development screencasts, exemplifies the practical application of Data Science methodologies. It showcases how data science techniques are used to \"find, understand, and extend development\" by analyzing various forms of data. Specifically, it highlights:\n",
      "    *   **Similarity Analysis:** Techniques like **Cosine similarity** and **Jaccard coefficient** are employed to measure the similarity between video frames and between text transcripts (spoken words in screencasts and API documents). This is a common data science task for clustering, classification, and recommendation systems.\n",
      "    *   **Natural Language Processing (NLP):** The paper discusses extracting \"popular development topics\" using \"LDA topics\" (Latent Dirichlet Allocation, a topic modeling technique) from screencast transcripts. This involves processing unstructured text data to identify underlying themes, a core area of NLP within Data Science.\n",
      "    *   **Tool Use and Evaluation:** The use of Python toolkits like `pyLDAvis` and `NLTK` for text processing and visualization, along with evaluating performance metrics such as **precision, recall, and F1-score**, demonstrates the practical toolkit and rigorous evaluation standards characteristic of data science projects.\n",
      "    In essence, the ArXiv paper illustrates Data Science in action: applying statistical and computational methods to complex data (video frames, audio transcripts) to extract meaningful information, discover patterns (recurring tasks, popular topics), and inform practical applications (extending screencasts with relevant API documentation).\n",
      "\n",
      "4.  **Advanced Applications and Future Focus (YouTube):**\n",
      "    The YouTube transcript, although not defining Data Science directly, points to cutting-edge areas where data science plays a critical enabling role. It discusses \"agentic AI systems\" that rely on \"feedback loops, memory, and and tool use,\" emphasizing \"temporal reasoning and autonomous tasking.\" Data Science provides the algorithms, models, and data-driven insights necessary to build, optimize, and understand such sophisticated AI systems. Data scientists are crucial in preparing and analyzing the vast datasets needed to train Large Language Models (LLMs) and design the feedback mechanisms that allow agentic AI to learn and adapt autonomously. This shows Data Science's contribution to the evolution of intelligent systems that can make connections and solve complex challenges in dynamic, real-world environments.\n",
      "\n",
      "In summary, Data Science is a dynamic and evolving field that combines rigorous scientific inquiry with advanced computational and statistical techniques to transform raw data into actionable knowledge, predict future trends, and drive innovation across various domains, from improving software development resources to powering the next generation of AI systems. Its significance stems from treating data as a vital resource, \"the new oil,\" for addressing the complex challenges of the 21st century.\n"
     ]
    }
   ],
   "source": [
    "question = \"what is data science? Explain in detail using references from youtube, wikipedia, arxiv and internal documents.\"\n",
    "state = MultiSourceRAGState(question=question)\n",
    "result = graph.invoke(state)\n",
    "\n",
    "print(\"‚úÖ Final Answer:\\n\")\n",
    "print(result[\"final_answer\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "565b377e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is data science? Explain in detail using references from youtube, wikipedia, arxiv and internal documents.',\n",
       " 'text_docs': [Document(id='6ef98c5f-41cb-41d7-9e31-7adacc9c79ba', metadata={'source': 'data_science_interview_cleaned.txt'}, page_content='employment roles of the twenty-first century.\\nBackground Interview Questions and Solutions\\n1. What exactly does the term \"Data Science\" mean?\\nData Science is an interdisciplinary discipline that encompasses a variety of\\nscientific procedures, algorithms, tools, and machine learning algorithms'),\n",
       "  Document(id='b8bec5b3-efb3-4b80-808f-28b6c6eb7002', metadata={'source': 'data_science_interview_cleaned.txt'}, page_content='it, and creates patterns from which valuable insights may be extracted.\\nData science is based on a foundation of statistics, computer science,\\nmachine learning, deep learning, data analysis, data visualization, and a\\nvariety of other technologies.'),\n",
       "  Document(id='ec9786b0-e4eb-4c59-8ae7-1758162c2703', metadata={'source': 'data_science_interview_cleaned.txt'}, page_content='people make connections and solve challenges in the future. Data analytics\\nis concerned with removing current meaning from past context, whereas\\ndata science is concerned with predictive modelling.\\nData science is a wide topic that employs a variety of mathematical and'),\n",
       "  Document(id='4c439773-cf5a-467b-afca-ad1e1efe47f8', metadata={'source': 'data_science_interview_cleaned.txt'}, page_content='variety of other technologies.\\nBecause of the importance of data, data science has grown in popularity in\\nrecent times. Data is seen as the new oil, which may be extremely useful to\\nall parties when correctly examined and utilized. Not only that but a data')],\n",
       " 'yt_docs': [Document(id='3101269c-9602-4a14-afac-9d3014e6f584', metadata={'source': 'youtube'}, page_content='\\n    This video explains \\n    how agentic AI systems rely on feedback loops, memory, and tool use.\\n    It compares them to traditional pipeline-based LLMs. \\n    Temporal reasoning and autonomous tasking are emphasized.\\n    ')],\n",
       " 'wiki_context': \"Page: Scientific method\\nSummary: The scientific method is an empirical method for acquiring knowledge that has been referred to while doing science since at least the 17th century. Historically, it was developed through the centuries from the ancient and medieval world. The scientific method involves careful observation coupled with rigorous skepticism, because cognitive assumptions can distort the interpretation of the observation. Scientific inquiry includes creating a testable hypothesis through inductive reasoning, testing it through experiments and statistical analysis, and adjusting or discarding the hypothesis based on the results.\\nAlthough procedures vary across fields, the underlying process is often similar. In more detail: the scientific method involves making conjectures (hypothetical explanations), predicting the logical consequences of hypothesis, then carrying out experiments or empirical observations based on those predictions. A hypothesis is a conjecture based on knowledge obtained while seeking answers to the question. Hypotheses can be very specific or broad but must be falsifiable, implying that it is possible to identify a possible outcome of an experiment or observation that conflicts with predictions deduced from the hypothesis; otherwise, the hypothesis cannot be meaningfully tested.\\nWhile the scientific method is often presented as a fixed sequence of steps, it actually represents a set of general principles. Not all steps take place in every scientific inquiry (nor to the same degree), and they are not always in the same order. Numerous discoveries have not followed the textbook model of the scientific method, and, in some cases, chance has played a role.\\n\\n\\n\\nPage: Unidentified flying object\\nSummary: An unidentified flying object (UFO) is an object or phenomenon seen in the sky but not yet identified or explained. The term was coined when United States Air Force (USAF) investigations into flying saucers found too broad a range of shapes reported to consider them all saucers or discs. UFOs are also known as unidentified aerial phenomena or unidentified anomalous phenomena (UAP). Upon investigation, most UFOs are identified as known objects or atmospheric phenomena, while a small number remain unexplained.\\nWhile unusual sightings in the sky have been reported since at least the 3rd century BC, UFOs became culturally prominent after World War II, escalating during the Space Age. Studies and investigations into UFO reports conducted by governments (such as Project Blue Book in the United States and Project Condign in the United Kingdom), as well as by organisations and individuals have occurred over the years without confirmation of the fantastical claims of small but vocal groups of ufologists who favour unconventional or pseudoscientific hypotheses, often claiming that UFOs are evidence of extraterrestrial intelligence, technologically advanced cryptids, interdimensional contact or future time travelers. After decades of promotion of such ideas by believers and in popular media, the kind of evidence required to solidly support such claims has not been forthcoming. Scientists and skeptic organizations such as the Committee for Skeptical Inquiry have provided prosaic explanations for UFOs, namely that they are caused by natural phenomena, human technology, delusions, and hoaxes. Although certain beliefs surrounding UFOs have inspired parts of new religions, social scientists have identified the ongoing interest and storytelling surrounding UFOs as a modern example of folklore and mythology understandable with psychosocial explanations.\\nThe problems of temporarily or permanently non-knowable anomalous phenomenon or perceived objects in flight is part of the philosophical subject epistemology.\\nThe U.S. government has two entities dedicated to UFO data collection and analysis: NASA's UAP independent study team and the Department of Defense All-domain Anomaly Resolution Office.\\n\\nPage: Nuclear fusion\\nSummary: \",\n",
       " 'arxiv_context': 'Find, Understand, and Extend Development\\nScreencasts on YouTube\\nMathias Ellmann\\nUniversity of Hamburg\\nellmann@informatik.uni-hamburg.de\\nAlexander Oeser\\nUniversity of Hamburg\\n9oeser@informatik.uni-hamburg.de\\nWalid Maalej\\nUniversity of Hamburg\\nmaalej@informatik.uni-hamburg.de\\nDavide Fucci\\nUniversity of Hamburg\\nfucci@informatik.uni-hamburg.de\\nABSTRACT\\nA sofware development screencast is a video that captures the\\nscreen of a developer working on a particular task while explaining\\nits implementation details. Due to the increased popularity of\\nsofware development screencasts (e.g., available on YouTube), we\\nstudy how and to what extent they can be used as additional source\\nof knowledge to answer developer‚Äôs questions about, for example,\\nthe use of a speciÔ¨Åc API. We Ô¨Årst diÔ¨Äerentiate between development\\nand other types of screencasts using video frame analysis. By\\nusing the Cosine algorithm, developers can expect ten development\\nscreencasts in the top 20 out of 100 diÔ¨Äerent YouTube videos. We\\nthen extracted popular development topics on which screencasts are\\nreporting on YouTube: database operations, system set-up, plug-in\\ndevelopment, game development, and testing.\\nBesides, we found six recurring tasks performed in development\\nscreencasts, such as object usage and UI operations.\\nFinally, we conducted a similarity analysis by considering only\\nthe spoken words (i.e., the screencast transcripts but not the text\\nthat might appear in a scene) to link API documents, such as the\\nJavadoc, to the appropriate screencasts. By using Cosine similarity,\\nwe identiÔ¨Åed 38 relevant documents in the top 20 out of 9455 API\\ndocuments.\\nKEYWORDS\\nDevelopment screencasts, API reference documents, Similarity anal-\\nysis\\n1\\nINTRODUCTION\\nSofware development is a knowledge-intensive work [1‚Äì4] in\\nwhich developers spend a substantial amount of their time look-\\ning for information [3]‚Äîe.g., how to Ô¨Åx a bug or how to use an\\nAPI. Tey access and share knowledge through various media and\\nsources, including API documentation [5], Q&A sites, wikis, or\\ntutorials [1, 6, 7]. Regardless of how rich or popular a single knowl-\\nedge source might be, it barely satisÔ¨Åes all the information needs\\nof a speciÔ¨Åc developer within a certain context [1, 4, 8].\\nNowadays, there is a growing trend to use videos instead of text\\nto capture and share knowledge [9]. Video content, from movies\\nto webinars and screencasts, accounts for more than half of the\\ninternet traÔ¨Éc1. Sofware developers are concerned with this trend\\nas they are using more and more video resources in their job [10, 11].\\n1htp://www.cisco.com/c/en/us/solutions/collateral/service-provider/\\nvisual-networking-index-vni/complete-white-paper-c11-481360.html\\nIn particular, development screencasts are geting popular among\\ntechnical bloggers2 and on general purpose video-sharing platforms\\nsuch as YouTube.\\nA screencast is a ‚Äúdigital movie in which the seting is partly or\\nwholly a computer screen, and in which audio narration describes\\nthe on-screen action‚Äù [7]. In particular, a development screencast\\nis created by a developer to describe and visualize a certain devel-\\nopment task [10]. Screencasts are more comprehensive than plain\\ntext since they capture, in the form of video and audio, human\\ninteraction[12]‚Äîe.g., following the instruction of a developer.\\nYouTube3 does not yet oÔ¨Äer the possibility to explicitly search\\nfor a development screencast that explains how to accomplish a\\nspeciÔ¨Åc development task [6, 13] in a certain development context\\n[13, 14].\\nMoreover, there is a lack of understanding about the diÔ¨Äerent\\ntypes of videosÔ¨Å!i.e., development screencast [10] cannot be dis-\\ntinguished from other types of videos.\\nIn a development screencast, a sofware developer performs a\\ntask which can be assigned to a topic and to a speciÔ¨Åc context\\n[10, 14], such as an IDE, a web browser or a virtual machine. Tere\\nare recurring tasks performed in several development screencasts\\nand in diÔ¨Äerent development contexts that require the consultation\\nof API documents[5, 15]. Te text transcript of the screencast audio\\ncontains searchable and indexable technical terms that can refer to\\nother artifacts, such as an API or a tool. For example, the screencast\\npresented in Figure 1 can be extended with an API document‚Äîas\\nshown in Figure 2‚Äîsince it contains references to classes, methods\\nand other units.\\nBased on the mentioned observations, we will tackle the follow-\\ning research questions in this paper:\\n(1) RQ1: Is it possible to distinctively identify a developer\\nscreencasts from other video types based on a frame anal-\\nysis?\\n(2) RQ2: Which development tasks are performed in sofware\\ndevelopment screencasts?\\n(3) RQ3: Can a development screencast be extended with rel-\\nevant API reference documents by considering only the\\nspoken words?\\nIn particular, in Section 1, we evaluate diÔ¨Äerent algorithms (Jac-\\ncard, Cosine & LSI) and their performance in identifying develop-\\nment screencasts by simply considering video frames. In Section\\n2htp://www.virtuouscode.com/a-list-of-programming-screencast-series/\\nhtps://www.rubytapas.com/new-list-programming-screencast-series/\\n3htps://support.google.com/youtube/answer/4594615?hl=en\\narXiv:1707.08824v1  [cs.SE]  27 Jul 2017\\nFigure 1: Example of a development screencast on YouTube.\\nIt contains a video (screencast), a title describing the sof-\\nware development task, and a transcript.\\n2, we use the visualization techniques introduced by Sievert et al.\\n[16] and Chuang et al. [17] to identify the sofware development\\ntopics and the recurring development tasks present in development\\nscreencasts. In Section 3, we analyse the similarity between a task\\nperformed in the screencast and the relevant API documents using\\nthe TaskNav tool [15]. Section 5 discusses the results, while Section\\n6 concludes the paper and describes future work.\\nA development screencast is a special type of video which cannot\\nbe directly searched on YouTube due to the lack of a pre-deÔ¨Åned\\ncategory. Nonetheless, a development screencast is characterized\\nby the small number of scenes, length, and the speciÔ¨Åc actions\\n(e.g., inputing text) performed by a developer [10]. Moreover, in\\ntheir screencasts, developers use several tools (e.g., and IDE or code\\neditor, a terminal, a browser) to perform a development task. In this\\nsection, we present how we used the information available on the\\nvideo frames to distinguish a development screencast from other\\ntypes of videos.\\nWe sampled a set of frames (i.e., a rectangular raster of pixels)\\nfrom diÔ¨Äerent videos and compared their stability and variation.\\nWe deÔ¨Åne the similarity between two frames as Sim(f1, f2). Te\\nframe similarity of a video is calculated by\\nn√ç\\n1\\nSim(fn,fn+1)\\nn\\nwith n as\\nthe number of analyzed frames. Frame fn+1 is the direct successor\\nof frame fn and fn , fn+1. For each video, we sampled a frame\\nevery 10 seconds.\\nWe randomly selected 100 YouTube videos associated to one of\\nthe following types:\\n‚Ä¢ Development screencast (n=20): videos showing sof-\\nware development activities in several programming lan-\\nguages, such as PHP, Python, Java, SQL and C#. DiÔ¨Äerent\\nFigure 2: Example of an API reference document. It contains\\nclass and method deÔ¨Ånitions as well as the descriptions of it.\\ntools (e.g., an IDE, code editor, or simple a web browser)\\nare used to perform a task.\\n‚Ä¢ Non-development screencasts (n=20): videos showing\\nthe desktop of a user solving problems unrelated to sof-\\nware development, including mathematical problems, game\\ntutorials, or sofware utilization.\\n‚Ä¢ Non-development, non-screencast (n=20): videos show-\\ning how to perform a task not related to sofware devel-\\nopment (e.g., learn Spanish, or how-to change a phone\\nscreen) in which a computer screen is not recorded.\\n‚Ä¢ Others (n=40)4: videos in none of the above categories\\n(e.g., a music video). Tis set contains 40 videos because\\nmost of them had a short length (2-3 minutes).\\nTe sample contains approximately 2000 frames for each video\\ntype. Every frame contains a particular number of color informa-\\ntion that changes in the diÔ¨Äerent scenes throughout the developer\\nscreencast‚Äîfor example, when using an IDE, a web browser or a\\nterminal.\\nTe similarity between two frames was calculated using the Jac-\\ncard coeÔ¨Écient, Cosine similarity, and LSI. Each color information\\nper pixel is considered a bag of words [18]. Te Jaccard coeÔ¨Écient\\nis used to measure the similarity between two sets of data. Te car-\\ndinality of the intersection is divided by the cardinality of union of\\nthe sets [19]. Te similarity value of the Jaccard coeÔ¨Écient ranges\\nbetween 0 and 1. If the documents ¬Æta and ¬Ætb contain the same set\\nof objects, the coeÔ¨Écient is one (or zero in case the documents\\ndo not have objects in common). Te similarity between the two\\n4Provided from the owner of htp://randomyoutube.net\\n2\\nFigure 3: Frame similarity of development screencasts com-\\npared to other video types (using cosine similarity values).\\ndocuments ¬Æta and ¬Ætb is\\nSIMJ ( ¬Æta, ¬Ætb) =\\n| ¬Æta ¬∑ ¬Ætb |\\n| ¬Æta|2 + | ¬Ætb |2 ‚àí¬Æta ¬∑ ¬Ætb\\nTe Cosine approach is commonly used for a similarity comparison\\nof documents [20],[21],[22]. Documents are converted into term\\nvectors to compute their Cosine similarity, which is quantiÔ¨Åed as\\nthe angle between these vectors and ranges between 0 and 1[19].\\nFinally, the LSI ranges between -1 and +1; it uses term frequen-\\ncies for dimensionality reduction, followed by a singular value\\ndecomposition (SVD)[23]. We use the Cosine and LSI algorithms to\\nevaluate the frequency of scene switches in a video. Te Jaccard\\nalgorithm is more sensitive than Cosine and LSI as the later two\\nonly recognize a low number of scene switches and moving objects\\n(mouse, keyboard, etc.) used in the development screencasts.\\nTe analysis of 2127 frames from 20 development screencasts\\nshows that the values of Cosine and LSI are close to 1.0, indicating\\nthat, in a development screencasts, there is only a small number of\\nscene switches. Te Jaccard similarity has an average value of 0.768,\\nshowing that small objects are moved. We analyzed the similarity\\ndistributions of the four sets of videos using each algorithm.\\nTe highest concentration of similar values for the development\\nscreencasts can be calculated using the Cosine algorithm (see Figure\\n3). For the Jaccard algorithm, the characteristics of the distribution\\nvaries a lot, making it diÔ¨Écult to identify a developer screencast\\nfrom other types of video. Te LSI algorithm has similar distribu-\\ntions. On the other hand, the Cosine algorithm shows a higher\\nconcentration, particularly for Developer Screencasts. Tus, it is\\nbeter suited to distinguish developer screencasts from other video\\ntypes.\\nWe identiÔ¨Åed 20 development screencast within other video\\ntypes (n = 100) by using the Cosine algorithm. We calculated the\\nframe similarity of every video type and ranked the videos based on\\ntheir Cosine similarity (in descending order). All developer screen-\\ncasts are correctly predicted until the Ô¨Årst 45 recommendations\\ndue to the high concentration of similarity values for development\\nscreencasts with respect to other video types. Within a list of 20\\nvideos, we could identify 55% of the development screencasts. In\\nother words, developers can expect to correctly identify over ten\\ndevelopment screencast in a list of 20 diÔ¨Äerent YouTube videos\\nwith the support of the Cosine algorithm. (precision = 0.028, recall\\n= 0.55, and F1 = 0.052 [24]).\\nTo answer RQ1, we found that the Cosine algorithm is beter\\nsuited to distinguish development screencast from other types of\\nvideo due to its capability of beter concentrating similarity values.\\n‚Ä¢ Development screencasts are diÔ¨Äerent from other\\ntypes of videos. Development screencasts seem to\\nbe more static‚Äîi.e., they have less scenes and objects.\\n‚Ä¢ Te Cosine algorithm is the best, among the studied\\nalgorithms, at identifying a development screencast\\nfrom other video types (highest concentration of sim-\\nilarity values).\\n‚Ä¢ All development screencasts could be identiÔ¨Åed\\nwithin the Ô¨Årst third of the retrieved items.\\n2\\nTOPICS OF SOFTWARE DEVELOPMENT\\nSCREENCASTS\\nIn this section, we analyze the sofware development topics of the\\ntask performed during a development screencast. To this end, we\\nanalyzed the title of the screencast as well as its audio transcripts\\nand assigned the task performed on screen to diÔ¨Äerent sofware\\ndevelopment topics, such as implementation or system set-up.\\nTable 1: Topics of and within Java screencasts.\\nTopic label\\nMost relevant terms\\n6 Topics of development screencasts (tasks performed in SD according to their titles)\\ndatabase operation with Java\\nnetbean, database, create, mysql\\ndatabase operation with Android\\nclass, table, key, android\\nsystem set-up\\nrun, make, Window, JDK\\nplug-in development\\nconnect, jframe, constructor, jbuton\\ngame development\\ngame, develop, object, implement\\ntesting\\nselenium, use, program, Ô¨Åle, write, learn\\n6 Topics within development screencasts (repeatable tasks performed in SD according to the transcripts)\\nAPI usage (Object/Classes)\\nuse, create, class, code, method, click, type\\nFiles\\nÔ¨Åle, create, call, time, program\\nLists\\nlist, move, get, create\\nUI operations\\nbox, Ô¨Åle, slider, inputs\\nMethods\\nproperty, get, input, statement\\nSystem operations\\nprogram, time, system, get\\nDue to its popularity, we focused on development tasks per-\\nformed using the Java programming language5. In particular, we\\nsearched for ‚Äúhow-to‚Äù development screencasts6. Terefore, the\\nsearch string used to retrieve relevant videos from YouTube was\\n‚ÄúJava + How to‚Äù. Our dataset includes 431 Java development screen-\\ncasts; for all videos a transcript is available. We used the Python\\ntoolkit pyLDAvis [16, 17, 26] to identify the topics of sofware de-\\nvelopment in which the task are performed. Using the toolkit, it is\\npossible to visualize diÔ¨Äerent sofware development topics and to\\ncluster them according to a varying number of LDA topics.\\nWe started by removing from the text all the special characters,\\nnumbers and the term ‚ÄúJava‚Äù which interferes with our analysis.\\nWe tuned the number of LDA topics until we reached a set of\\nnon-overlapping clusters that have enough distance between each\\nother (see Figure 4). We also modiÔ¨Åed the relevance metric Œª until\\nwe found the most relevant terms for a topic of sofware develop-\\nment We perform two diÔ¨Äerent analyses of sofware development\\n5htp://stackoverÔ¨Çow.com/tags\\n6How-tos are also among the most requested on Stack OverÔ¨Çow [25]\\n3\\nFigure 4: Topics of Java sofware development topic in rela-\\ntion to other topics.\\ntopics found in sofware development screencasts. In the Ô¨Årst anal-\\nysis, we consider only the titles of the screencast to understand\\nwhich sofware development topics is associated with the task per-\\nformed in the screencast. Te second analysis considers the textual\\ntranscript of the development screencasts. We only consider the\\nnouns‚Äîextracted using the NLTK library [27]‚Äîsince including also\\nverbs caused the algorithm (LDA) to overÔ¨Åt. Te output of this\\nstep was inaccurate because some verbs were included in addition\\nto nouns. We listed them in Table 1 because we believe that they\\nmight be useful for interpreting the overall tasks performed during\\nthe screencast.\\nTable 1 summarizes the topics we found in the titles and in the\\ntranscripts of the development screencasts. Figure 4 shows the\\nclusters of all the topics within the chosen sofware development\\nscreencasts. We stopped searching for the best number of topics\\nwhen the topic clusters did not overlap anymore or when the topics\\nbecame not visible. Te size of the clusters represents the impor-\\ntance of the topic within the overall set of topics. Figure 5 shows\\nthe distribution of terms used to derive the topic of a task.\\nDatabase-related operations are some of the most popular topics\\ndiscussed in developers screencasts. Similarly, the database man-\\nagement system MySQL is one of the most popular topic discussed\\non StackOverÔ¨Çow7. Tis observation could indicate the need for a\\nsystem to support database operations in the IDE. Tutorials [11] as\\nwell as FAQs [28] provide a Ô¨Årst entry to start developing a certain\\nsystem.\\n7htp://stackoverÔ¨Çow.com/tags\\nFigure 5: Distribution of terms for a sofware development\\nscreencast topic.\\nPlug-in installation is also discussed in development screencasts.\\nTis topic can extend traditional tutorials as they provide knowl-\\nedge for similar development tasks8. We observed some niche topic,\\nsuch as game development, discussed in development screencasts.\\nSofware testing, a frequent sofware engineering activity[29], is\\nalso covered in sofware development screencasts. Tis might re-\\nveal the need for screencasts that teach how to test sofware [30].\\nTe use of a method, objects, or class in Java is a frequently occur-\\nring topic that could be augmented by API reference documents.\\nIn particular, list operations one of the most commonly occurring\\ntasks showing the importance of this data type with respect to\\nsimilar ones, such as hash-maps. Finally, UI operations are also\\nshown to be one of the main activities.\\n‚Ä¢ Database operations are popular development tasks\\nperformed in development screencasts.\\n‚Ä¢ Testing‚Äîa\\nconventional\\ntask\\nin\\nsofware\\ndevelopment‚Äî is also performed in development\\nscreencasts.\\n‚Ä¢ Sofware development tasks such as methods and\\nclasses usage, are taught in sofware development\\nscreencast.\\n‚Ä¢ An advanced search that considers the transcripts of\\na sofware development screencast can help Ô¨Ånding\\ntasks that matches the development context.\\n8htp://www.vogella.com/tutorials/EclipsePlugin/article.html\\n4\\n3\\nANALYSIS OF SIMILARITY TO API\\nDOCUMENTATION\\nFigure 6: Method to identify relevant API documents\\nTable 2: Prediction results for 65 relevant documents (pages).\\nTe search space includes 9,455 API documents.\\nTop Documents Retrieved\\nPrecision\\nRecall\\n3\\n18/65\\n0.0514\\n0.30\\n5\\n22/65\\n0.062\\n0.367\\n10\\n33/65\\n0.094\\n0.524\\n20\\n38/65\\n0.0542\\n0.605\\nOur dataset contains 35 randomly selected Java development\\nscreencasts with high-quality transcripts (e.g., no misspelled words).\\nWe identiÔ¨Åed 1-3 relevant API reference documents for every devel-\\nopment task that was performed in a development screencast (see\\nresearch method in Figure 6). Initially, we used TaskNav‚Äîa popu-\\nlar tool for identifying relevant API documents based on natural\\nlanguage[15]‚Äîto Ô¨Ånd the relevant API document for a development\\ntask. Te input parameter for this tool is a phrase (i.e., the title of\\nthe screencast) describing a certain development task. In several\\noccasions, we could not Ô¨Ånd more than one relevant document\\nbecause the screencast titles were not self-explanatory (for exam-\\nple, ‚ÄúJava How To: Dialog Boxes‚Äù or ‚ÄúHow to make a Tic Tac Toe\\ngame in Java‚Äù), and a deeper look into the development screencast\\nand its transcript was ofen required. Terefore, we qualitatively\\nevaluated the recommendations of the API documents by deÔ¨Ån-\\ning documents as relevant if they contain the same classes (e.g.,\\nArrayList) or method signatures (e.g., boolean contains(Object o))\\nmentioned in the screencasts as well as additional useful informa-\\ntion needed when repeating the development tasks (e.g., implement\\nArrayList, LinkedList). We could identify 65 relevant documents\\nfrom 9,455 potential candidates.\\nFor the automatic identiÔ¨Åcation of the relevant development\\nscreencasts, we have used the Cosine algorithm. We calculated the\\nCosine similarity value for each transcript of a developer screencast\\nand each of the 9,455 Java API documents in the dataset which\\nresulted in a ranked list of API documents ordered by their similarity\\nvalues. For the evaluation of the recommendations, we calculated\\nprecision and recall [24] (as identiÔ¨Åed by TaskNav using manual\\nchecking) within the top three, Ô¨Åve, 10, and 20 Cosine positions (see\\nTable 2). Precision shows the percentage of relevant documents\\nidentiÔ¨Åed within a predeÔ¨Åned list, whereas recall shows how many\\nrelevant documents were identiÔ¨Åed from all the relevant ones within\\nthe same list.\\nFor the best three retrieved results, we found that the transcripts\\nfrequently and clearly mention technical terms, such as class and\\nmethod names contained in an API documentation page. Precision\\nvaries between 5 and 10%, with the best result being yielded by the\\ntop-10 retrieved pages. Table 2 shows that more than 50% of the\\nrelevant documentation pages were found in the top-10 retrieved\\npositions. Te percentage increases to more than 60 when the top-20\\npositions are considered. Overall, we could Ô¨Ånd 38 out 65 relevant\\ndocuments until the top-20 in a set of 9,455 potential candidates by\\njust analyzing the screencast transcript and ignoring the text that\\nmight appear in a scene (e.g., the source code in the IDE).\\nMoreover, we found that 98.8% of the API documents are below\\na similarity threshold of 0.12 while 55% of the relevant API docu-\\nments are above the same threshold. Considering this threshold\\nwhen searching for relevant API documents can help developers\\nto Ô¨Ånd 55% of the relevant API documents in a list of 114 potential\\ncandidates from the overall corpora of 9,444 documents. Based\\non this results, we believe that development screencasts can be\\nextended using API documents considering only their transcript.\\n‚Ä¢ By comparing only the audio transcript (the screen-\\ncast transcripts but not the text that might appear\\nin a scene, e.g. an IDE) of a development screencast\\nwith the API documentation, we could identify 38\\nout of the 65 relevant API documents in the Ô¨Årst 20\\npositions.\\n‚Ä¢ Tere is a similarity threshold for relevant API doc-\\numents. A high quantity of relevant API documents\\ncan be found above such threshold.\\n4\\nRELATED WORK\\nMacLeod et al.[10] report on the structure and content of develop-\\nment screencasts, as well as the diÔ¨Äerent types of knowledge located\\nin such screencasts. Tey studied how knowledge was presented\\nand used axial coding extract higher-level topics. In our study, we\\nassociated Java screencasts to high-level topics, conducted a frame\\nand a similarity analysis, and discussed how screencasts can be\\nused to enrich API reference documentation\\nTreude et al.[6] discuss how to link StackOverÔ¨Çow answers to\\nthe Java 6 SE SDK API. Tey use the Cosine approach to measure\\nthe similarity and LexRank to evaluate the relevance of the API\\ndocuments. We extend their work by linking screencasts with API\\ndocuments and by showing how similar they are.\\nPonzanelli et al.[31] developed a recommender system to predict\\nrelevant YouTube videos9 for Android development. In addition to\\nthe audio transcripts, they used an OCR tool10 to transfers the actual\\n9htp://codetube.inf.usi.ch/\\n10htps://github.com/tesseract-ocr/tesseract/wiki\\n5\\nvideo information (e.g., slides or subtitles) into text. Tey focus on\\nshowing relevant StackOverÔ¨Çow posts for random YouTube videos.\\nA technique for linking API documentations to code examples\\nis introduced by Subramanian et al. [32] and Chen et. al [33].\\nTey deÔ¨Åned a writen code as a development task for which an\\nAPI reference documentation is needed to get insights about the\\nimplementation.\\n5\\nDISCUSSION AND LIMITATIONS\\nBased on the similarity analysis, we found that frames in a develop-\\nment screencast are much alike in contrast to other types of video.\\nTerefore, an identiÔ¨Åcation of screencasts should be possible by\\nusing algorithms such as the Cosine similarity or LSI without know-\\ning the actual title, tags, or the transcript of the video. Similarly,\\nother types of videos (e.g., recorded interviews or slow motion)\\nare also very static. We acknowledge that such approach, based\\non frames comparison, might mistakenly Ô¨Ånd these other types of\\nstatic videos.\\nTe analysis of the development topics showed that development\\nscreencasts contain knowledge provided in API reference documen-\\ntation. Tus, API reference documents can extend a development\\nscreencast to provide additional implementation details, making it\\nan atractive media for those developers who do not read documen-\\ntation [9]. By leveraging our results, a simple tool‚Äîe.g., based on\\nCosine similarity calculation‚Äîcan suggest relevant documentation\\npages from a large corpus, like the Java SDK documentation, with\\na 61% recall for a list of 20 items.\\nTis preliminary study focuses on screencasts related to a spe-\\nciÔ¨Åc programming language. However, there is a broad range of\\nother development screencasts which tackle the same topics but in\\na diÔ¨Äerent manner, or which use diÔ¨Äerent programming languages\\nwith diÔ¨Äerent syntax, semantics or speciÔ¨Åc tools. Terefore, devel-\\nopment screencasts might diÔ¨Äer according to the tools used, or to\\nthe sofware engineering activities and phenomena.\\nTe selection of the dataset might thus have inÔ¨Çuenced the study\\nresults. We used the title to understand the tasks performed in a\\ndevelopment screencast, and the transcripts to understand its sub-\\ntasks. Tose two elements (i.e., titles and transcripts) complement\\neach other. For example, if a developer wants to know how to use\\nlists, Ô¨Åles and methods in a programming language like Java she\\nmight search them through an algorithm that considers the tran-\\nscripts. In this way, the developers can Ô¨Ånd tasks that match the\\ndevelopment context of interest, such as speciÔ¨Åc IDEs or libraries.\\nWe found that UI operations‚Äîone of the most important activity\\nperformed when comprehending sofware [34]‚Äîare also largely\\nperformed in development screencasts. By watching screencasts,\\ndevelopers can understand how other developer debugged and\\nsolved similar problems.\\nTe transcripts we obtained might miss important terms, or\\ninclude misspelled ones. Tis can impact the comparison of those\\ntranscripts with the API documentation pages, leading to poor\\nresults. We studied and manually inspected 35 screencasts and\\ntheir transcripts.\\nBuilding a large dataset using the YouTube API poses some\\nlimitations since they only returns a limited number of search\\nresults11. Tus, multiple searches, with diÔ¨Äerent search terms, need\\nto be performed. Moreover, the persistence of retrieved data is not\\nguaranteed due to the possible deletion of the videos included in\\nour sample.\\nTe library we used, could not completely identify and remove\\nthe verbs or stop words from the title or the transcripts. Terefore,\\na replication of this study could lead to diÔ¨Äerent results. We rec-\\nommend to use the NLTK and the pyLDAvis library to pre-process\\nthe titles and the transcripts as well as to summarize the topics of\\nthe tasks. Although diÔ¨Äerent people from diÔ¨Äerent countries might\\ncreate developments screencasts, we did not evaluate the language\\nquality of the screencasts which might also inÔ¨Çuence our results.\\nWhen performing a development task there is ofen the need\\nfor additional information to be gathered‚Äîfor example, from Stack\\nOverÔ¨Çow, YouTube or an API documentation. Combining all of\\nthem mean to use diÔ¨Äerent types of information to perform a de-\\nvelopment task.\\nWe conclude that sofware development screencasts can help\\ndevelopers to search for recurring development tasks in a speciÔ¨Åc\\ncontext (e.g., within an IDE) independently from the topic of sof-\\nware development.\\n6\\nCONCLUSION AND FUTURE WORK\\nWe analyzed diÔ¨Äerent development screencasts on YouTube and\\nfound six main topics for the Java programming language.\\nA sofware development screencast is a particular type of video\\nin which developers perform a tasks by focusing on relevant tools.\\nDevelopment screencasts are not much diÔ¨Äerent from other types\\nof screencasts. We found that frame similarity can be used to detect\\na development screencast on YouTube. Development screencasts\\ncan be extended by API documents to bete support sofware devel-\\nopers. We found that more than half of the relevant API documents\\ncould be provided within a list of 20 items. A Cosine comparison\\nbetween a screencast and a large API documentation corpus is\\nonly a preliminary, simple approach to oÔ¨Äer developers the most\\nrelevant documents.\\nTis paper provided a Ô¨Årst insight on how to categorize and\\nidentify development screencasts, and how to enrich them with\\nAPI documentation. A further extension of our approach will focus\\non extracting the content of the development screencast‚Äîe.g., the\\ncode showed on the screen when using an IDE‚Äîto reach a higher\\nprecision/recall when identifying development screencast.\\nTere is also further work needed to determine the diÔ¨Äerent\\ntypes of knowledge [5, 10] located in screencasts to achieve a more\\nÔ¨Åne-grained and precise mapping between the API reference doc-\\numentation and the API elements within the IDE. Tis approach\\nmight require labeling every unique piece of knowledge within a\\nscreencast and use video and image features. We believe that the\\ncommunity needs to study which types of screencasts are useful\\nfor which developers in which situations.\\nREFERENCES\\n[1] W. Maalej, R. Tiarks, T. Roehm, and R. Koschke, ‚ÄúOn the comprehension of pro-\\ngram comprehension,‚Äù ACM Transactions on Sofware Engineering and Methodol-\\nogy, vol. 23, pp. 31:1‚Äì31:37, Sept. 2014.\\n11htp://stackoverÔ¨Çow.com/questions/25918405/youtube-api-v3-page-tokens\\n6\\n[2] J. Sillito, G. C. Murphy, and K. De Volder, ‚ÄúAsking and answering questions\\nduring a programming change task,‚Äù IEEE Transactions on Sofware Engineering,\\nvol. 34, no. 4, pp. 434‚Äì451, 2008.\\n[3] A. J. Ko, B. A. Myers, M. J. Coblenz, and H. H. Aung, ‚ÄúAn exploratory study of\\nhow developers seek, relate, and collect relevant information during sofware\\nmaintenance tasks,‚Äù IEEE Transactions on sofware engineering, vol. 32, no. 12,\\n2006.\\n[4] T. Fritz and G. C. Murphy, ‚ÄúUsing information fragments to answer the questions\\ndevelopers ask,‚Äù in Proceedings of the 32nd ACM/IEEE International Conference on\\nSofware Engineering-Volume 1, pp. 175‚Äì184, ACM, 2010.\\n[5] W. Maalej and M. P. Robillard, ‚ÄúPaterns of knowledge in api reference documen-\\ntation,‚Äù IEEE Transactions on Sofware Engineering, vol. 39, no. 9, pp. 1264‚Äì1282,\\n2013.\\n[6] C. Treude and M. P. Robillard, ‚ÄúAugmenting api documentation with insights\\nfrom stack overÔ¨Çow,‚Äù in Proceedings of the 38th International Conference on Sof-\\nware Engineering, pp. 392‚Äì403, ACM, 2016.\\n[7] J. Udell, ‚ÄúWhat is screencasting - o‚Äôreilly media.‚Äù htp://archive.oreilly.com/pub/\\na/oreilly/digitalmedia/2005/11/16/what-is-screencasting.html, November 2005.\\n(Accessed on 11/01/2016).\\n[8] W. Maalej, ‚ÄúTask-Ô¨Årst or context-Ô¨Årst? tool integration revisited,‚Äù in Proceedings\\nof the 2009 IEEE/ACM International Conference on Automated Sofware Engineering,\\npp. 344‚Äì355, IEEE Computer Society, 2009.\\n[9] T. C. Lethbridge, J. Singer, and A. Forward, ‚ÄúHow sofware engineers use doc-\\numentation: Te state of the practice,‚Äù IEEE sofware, vol. 20, no. 6, pp. 35‚Äì39,\\n2003.\\n[10] L. MacLeod, M.-A. Storey, and A. Bergen, ‚ÄúCode, camera, action: How sofware\\ndevelopers document and share program knowledge using youtube,‚Äù in Proceed-\\nings of the 2015 IEEE 23rd International Conference on Program Comprehension,\\npp. 104‚Äì114, IEEE Press, 2015.\\n[11] R. Tiarks and W. Maalej, ‚ÄúHow does a typical tutorial for mobile development\\nlook like?,‚Äù in Proceedings of the 11th Working Conference on Mining Sofware\\nRepositories, pp. 272‚Äì281, ACM, 2014.\\n[12] I. Laptev, M. Marszalek, C. Schmid, and B. Rozenfeld, ‚ÄúLearning realistic human\\nactions from movies,‚Äù in Computer Vision and Patern Recognition, 2008. CVPR\\n2008. IEEE Conference on, pp. 1‚Äì8, IEEE, 2008.\\n[13] W. Maalej, M. Ellmann, and R. Robbes, ‚ÄúUsing contexts similarity to predict\\nrelationships between tasks,‚Äù Journal of Systems and Sofware, 2016.\\n[14] W. Maalej and M. Ellmann, ‚ÄúOn the similarity of task contexts,‚Äù in Proceedings of\\nthe Second International Workshop on Context for Sofware Development, pp. 8‚Äì12,\\nIEEE Press, 2015.\\n[15] C. Treude, M. Sicard, M. Klocke, and M. Robillard, ‚ÄúTasknav: Task-based naviga-\\ntion of sofware documentation,‚Äù in Sofware Engineering (ICSE), 2015 IEEE/ACM\\n37th IEEE International Conference on, vol. 2, pp. 649‚Äì652, IEEE, 2015.\\n[16] C. Sievert and K. E. Shirley, ‚ÄúLdavis: A method for visualizing and interpreting\\ntopics,‚Äù in Proceedings of the workshop on interactive language learning, visualiza-\\ntion, and interfaces, pp. 63‚Äì70, 2014.\\n[17] J. Chuang, C. D. Manning, and J. Heer, ‚ÄúTermite: Visualization techniques for\\nassessing textual topic models,‚Äù in Proceedings of the International Working Con-\\nference on Advanced Visual Interfaces, pp. 74‚Äì77, ACM, 2012.\\n[18] X. Wang and E. Grimson, ‚ÄúSpatial latent dirichlet allocation,‚Äù in Advances in\\nneural information processing systems, pp. 1577‚Äì1584, 2008.\\n[19] A. Huang, ‚ÄúSimilarity measures for text document clustering,‚Äù in Proceedings of\\nthe sixth new zealand computer science research student conference (NZCSRSC2008),\\nChristchurch, New Zealand, pp. 49‚Äì56, 2008.\\n[20] M. Ahasanuzzaman, M. Asaduzzaman, C. K. Roy, and K. A. Schneider, ‚ÄúMining\\nduplicate questions in stack overÔ¨Çow,‚Äù in Proceedings of the 13th International\\nConference on Mining Sofware Repositories, pp. 402‚Äì412, ACM, 2016.\\n[21] J. G. Conrad, X. S. Guo, and C. P. Schriber, ‚ÄúOnline duplicate document detec-\\ntion: signature reliability in a dynamic retrieval environment,‚Äù in Proceedings of\\nthe twelfh international conference on Information and knowledge management,\\npp. 443‚Äì452, ACM, 2003.\\n[22] S.-T. Park, D. M. Pennock, C. L. Giles, and R. Krovetz, ‚ÄúAnalysis of lexical sig-\\nnatures for Ô¨Ånding lost or related documents,‚Äù in Proceedings of the 25th annual\\ninternational ACM SIGIR conference on Research and development in information\\nretrieval, pp. 11‚Äì18, ACM, 2002.\\n[23] R. Mihalcea, C. Corley, and C. Strapparava, ‚ÄúCorpus-based and knowledge-based\\nmeasures of text semantic similarity,‚Äù in AAAI, vol. 6, pp. 775‚Äì780, 2006.\\n[24] M. P. Robillard, W. Maalej, R. J. Walker, and T. Zimmermann, Recommendation\\nsystems in sofware engineering. Springer, 2014.\\n[25] C. Treude, O. Barzilay, and M.-A. Storey, ‚ÄúHow do programmers ask and answer\\nquestions on the web?: Nier track,‚Äù in Sofware Engineering (ICSE), 2011 33rd\\nInternational Conference on, pp. 804‚Äì807, IEEE, 2011.\\n[26] pyLDAvis, ‚ÄúPython library for interactive topic model visualization,‚Äù 2014.\\n[27] S. Bird, E. Klein, and E. Loper, Natural language processing with Python: analyzing\\ntext with the natural language toolkit. ‚Äù O‚ÄôReilly Media, Inc.‚Äù, 2009.\\n[28] I. Timmann, ‚ÄúAn empirical study towards a quality model for faqs in sofware\\ndevelopment,‚Äù tech. rep., 2015.\\n[29] J. Singer, T. Lethbridge, N. Vinson, and N. Anquetil, ‚ÄúAn examination of sofware\\nengineering work practices,‚Äù in CASCON First Decade High Impact Papers, pp. 174‚Äì\\n188, IBM Corp., 2010.\\n[30] T. Shepard, M. Lamb, and D. Kelly, ‚ÄúMore testing should be taught,‚Äù Communica-\\ntions of the ACM, vol. 44, no. 6, pp. 103‚Äì108, 2001.\\n[31] L. Ponzanelli, G. Bavota, A. Mocci, M. Di Penta, R. Oliveto, B. Russo, S. Haiduc,\\nand M. Lanza, ‚ÄúCodetube: extracting relevant fragments from sofware devel-\\nopment video tutorials,‚Äù in Proceedings of the 38th International Conference on\\nSofware Engineering Companion, pp. 645‚Äì648, ACM, 2016.\\n[32] S. Subramanian, L. Inozemtseva, and R. Holmes, ‚ÄúLive api documentation,‚Äù in\\nProceedings of the 36th International Conference on Sofware Engineering, pp. 643‚Äì\\n652, ACM, 2014.\\n[33] C. Chen and K. Zhang, ‚ÄúWho asked what: Integrating crowdsourced faqs into api\\ndocumentation,‚Äù in Companion Proceedings of the 36th International Conference\\non Sofware Engineering, pp. 456‚Äì459, ACM, 2014.\\n[34] W. Maalej, R. Tiarks, T. Roehm, and R. Koschke, ‚ÄúOn the comprehension of pro-\\ngram comprehension,‚Äù ACM Transactions on Sofware Engineering and Methodol-\\nogy (TOSEM), vol. 23, no. 4, p. 31, 2014.\\n7\\n\\n\\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n1 \\n\\'I Updated the <ref>\\': The Evolution of References in the \\nEnglish Wikipedia and the Implications for Altmetrics \\n \\nZagovora, Olga*, Ulloa, Roberto, Weller, Katrin, Fl√∂ck, Fabian \\n \\n* corresponding author: olga.zagovora@gesis.org \\n \\nWith this work, we present a publicly available dataset of the history of all the references (more \\nthan 55 million) ever used in the English Wikipedia until June 2019. We have applied a new method \\nfor identifying and monitoring references in Wikipedia, so that for each reference we can provide \\ndata about associated actions: creation, modifications, deletions, and reinsertions. The high \\naccuracy of this method and the resulting dataset was confirmed via a comprehensive crowdworker \\nlabelling campaign. We use the dataset to study the temporal evolution of Wikipedia references as \\nwell as users‚Äô editing behaviour. We find evidence of a mostly productive and continuous effort to \\nimprove the quality of references: (1) there is a persistent increase of reference and document \\nidentifiers (DOI, PubMedID, PMC, ISBN, ISSN, ArXiv ID), and (2) most of the reference curation \\nwork is done by registered humans (not bots or anonymous editors). We conclude that the evolution \\nof Wikipedia references, including the dynamics of the community processes that tend to them \\nshould be leveraged in the design of relevance indexes for altmetrics, and our dataset can be pivotal \\nfor such effort. \\n \\nKeywords: altmetrics, Wikipedia references, edit histories, data quality, dataset, Wikipedia \\neditors \\n \\n1. Introduction \\n1.1. Wikipedia references and their challenges for altmetrics \\nWikipedia incorporates one of the largest reference repositories in existence. This is primarily due \\nto its guidelines strongly encouraging that all content have to be verifiable, which is mostly \\nachieved by providing a pointer to a reliable source that supports content added to the article text.1 \\nThus, Wikipedia articles usually include reference lists; and overall, the English Wikipedia \\ncontains more than 55 million references.2 Cited sources can be different types of publications, \\nincluding for example formally published scientific papers, books, and news media articles, but \\nalso links to websites or any other type of Web documents (Lewoniewski et al., 2017).  \\n \\nThese references are exposed to an enormous readership, as Wikipedia is accessed by a wide \\naudience around the world. With more than 300 million page views per day for the English \\n \\n1 https://en.wikipedia.org/wiki/Wikipedia:Verifiability  \\n2 According to our dataset as of June 2019, see details about the dataset in Section 3. \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n2 \\nWikipedia alone, it is one of the top-15 most visited Websites in the world.3 While recent studies \\nseem to indicate that a large number of users do not fully engage with references by visiting links \\nor retrieving the referenced document otherwise (Piccardi et.al., 2020), references can still make \\nstatements more credible simply by appearing alongside them; and they are actively being \\ninteracted with more than 32 million times a month - measured by mouse-hovering over the \\nreference footnote (Piccardi et.al., 2020). \\n \\nThus, Wikipedia‚Äôs references have a tremendous impact on its millions of readers - who encounter \\nthem while browsing serendipitously or while actively researching a topic. In addition, Wikipedia \\ncontent, including its references, is incorporated into other data sources and projects, and thus \\nreaches even wider audiences. For instance, Wikipedia content is used as a source for the \\ncollaborative knowledge base WikiData4, which is also used by other platforms. Scholia5, for \\ninstance, creates scholarly profile pages based on WikiData.  \\n \\nApart from its appeal to the general public, Wikipedia has also become an object of interest in the \\nfield of altmetrics, an area of research dedicated to studying ways of measuring the impact of \\nscientific work outside of traditional scholarly citation schemes, and often based on social media \\ninteractions (Priem et al., 2010; Kousha & Thelwall, 2017). In this context, the value ascribed to \\nWikipedia as a data source is that it provides an immense repository of literature curated by a large \\neditor community. And with the self-control mechanisms and guidelines applied within this \\ncommunity, Wikipedia references are expected to meet basic quality standards. At the very least \\nthey are presumed to be topically relevant and ideally to represent a comprehensive, up-to-date and \\nbalanced collection of the most relevant sources. Given the dynamic nature of Wikipedia, it might \\nalso be possible to opportunely detect novel and trending publications through the additions and \\nchanges to the community-created repository of references. Overall, being cited in a Wikipedia \\narticle is considered to be an indicator of some form of impact for a (scientific) publication (Kousha \\n& Thelwall, 2017).  \\n \\nIndeed, Wikipedia data is already considered in altmetrics data implementations (and sold) by \\naggregators in the field. Currently the most prominent are Altmetric.com6, PlumX7, CrossRef8, and \\nLagotto9. Their indicators are applied in different settings such as publishers‚Äô sites or repositories \\n \\n3 https://tools.wmflabs.org/siteviews/?sites=en.wikipedia.org, https://www.alexa.com/topsites, as of 15.02.2020 \\n4 https://www.wikidata.org/ \\n5 https://tools.wmflabs.org/scholia/ \\n6 https://www.altmetric.com/explorer/  \\n7 ttps://plumanalytics.com  \\n8 https://www.crossref.org/ \\n9 http://www.lagotto.io/docs/api/ \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n3 \\n(e.g. institutional or discipline-specific publication databases), and they are used to advertise \\n‚Äúimpactful‚Äù publications. These metrics vary substantially, as aggregators apply different modes \\nof accessing and collecting the data, and there is no standard for detecting or aggregating references \\non Wikipedia (and several other platforms). Although it can be assumed that data collection is to a \\nconsiderable degree based on standard document identifiers such as DOIs10 (Haustein, 2016). \\nHowever, the specific procedures are not transparent and, thus, altmetrics aggregators have to be \\nviewed as black boxes that could be subject to manipulations (Kousha & Thelwall, 2017), such as \\nresearchers adding references to their own publications into Wikipedia articles11, or even strategic \\ncampaigns to insert publications from a specific publisher into Wikipedia articles12. \\n \\nTo illustrate some of the challenges in using Wikipedia references as reliable indicators, we will \\ntake a closer look at a particular example publication and how it is referenced in English Wikipedia, \\nas identified by our extraction method and dataset (see Section 3). Our example is based on several \\nreferences to the publication ‚ÄúRoy et al. (2001) Structure and function of south-east Australian \\nestuaries. Estuarine, Coastal and Shelf Science 53(3): 351‚Äì384.‚Äù on different article pages. The \\nfirst reference to this publication was added to a Wikipedia article in August 2012 (Figure 1, blue \\nline), i.e. more than ten years after the paper‚Äôs release. Nine months later (1st of June 2013), there \\nwere already 53 articles that referenced this publication, all of them done by the same editor (id: \\n7739861). However, none of the references included its existing Digital Object Identifier (DOI). \\nThe corresponding DOI to this publication was added to the existing Wikipedia references during \\nthe first quarter of 2014 (Figure 1, orange line), and this was mostly done by one single editor in \\nMarch 2014 (id: 203434). In November 2018, another editor (id: 15881234) removed 27 instances \\n(50%) of the references although some of them were quickly reinstated. \\n \\n10 \\nFor \\nexample, \\nAltmetric.com \\nis \\ncollecting \\ndata \\nusing \\nthe \\nfollowing \\nidentifiers \\nhttps://help.altmetric.com/support/solutions/articles/6000234171-how-outputs-are-tracked-and-measured, \\nand \\nCrossRef is collecting using DOI and landing page URLs https://www.crossref.org/services/event-data/  \\n11 Wikipedia‚Äôs guidelines about Conflict of Interest include a section on ‚ÄúCiting yourself‚Äù, which allows self-citations \\nwithin certain boundaries. see https://en.wikipedia.org/wiki/Wikipedia:Conflict_of_interest. To the best of our \\nknowledge, there are no studies that investigate in detail how common self-citations are in Wikipedia or that aim to \\nidentify misconduct in the area of self-promoting scientific articles through Wikipedia.  \\n12One \\nexample \\ncan \\nbe \\nfound \\nat: \\nhttps://web.archive.org/web/20200323131800/https://annualreviewsnews.org/2020/02/25/seeking-a-wikipedian-in-\\nresidence/    \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n4 \\n \\nFigure 1 - References in the English Wikipedia for one example paper, as identified by our \\napproach (blue line) and by approaches based only on document identifiers (orange line).  Areas \\nhighlighted by circles correspond to edits made by one specific Wikipedia user (see editor ID): the green \\ncircle indicates an editor adding instances of the reference without any document identifier, violet circles \\nrepresent different editors who modified existing references (e.g., by adding a DOI) and red for editors \\nwho deleted references from articles. \\n  \\nDespite the widespread popularity and importance of Wikipedia and the practical role it plays in \\napplied altmetrics, this basic example illustrates several issues that motivated our work. First, it \\nhighlights a weakness of mining references based only on document identifiers (orange line), that \\nlead us to create an alternative method that uses the entire text of the reference (blue line); the \\nformer misses the reference for the first two years of its existence. Second, it shows the impact that \\na single editor can have on the visibility of a reference by systematically adding or removing it \\nfrom different articles. Third, it exposes the lack of understanding about Wikipedia editors as the \\ncreators and curators of Wikipedia and their impact of references being implemented. Together \\nwith the intransparency of aggregated altmetrics indicators, it exposes a current gap in our \\nunderstanding of the nature and quality of Wikipedia references in altmetrics.  \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n5 \\nAt the same time, the example captures the value of our investigation as an important step to close \\nthis gap. It suggests that the origin and evolution of references inside Wikipedia articles can \\ndisclose anomalies in the activity around Wikipedia references and that many of the collaborative \\nnegotiation processes that govern the inclusion, modification, and deletion of references are \\nencoded within article revisions, and therefore can reveal information about the editor community \\nresponsible for the maintenance of this valuable asset. \\n \\n1.2 Research focus and contributions \\nWith this in mind, we outline the specific research areas which our research questions and \\ncontributions fall into.  \\n \\nInsights into reference evolution over time. We are interested in the development of references in \\nWikipedia from a longitudinal perspective. The ongoing transformation and expansion of \\nWikipedia content affects the potential (measured) impact of cited sources by constantly increasing \\nor decreasing the number of instances that reference them, either by internal changes in the articles \\nor by introduction and deletion of entirely new articles. Therefore, Wikipedia presents a scenario \\nthat is very different from other settings in citation analysis in areas of bibliometrics and altmetrics. \\nIn bibliometrics based on citations from traditional publication outlets, both (1) alterations to \\nexisting references lists and (2) retractions of entire articles are rare events (Shema et al., 2019). \\nTraditional citations typically produce growing numbers that are then incorporated into different \\nbibliometric indicators (citations add up as more publications that cite a source are published and \\nalmost none of them disappear again).  \\n \\nIn contrast to bibliometrics, the altmetrics field has to deal with fluid types of data sources, as they \\ninclude dynamic material13 such as tweets or Facebook posts that might be deleted or modified. \\nWikipedia adds another challenge. While many other altmetric sources are based on collective \\napproaches in which individuals act on their own and indicators merely cumulatively aggregate \\nthese individual activities, Wikipedia as a collaborative system relies on a consensus between \\nmembers that can take time to reach an equilibrium, and which might be perturbed again as new \\ninformation becomes available. References may be added by one person, removed by another, and \\nthen re-inserted or edited again. These processes can happen multiple times, and little is known \\nabout how this has affected Wikipedia‚Äôs references in the past and how many editing activities are \\nperformed on references overall.  \\n  \\n \\n13 While in fact, social media content containing altmetrics indicators (e.g., Facebook posts) is deleted to some extent \\nafter the initial altmetrics detection, we are not aware of aggregators‚Äô metrics that take these deletions into account. To \\nthe best of our knowledge, most aggregators are removing only deleted Tweets as per terms of Twitter data usage.  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n6 \\nThis leads to our first research question: (RQ1) How do Wikipedia references evolve over time? \\nWe examine the fluctuation of all references of Wikipedia by analyzing the number of actions \\nperformed on them. To the best of our knowledge, we provide the first longitudinal study of the \\nevolution of references across all revisions in the English Wikipedia by also considering all \\nreferences - not only those that include a standard document identifier (DID), such as the Digital \\nObject Identifier (DOI).  \\n \\nFor practical reasons, altmetrics indicators often rely on DIDs for the detection of publications, \\nleading to the question: (RQ2) What is the current and past coverage of references that include \\nDIDs? The references that lack DIDs are simply missed by methods that rely solely on them, and \\ntheir visibility is therefore decreased. We will tackle this question by estimating, at different points \\nin time, the proportion of references that include DIDs, and by using current knowledge from our \\n2019 dataset to calculate which references lacked DIDs in the past.  \\n \\nInsights into the editors of Wikipedia references. We are interested in getting a better \\nunderstanding of who adds, modifies, or deletes Wikipedia references. Currently learning more \\nabout the people who actually produce the social media contents that are then considered sources \\nfor altmetrics is in its beginnings (Holmberg, 2015; Imran et al., 2018). For example, little is known \\nabout who actively tweets, blogs, and posts about science - or incorporates references as sources \\nfor encyclopedic articles. \\n \\nWe, therefore, set out to answer RQ3: Who creates and maintains Wikipedia references, and in \\nwhich way? This question pertains, on one hand, to which parts of the Wikipedia editor base engage \\nin different reference-related activities, e.g., occasional users, very active registered editors, \\nunregistered users, or automated bots. On the other hand, discovering patterns of interaction with \\nreferences exhibited by  editors that tend to references, such as focussing their actions mostly on \\nreference maintenance as opposed to only adding references occasionally as part of other writing \\nefforts.  This more fine-grained picture of possible roles of editors in the reference ecosystem can \\nhelp to understand the editor community that is responsible for the activity around the Wikipedia \\nreferences. \\n \\nA comprehensive reference dataset based on edit histories. Addressing these and future research \\nquestions becomes only viable with a novel dataset (Zagovora et al., 2020) of individual revision \\nhistories of all Wikipedia references14 ever created in the English Wikipedia until June 2019 \\ntogether with information about editorship. We created the dataset by leveraging WikiWho15 \\n \\n14 ‚Äúreference‚Äù defined as the content included inside a Wikipedia <ref> tag \\n15 https://www.wikiwho.net/  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n7 \\n(Fl√∂ck & Acosta, 2014), a service that tracks the additions, changes, and reinsertions of words \\n(tokens) written in Wikipedia (see Section 3 for details). With our new method, we are able to track \\nthe types of actions performed on references as well as the editors that contribute and maintain \\nreferences in the English Wikipedia. We evaluated our dataset with crowdworkers and \\ndemonstrated its high accuracy, despite our method not relying on any types of document \\nidentifiers for tracking references to reach the maximum coverage.  \\n \\nWe find that the quality of Wikipedia references is in continuous improvement based on constant \\nactivity, including focussed efforts to add DIDs, and the fact that registered humans (as opposed to \\nbots or anonymous editors) are mainly responsible for the curation of the references. Nonetheless, \\nwe believe that the reliance on DIDs is not currently sufficient to capture all relevant publications \\ncited in Wikipedia, and the specialization of editors in certain actions (e.g. only adding or deleting \\nreferences) deserves more attention to discard the possibility of exploitative behaviour.  \\n \\nNotwithstanding the pitfalls of an open, collaborative system where little control exists over the \\ncontent, the historical record of Wikipedia can be used to improve methods of mining and ranking \\nthe relevance of references. We recommend that altmetrics indicators should leverage the \\nWikipedia revisions to decrease manipulations, increase coverage, and assign impact based on past \\nactivity and the editor community that surrounds each reference. Our dataset could be pivotal in \\ndeveloping such improvements.  \\n \\nThe rest of this paper is organized as follows: Section 2 will offer an overview of the related work \\nrelevant for Wikipedia references and altmetrics, Section 3 is dedicated to the description of \\nmethods to build the dataset, Section 4 presents a gold standard dataset that is used for the \\nevaluation of our references dataset, Section 5 presents general statistics of the Wikipedia \\nreferences and main findings regarding our research questions, and Section 6 will conclude and \\nsummarize our findings. \\n \\n2. Related work \\nThe most comparable dataset to the one we provide is presented by Halfaker et al. (2019) and Redi \\n& Taraborelli (2018). They also include some form of historical data about references in \\nWikipedia. However, their work differs from our approach as the authors relied on the presence of \\nstandardized DIDs as part of the reference and thus were (1) not capturing all references, and were \\n(2) assigning editors and timestamps of origin to references according to the Wikipedia revision in \\nwhich the identifier was included, even if in fact the reference as such was created earlier (cf. Figure \\n1). Lastly, (3) modifications and deletions done to the references after the inclusion of the \\nidentifiers were not tracked. While the dataset has been publicly shared with the interested \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n8 \\ncommunity and was used, e.g., to study topics of citations, to the best of our knowledge no-one has \\nyet used it to study the evolution of references or editing behaviour related to references.  \\n \\nOther works only provide static (non-historical) snapshots of references in Wikipedia language \\neditions, like Nielsen (2008)16 or Singh et al. (2020), that were created for specific tasks. Nielsen \\n(2008) used the ‚Äúcite journal‚Äù template from references to create a dataset of journal papers that \\nwere cited in Wikipedia pages. This dataset was then used to cluster Wikipedia pages and \\ncorresponding scientific journals into distinct research topics. Singh et al. (2020) created a dataset \\nof references and classified them into 3 groups: journal articles, books, and other Web content.  \\n \\nRecently, research has started to look more closely at how Wikipedia readers interact with \\nreferences. With Wikipedia references being actionable items that users can click on, they have \\nbeen described as a ‚Äúbridge to the next layer of academic resources‚Äù (Grathwohl, 2011). However, \\nrecent studies (Redi, 2018; Piccardi et al., 2020) show that not all references are being equally \\nvisited by Wikipedia readers. Piccardi et al. (2020) conclude that regarding references ‚Äúreaders are \\nmore likely to use Wikipedia as a gateway on topics where Wikipedia is still wanting and where \\narticles are of low quality and not sufficiently informative‚Äù. They found that in the large majority \\nof cases where Wikipedia articles are of high-quality readers do not make use of the references but \\nstay at the Wikipedia article as the ‚Äúfinal destination‚Äù of their information journey (Piccardi et al., \\n2020). This kind of work gives us more insights on the consumer perspective of Wikipedia \\nreferences, which adds to the general perspective of how Wikipedia is used, e.g. how Wikipedia \\narticles are read or how people are citing from Wikipedia articles (Bould et al., 2014; Okoli et al., \\n2014).  \\n \\nTo the best of our knowledge, there are only few studies focusing on editors as the creators of \\nreferences in Wikipedia and thus contributing to the producer perspective. With a comparatively \\nsmall data sample (~5000 articles), Chen & Roth (2012) showed that ‚Äúa reference occurs when a \\nset of committed and qualiÔ¨Åed editors are attracted to the article‚Äù. Huvila (2010) conducted a survey \\nof Wikipedia editors, also including questions broadly related to reference editing. Specifically, the \\nsurvey enabled them to differentiate editors based on their information behaviour and the sources \\nthe editors were using for editing articles. The results indicate a preference for sources that are \\navailable online. There is also some specific, ongoing research on other and more general \\nperspectives on the producer side of Wikipedia, e.g. on who edits Wikipedia, general editing \\npatterns (Fl√∂ck et al. 2017), who becomes a power editor (Panciera et al., 2009), or how editors \\ncollaborate (Kittur et al. 2007; Muriƒá et al, 2019).  \\n \\n \\n16 The dataset is available via  http://hendrix.imm.dtu.dk/services/wikipedia/citejournalminer.html  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n9 \\nFurthermore, in the area of altmetrics research, a certain focus has been placed on untangling the \\nrelations between references in Wikipedia and the scientific publications they are referring to. For \\nexample, altmetrics studies have scrutinized the relevance of scientific publications mentioned on \\nWikipedia (Sugimoto et al., 2017; Kousha & Thelwall, 2017). Shuai et al. (2013) found that papers, \\nauthors, and topics that were used as references on Wikipedia have higher citation counts than \\nthose that were not mentioned. At the same time, only a narrow part of influential works is cited \\non Wikipedia (Kousha & Thelwall, 2017). Nielsen (2007) showed that citations from Wikipedia \\nare correlated with the total number of journal citations, whereas the correlation was weak with the \\njournal impact factor. Yet, according to Nielsen (2007), Wikipedia editors tend to cite articles from \\nhigh impact journals such as Nature, Science, or New England Journal of Medicine. Teplitskiy et \\nal. (2017) conducted a similar experiment with a newer dataset and found that not only impact \\nfactor increases the probability of a paper being mentioned on Wikipedia, but also open access \\nprinciples. According to Mesgari et al. (2015), the quality of content and of referenced sources, in \\nparticular, was one of the major study objects on Wikipedia. For example, Lewoniewski et al. \\n(2017) studied the similarity of sources from different Wikipedia language editions. They found \\nthat URLs in references shared many domain names between language versions, but there were not \\nmany cases of exact matches of URLs in references across languages. Lin & Fenner, (2014) showed \\nthat ecology and evolution are better covered with references from PLOS than other subjects. \\nNevertheless, these results might not show the full picture while references were reported as \\nincomplete and accompanied by the lack of standardization (Pooladian & Boorego, 2017). \\n \\nThe altmetrics community is interested in, for example, learning more about whether being cited \\nin Wikipedia articles indicates that a scientific publication has an impact beyond academia into the \\nbroader public (Lin & Fenner, 2013; Thelwall, 2016). Lin & Fenner (2013) argue that Wikipedia \\nreferences might capture a ‚Äúdiscussion‚Äù group, one of the engagement types with research \\npublications. We provide some first insights in this description together with descriptive statistics \\nthat we hope will inspire more detailed reflections on how to interpret different types of reference \\nediting behaviour patterns. This will have to be translated into a broader discussion about how the \\naltmetrics community wants to define the impact of a Wikipedia reference (considering its edit \\nhistory), e.g. when reporting Wikipedia citation counts for a specific publication. For example, our \\ndataset can enable a finer analysis of the revisions of references that can help to detect potential \\ndisruptions (e.g. sudden appearance of the same reference across various articles, or highly active \\nindividual editors who are responsible for large numbers of new references).  \\n \\nOur work may thus contribute to the theoretical value of being cited by a Wikipedia article related \\nto another area of altmetrics research focused on understanding the quality of data obtained from \\naggregators. For example, Zahedi & Costas (2018) and Ortega (2018) have started to compare \\ndifferent altmetrics aggregators to illustrate potential challenges for data quality. Differences start \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n10 \\nwith coverage by aggregators. In the context of Wikipedia, this means that references appearing on \\nWikipedia make up from 2% of publications tracked by Altmetric.com up to 5.1% of those tracked \\nby Lagotto. Those differences are due to the aggregator‚Äôs methodology and the datasets of \\npublications they are tracking (Zahedi & Costas, 2018). These studies also observe different mean \\nvalues for how often publications are mentioned on Wikipedia: publications in the Altmetric.com \\ncollection are on average included by 1.7 Wikipedia pages, publications in the Lagotto collection \\nare on average cited by 2.9 Wikipedia pages, and publications in CrossRef Event Data are on \\naverage cited by 15.7 Wikipedia pages (Zahedi & Costas, 2018). We assume that these wide \\ndifferences are not only due to the diverse sets of publications covered by the aggregators but also \\ndue to their distinct methods of tracing Wikipedia references that are prone to various errors \\nconsidering the challenges inherent to Wikipedia data. Besides the difficulties of keeping track of \\ncontinuous changes in Wikipedia where references may be modified or removed, one important \\nerror source is the reliance on standard document identifiers to trace publications (Ortega, 2018). \\nNevertheless, even other approaches that rely on title and first author name (Kousha & Thelwall, \\n2017) may fail to extract mentions from incomplete references (Pooladian & Borrego, 2017). Given \\nthe quality of our dataset, it has the potential to serve as an external base for comparing different \\ndata collection approaches used by altmetrics aggregators, giving them the opportunity to increase \\ntheir coverage and impact indexes by looking at different points in time of the revision history.   \\n \\n \\n3. Creating the reference histories dataset \\nWe will first introduce our dataset that underlies the following analyses. The dataset is based on \\nthe revisions of all articles in the English Wikipedia edition since its origin until June 2019 and \\ncontains the change history of all individual references. References, i.e., pointers to external \\nsources (which may be any type of document, including academic and non-academic publications), \\nare inserted into Wikipedia in a standardized way. They appear as ‚Äúinline citations‚Äù17 in the main \\nbody of the article, formatted by <ref> ‚Ä¶ </ref> markers in Wiki markup. Based on these ref \\ntags, they can be identified inside the main text and Wikipedia also uses them to create the reference \\nlists at the end of the article. For our work we consider all inline citations marked by ref tags.18  \\nTypical examples of reference formats would be the following:  \\n \\n17 The Wikipedia community utilizes the term ‚Äúinline citation‚Äù, which broadly speaking corresponds to the ‚Äúin-text \\ncitation‚Äù as known from bibliometrics.  See more details here https://en.wikipedia.org/wiki/Wikipedia:Inline_citation \\n18 References are generally created with Wiki markup language, by adding <ref> tags around the source.  Additionally, \\nsome references can be added automatically by dedicated templates. We are not considering materials that are not \\nreferenced as inline citations (e.g., publications from the ‚ÄúAdditional reading‚Äù section) as the guidelines recommend \\nto \\ninclude \\nreferences \\nvia \\n<ref> \\ntags \\n(inline \\ncitations) \\nas \\nthe \\nstandard \\n(https://en.wikipedia.org/wiki/Wikipedia:Citing_sources). \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n11 \\n‚óè <ref>Clark 1971, p.18</ref> \\n‚óè <ref>Clark, Ronald W. (1971). Einstein: The Life and Times. \\nISBN 0-380-<44123-3</ref> \\n‚óè <ref name=ronald>{{cite book |last=Clark |first=Ronald \\n|title=Einstein: The Life and Times |publisher= |date=1971 \\n|page=18 |url= }}</ref>. \\n  \\nIn the following subsections, we (3.1) highlight the idiosyncrasies of Wikipedia and its references \\nand the key challenges in tracing the revision histories of individual references from articles \\nrevision histories. We then (3.2) describe our solution in detail, and (3.3) explain how we extract \\ndocument identifiers (DIDs) for the references. \\n \\n3.1 Wikipedia Articles: Revisions, References and Tokens \\nThe main content corpus of the Wikipedia encyclopedia is organized in articles. Each article A \\nconsists of an ordered list of revisions ùëÖ, i.e. ùê¥= [ùëÖ0, . . . , ùëÖùëõ], where each revision is a new version \\nof the text which was contributed by editor ùëí at timestamp ùëß. Inside the Wiki markup text, \\nreferences are added inline, immediately after the facts they support and are displayed as footnotes \\nat the bottom of the Wikipedia article in a dedicated section. They are bracketed by  <ref> ‚Ä¶ \\n</ref> tags. For the front-end HTML representation, the ref tags are converted by a Wikitext \\nparser into footnote references with an appropriate order.   \\n \\nThe revision history of a reference is given by the article revisions in which it was added, or edited, \\neither in its entirety or partially. Edits can be performed by registered Wikipedia user accounts \\n(including automated scripts/bots), or through non-registered sessions represented via an ‚Äì mostly \\ndynamic ‚Äì IP address (see Subsection 5.2 for a more detailed typology of editors). As each revision \\nwithin an article is associated with exactly one editor e, so is each creation or change action \\nperformed on a specific reference through that revision.  \\n \\nThe revision history of a reference within a specific Wikipedia article starts with the reference‚Äôs \\nfirst creation. After a reference is created, it can be modified, e.g., by correcting the name of an \\nauthor. A reference can also be deleted, and then be reinserted in its entirety after deletion. \\nGenerally, a considerable part of editing activity in article revisions concerns the deletion and \\nreinsertion of entire content sequences, for example in edit disputes (Fl√∂ck et al. 2017).  \\n \\nIdentifying the specific revisions in which the above-described changes are applied to a given \\nuniquely identified reference in Wikipedia presents two major challenges.  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n12 \\n1. Tracking changes of any particular target sequence in a long text, and specifically \\nWikipedia articles, can be error-prone if either multiple other changes have been applied in \\nthe same revision and/or the sequence in question was moved to a remote part of the \\ndocument, which happens regularly in Wikipedia (Fl√∂ck & Acosta, 2014). In these \\ninstances, common text difference algorithms (such as employed by standard Mediawiki \\ninstances or text mining tools) can lose track of sequences and erroneously assign them as \\nnew content or as deleted.19  \\n2. Even if all concrete tokens (i.e., strings that are either words or combinations of \\nalphanumeric characters) constituting a reference are correctly tracked, deciding if a \\nreference is identical to another reference in two consecutive article revisions is non-trivial, \\nif either a majority of the tokens have been replaced or altered or if key tokens have been \\nchanged (e.g., the content of the ‚Äútitle‚Äù or ‚Äúyear‚Äù fields). This might indicate either a small \\ncorrection to an existing reference or, in this example,  its replacement with a new edition \\nof the underlying publication, which is not identical from a bibliometric point of view. \\n \\nFor the second issue, text similarity measurement (e.g., cosine similarity) based on strings of tokens \\nmight be an apparent choice for a potential solution. However, within the limited scope of a \\nWikipedia article, its references could be very similar in their text strings due to the recurrence of \\ntechnical terms that are germane to the article‚Äôs topic. The intrinsic structure of references, that \\nfollows standardized formats may further accentuate the perceived and measured similarity of text \\nstrings, making it likely to have two similar but distinct references being considered as the same \\none. For example, the reference \\'Darwin (1859). On the origin of species\\' has less formal similarity \\nto (i) \\'Darwin, Charles (1859). On the origin of species by means of natural selection.  London: \\nJohn Murray\\' than to (ii) \\'Crawford (1859). (Review of) On the origin of species\\'  - and yet (i) \\ncorresponds to the same reference, but (ii) does not. \\n \\nTo address these issues, we take advantage of WikiWho, an approach that solves the change \\nattribution problem at a token level with over 95% accuracy (Fl√∂ck & Acosta, 2014). Each token \\never inserted in an article has been assigned a token ID that uniquely identifies it through all \\nrevisions. WikiWho uses the relative location of tokens in sentences and paragraphs to accurately \\ntrack each of them between every pair of revisions. Figure 2 illustrates the allocation of WikiWho \\ntoken IDs for the two first revisions of an article.  \\n \\n \\n19 Cf. Wikipedia diffs (https://en.wikipedia.org/wiki/Help:Diff) and Fl√∂ck & Acosta (2014) for a more general discussion. \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n13 \\n \\na) \\n \\nb) \\nFigure 2 - Examples of token ID assignments by WikiWho before and after an edit. a)  In Revision 1, \\nthe different tokens are identified from 1 to 7. In Revision 2, ‚ÄúBenjamin‚Äù (blue) is inserted and WikiWho \\nassigns a token ID (8). Note how the older instance of ‚ÄúBenjamin\" (ID 6) is tracked as a distinct token, \\nmerely sharing an identical string.  b) In Revision 1, the different tokens are identified from 1 to 7. In \\nRevision 2, ‚ÄúCharles‚Äù and ‚Äúby means of ...‚Äù (blue) are inserted and new token IDs (8-24) are assigned. \\nAnother new reference ‚ÄúCrawford (1859)...‚Äù is added in Revision 2, in a paragraph further below (position \\nnot shown here). Note that new token IDs are assigned despite shared strings with the previous reference. \\nThis is achieved as WikiWho takes the larger context of a changed sequence into account. These toy \\nexamples do not track punctuation for simplicity, while WikiWho does so in practice. \\n \\nAs references are nested inside XML tags inside sentences inside paragraphs, WikiWho is \\nparticularly well suited to uniquely identify tokens of references. This solves our first issue of \\nchange tracking. Secondly, instead of using a string similarity measure, we proceed with the results \\nof WikiWho and apply Jaccard similarity between the token IDs of references to match them \\nbetween revisions, drastically reducing the false-positive cases. We describe this approach in detail \\nin the next Subsection and the evaluation in Section 4.  \\n  \\n \\n \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n14 \\n3.2 Extraction of reference histories \\nOur dataset of references is organized per Wikipedia article. I.e., we do not ‚Äì for this work ‚Äì match \\nreferences across articles. Formally, for each article A, the dataset contains a list of tuples \\nùêªùëì= [< ùëéùëì0, ùë°ùëì0, ùëüùëì0, ‚Ñéùëì0, ùëíùëì0, ùëßùëì0 >, . . . , < ùëéùëìùëõ, ùë°ùëìùëõ, ùëüùëìùëõ, ‚Ñéùëìùëõ, ùëíùëìùëõ, ùëßùëìùëõ>] that represents the history of \\nactions ùëéùëìùëñ(‚Äòcreation‚Äô, ‚Äòinsertion‚Äô, ‚Äòdeletion‚Äô or ‚Äòreinsertion‚Äô) performed over reference ùëì, where:   \\n‚óè ùë°ùëìùëñ is the list of WikiWho token IDs that were part of the reference in revision ùëüùëìùëñ \\n‚óè ‚Ñéùëìùëñ is a hash value calculated of over ùë°ùëìùëñ, \\n‚óè ùëíùëìùëñ is an editor that performed an action ùëéùëìùëñ at time ùëßùëìùëñ, and \\n‚óè ùêªùëìis sorted according to time ùëßùëì, ùëßùëì0is the oldest reference. \\nTo build this dataset, we first mine all inline citations of all Wikipedia revisions using the WikiWho \\ntoken IDs that correspond to the string tags <ref>...</ref> and <ref name= \\n‚Ä¶>...</ref>; the void tags, i.e. the one-sided tags (<ref name=... />) are excluded \\nbecause they correspond to duplications of existent references. For each revision ùëÖùëñ in each article \\nùê¥, we then have a list of references that belong to that revision ùê∫ùëñ= [ùëì0, . . . , ùëìùëö] where each \\nreference ùëìùëó is a tuple < ùë°ùëó, ‚Ñéùëó, ùëíùëó, ùëßùëó>.  \\nThe next step is to associate the references in ùê∫ùëñ to those in ùê∫ùëñ+ùëò, so that two references are added \\nto ùêªùëìif they are equivalent, i.e., referring to the same publication.  \\nIn trivial cases, a reference ùëì does not change between article revisions so we use the hash values \\nto match all identical references across all G, and we store the matched references of ùëì in ùêªùëì. For \\nnow, each ùêªùëì is incomplete as there could be two references histories ùêªùëìand ùêªùëîthat belong together \\nbecause with this procedure even a small modification is enough to change the hash value. \\nTherefore, all actions ùëéùëì are tagged as ‚Äòunknown‚Äô. \\nIn the nontrivial cases, the references have been modified between two consecutive revisions. We \\nthen rely on a combination of Jaccard similarity between the lists of WikiWho token IDs of \\nreferences in H and G. The following procedure was applied for each reference ùëì:  \\n1. (Creation) Select the oldest tuple  < ùëüùëìùëú, ‚Ñéùëìùëú, ùëéùëìùëú, ùëíùëìùëú, ùë°ùëìùëú, ùëßùëìùëú> in ùêªùëì. Search ùëÖùëó in ùê¥ that \\ncorresponds to ùëüùëì0 and switch the value of ùëéùëì0 from ‚Äòunknown‚Äô to ‚Äòcreation‚Äô. Go to Step 2. \\n2. (No action) If the successor revision of ùëüùëìùëñ is also in ùêªùëì, i.e. ùëüùëìùëñ+1 = ùëÖùëó+1, then: \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n15 \\n1. If hash ‚Ñéùëìùëñ of reference fi is the same as hash ‚Ñéùëìùëñ+1, i.e., ‚Ñéùëìùëñ= ‚Ñéùëìùëñ+1, then remove \\ntuple < ùëüùëìùëñ+1, ‚Ñéùëìùëñ+1, ùëéùëìùëñ+1, ùëíùëìùëñ+1, ùë°ùëìùëñ+1, ùëßùëìùëñ> (no action occurred between ùëüùëìùëñ and \\nùëüùëìùëñ+1, i.e. they have the same hash). \\n2. Otherwise,  switch the value of ùëéùëìùëñ+1 from ‚Äòunknown‚Äô to ‚Äòmodified‚Äô. \\n3. Go to Step 2 using the next oldest tuple < ùëüùëìùëñ+1, ‚Ñéùëìùëñ+1, ùëéùëìùëñ+1, ùëíùëìùëñ+1, ùë°ùëìùëñ+1, ùëßùëìùëñ+1 > in \\nùêªùëì. \\n3. (Modification) Otherwise, we identify a list of candidate references inùëÖùëó+1, i.e. reference \\nhashes that did not exist in revisions before ùëÖùëó+1. A reference is a candidate c if ùëüùëê0=ùëÖùëó+1,  \\nwhere < ùëüùëêùëñ+1, ‚Ñéùëêùëñ+1, ùëéùëêùëñ+1, ùëíùëêùëñ+1, ùë°ùëêùëñ+1, ùëßùëêùëñ+1 >inùêªùëê. \\n1. For each candidate reference c, we calculate the Jaccard similarity between its \\ntokens ùë°ùëê and  ùë°ùëìùëñ. \\n2. We select the candidate with the highest Jaccard similarity, given that it is higher \\nthan 0.2 (see Supplementary materials for the threshold selection process). \\n3. If all candidates have a similarity lower than 0.2, then we check if there is a \\ncandidate in which ùë°ùëê‚äÇùë°ùëìùëñ, i.e. all tokens of the previous reference are reused. This \\ncondition targets short references that are extended by many tokens in the new \\nrevision leading to very low Jaccard similarity.  \\n \\n \\n4. If a candidate is selected, then add the tuple < ùëÖùëó+1, ‚Ñéùëê, ‚Ä≤ùëöùëúùëëùëñùëìùëñùëíùëë‚Ä≤, ùëíùëê, ùë°ùëê,  ùëßùëê> to \\nùêªùëì. Then merge ùêªùëì and ùêªùëê and go to Step 2 using that tuple. \\n5. Otherwise, if no candidate is selected, go to Step 4.  \\n4. (Deletion) Insert the tuple < ùëüùëìùëñ+1, ‚Ñéùëìùëñ, ‚Ä≤ùëëùëíùëôùëíùë°ùëíùëë‚Ä≤, ùëíùëìùëñ+1, [], ùëßùëìùëñ> to ùêªùëì and go to Step 5.  \\n5. (Reinsertion)  Move to the next revision in ùê¥, i.e. let ùëÖùëó be ùëÖùëó+1.   \\n1. If there exists ùëüùëìùëñ in ùêªùëì, such that ùëüùëìùëñ= ùëÖùëó+1; then switch the value of ùëéùëìùëñ from \\n‚Äòunknown‚Äô to ‚Äòreinserted‚Äô and go to Step 2 using < ùëüùëìùëñ, ‚Ñéùëìùëñ, ùëéùëéùëñ, ùëíùëìùëñ, ùë°ùëìùëñ, ùëßùëìùëñ> in ùêªùëì.  \\n2. Otherwise, use steps 3.1, 3.2, and 3.3, in order to select a candidate in ùëÖùëó+1 that \\nmatches ùë°ùëìùëñ.  \\n3. If a candidate c is selected, then add the tuple < ùëÖùëó+1, ‚Ñéùëê, ‚Ä≤ùëüùëíùëñùëõùë†ùëíùëüùë°ùëíùëë‚Ä≤, ùëíùëê, ùë°ùëê, ùëßùëê> \\nto  ùêªùëì, and merge ùêªùëì with  ùêªùëê and go to Step 2 using that tuple. \\n4. Otherwise, if no candidate is selected (the reference was not yet reinserted). Go to \\nStep 5. \\nAfter this procedure, we obtain four types of actions for each reference: \\n(1) creation, the singular revision in which the reference appears for the first time,  \\n(2) modifications, revisions in which the text of the reference was changed, e.g., adding full names \\nof the authors or introducing a DOI; here we explicitly mean that the cited source of the reference \\nremains the same despite changes in reference presentation, \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n16 \\n(3) deletions, revisions in which the reference was removed from the Wikipedia article, \\n(4) reinsertions, revisions in which the same reference was added again after being removed. \\n \\n3.3 Tracking of DID references \\nThe content of a reference can also contain different types of document identifiers (DID) that have \\nbeen assigned to the referenced source during its publication process, e.g.,  Document Object \\nIdentifier (DOI). DIDs can easily be used to unambiguously trace individual references, both within \\nWikipedia and outside of it. They are also often used to trace references in different contexts in the \\narea of altmetrics. In our approach, we are able to extract and monitor all references in a Wikipedia \\narticle. However, we take a closer look at the subset of references containing DIDs for two reasons: \\nFirstly, this enables comparisons with previous works which have relied exclusively on document \\nidentifiers to extract references for Wikipedia articles. Secondly, Wikipedia includes references to \\npublications that range from strictly refereed and well-reputed scientific outlets to everyday blogs, \\ntwitter profiles, and Reddit posts, and we aim to narrow the focus of our investigation to such \\npublications relevant to altmetrics and the academic community. Although DIDs can be an \\nindicator that a reference is academic20, we are mindful that references with DIDs are not \\nnecessarily academic works. Yet, they provide a viable filter to concentrate on references relevant \\nin the context of this work.  \\n \\nAs one aspect of the evolution of Wikipedia references over time, we look at when DIDs are added \\nto references in the version history. This allows us to estimate how many references are missed by \\napproaches that rely solely on the presence of DIDs for identifying and counting Wikipedia \\nreferences. The missed references comprise not only those that by their nature do not have an \\nidentifier (e.g. a news article) but also those that did not include a DID at the time of the reference \\nextraction although they have been assigned a DID outside of Wikipedia that could be later on \\ninserted.  \\n \\nWe distinguish between several types of references based on DID information (Table 1). The term \\nDID-Reference (DID-R) corresponds to references that by the time of our data collection (June \\n2019) had a DID. If the DID was immediately included when the reference was created, we refer \\nto it as DID-Born Reference (DBorn). Otherwise, if the DID was added after the reference was \\ncreated, we call it DID-Lagged Reference (DLag). Their counterparts, i.e. references that by data \\ncollection date did not have a DID, are called No-DID References. Note that this classification \\ndepends on the time of data collection, as some of the DID-Lagged References would have been \\n \\n20 We use the term ‚Äúacademic‚Äù instead of ‚Äúscientific‚Äù to indicate the inclusion of all works not only from ‚Äúharder‚Äù \\nsciences but also from social sciences and humanities. This is in line with Halfaker et al. (2019).   \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n17 \\nclassified as No-DID References in previous years and current No-DID References may still \\nreceive a DID at a future point in time.  \\n \\nTable 1. Types of references according to if and when a DID was added. The first and second column \\nindicate the names that we use to identify the type and subtype of reference respectively. The third column \\ndescribes the subtype of references based on when the DID was added. \\nType \\nSubtype \\nDescription \\nDID Reference \\n(DID-R) \\nDID-Born \\nReference (DBorn) \\nReferences that already included a DID when they were created. \\nDID-Lagged \\nReference (DLag) \\nReferences that did not include a DID when they were created, but were \\nassigned a DID at a later point, before the time of our data collection. \\nNo-DID Reference \\n(No-DID) \\n \\nReferences that did not include a DID by the time of data collection. \\nThese might receive a DID later (after our data collection), if a DID, in \\nfact, exists for the referenced publication. \\n \\nAfter we rebuild the history of all references for each Wikipedia article as explained in the previous \\nsubsection, we proceed to extract the DIDs on all the versions of each reference. We used modified \\nversions of regular expressions based on  Halfaker et al. (2019) to extract the following DIDs: \\nDocument Object Identifier (DOI), International Standard Book Number (ISBN), PubMed \\nIdentifier (PMID), PubMed Central identifier (PMCID), International Standard Serial Number \\n(ISSN) and arxiv.org Identifiers (ArXiv ID). Once we extract the DIDs, we can retroactively \\nrecognize the DLag references and their content (ùë°ùëìùëñ) as our dataset already contains historical \\ninformation of each reference (ùêªùëì). Our method properly handles cases in which a reference has \\ntwo identifiers (e.g. correction of a DID, or one DOI and one ISBN). We keep the timestamp (ùëßùëìùëñ) \\nand editor (ùëíùëìùëñ) that introduced or modified the DID, so that we can further analyze the dynamics \\nof creation and addition of the DIDs.  \\n \\n4. Evaluation of the reference change tracking method \\nIn this section, we evaluate the performance of our method for tracking version histories for \\nreferences. First, we describe a gold standard dataset that we created for evaluation purposes using \\ncrowdworkers. Then, we present the overall performance. Last, we compare our method to a \\nbaseline relying on cosine similarity. \\n \\n4.1 Gold Standard Dataset \\nTo make sure that our method correctly identifies references in different forms across histories, we \\ncreated a gold standard dataset of 952 pairs of references, each pair similar to the example in Figure \\n2a. The pairs are labelled as Equivalent or Distinct, depending on whether each pair corresponds \\nto the same bibliographical resource or not. Each pair of references were judged by at least three \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n18 \\nFigureEight21 crowdworkers. Each worker indicated if the pair corresponds to the (1) same \\nresource, (2) different resources or (3) if it was not clear. See Appendix A for the instructions we \\nprovided for FigureEight crowdworkers, an example question, and a note on fair payment (Zaldivar \\net al., 2018).  \\n \\nIf the agreement22 between the workers falls below the limit of 0.7, additional persons are assigned \\nuntil the agreement reaches the required limit (0.7), or until at least five workers have made \\njudgments. Prior to the task, each worker was trained using 115 examples that illustrated different \\ncases, and they had to correctly label at least 5 out of 6 test pairs of references. Moreover, all the \\nanswers from a given worker were removed (and a new worker assigned) if their accuracy fell \\nbelow 0.8. Training and test items have been pre-labelled by the authors of this paper.  \\n \\nIn total, 1000 items were presented to the workers, out of which 952 were labelled as either \\nEquivalent or Distinct. We were not able to classify 48 pairs of references because five assigned \\nworkers did not agree above the 0.7 limit. One of the researchers closely inspected these cases23 \\nand confirmed that the low agreement score stemmed from the ambiguity of the items. For example, \\nit \\nis \\nnot \\npossible \\nto \\ndecide \\nbetween \\nthe \\npair \\n<http://www400.sos.louisiana.gov:8090/cgibin/?rqstyp=elcmpct&rqsdta=1021952051300605, \\nhttp://www400.sos.louisiana.gov:8090/cgibin/?rqstyp=elcpr&rqsdta=10050205> \\nas \\nthe \\nreferences contain the same domain but the value of the last URL parameter (‚Äúrqsdta‚Äù) is different, \\nvisiting the URL does not help as the page does no longer exist. \\n \\nCases like the above were included in the workers‚Äô task as we did not want to distort the true state \\nof Wikipedia references. The set of 1000 items was taken using a stratified random sample from \\nall the references in Wikipedia revisions (Appendix B). The set consists of 8 strata with similarities \\nfrom 0 to 1 with 0.125 steps, and 125 pairs of references per stratum. Therefore, we make sure that \\nour sample is representative of the Jaccard similarity scale we used in our method; given the \\naccuracy of WikiWho, we knew a priori that most pairs of references fall into the extreme values \\nof similarity (i.e. 0 or 1).  \\n \\n \\n \\n21 www.figure-eight.com  \\n22  We adopted the ‚Äúconfidence score of the row‚Äù of the Figure Eight platform. This value describes the level of \\nagreement between multiple contributors, where the sum of the contributors‚Äô trust scores of the most common answer \\nis divided by the sum of the trust scores of respondents to that question. See details here  \\nhttps://success.appen.com/hc/en-us/articles/201855939-How-to-Calculate-a-Confidence-Score   \\n23 The inspection was done using contextual information from the text surrounding the references in previous revisions, \\ntesting URLs, and using external resources (e.g. search engines, archive.org). \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n19 \\n4.2 Performance \\nWe compared the 952 pairs of references labelled by the crowdworkers, against the labels assigned \\nusing our method. Figure 3 illustrates the performance metrics for different Jaccard similarity \\nthresholds in our method (see Step 3.3 of Subsection 3.2). Based on this data, we selected a \\nthreshold of 0.2, a good trade-off between precision and recall. At that threshold, the method \\nequally balances the labelling errors between false positives and false negatives. \\n \\n \\nFigure 3 - Performance metrics for identifying equivalent references.  X-axis shows the threshold of \\nJaccard similarity between Wikiwho token IDs and Y-axis shows the Precision (Blue), Recall (Orange), \\nAccuracy (Green), and F1 (Red) scores. \\n \\nTo find the overall performance metrics for our method we resampled our stratified sample so that \\nit is representative of the original distribution of Jaccard similarities (calculated with a 100,000 \\nsample) of pairs of references extracted in the same fashion as described in Subsection 3.2. Table \\n2 presents the micro-average performance metrics for the identification of the same and different \\nreferences between revisions.  \\n \\nTable 2 - Micro-average performance metrics for the labelling of pairs of references. The three \\nmetrics are calculated so that they represent the original distribution of Jaccard similarities in the method \\nby resampling from the stratified sample. The metrics show the micro-average performance, so these are \\nthe expected overall score, in which each evaluated pair of references contributes equally to the score \\n(regardless of the strata they belong to). \\nPrecision \\nRecall \\nF1-score \\n0.96 \\n0.96 \\n0.96 \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n20 \\nUpon careful examination of the 48 cases (in which the crowdworkers could not decide) we found \\nthat in 30 cases our method is able to appropriately decide based on the contextual information that \\nis encoded in the WikiWho data model. Without this context, some reference pairs might be \\nperceived as equivalente. For example, there is an article about a music artist that has several \\nreferences to different music albums from the same platform. By quick inspection of the URL \\nwithout additional context, one might think that it is an equivalent reference. In this case, our \\napproach would differentiate these references if they have been placed at different (relative) \\npositions in the article, despite high string similarity. \\n \\n4.3 Baseline Comparison \\nTo our knowledge, there is no other approach that has mapped references through Wikipedia \\nrevisions, so there is no state of the art method that we can use to compare our method against. \\nTherefore, we implemented a straightforward baseline that maps references using cosine similarity \\nbetween numerical representations of the strings of the Gold Standard reference pairs (via Bag of \\nWords representations) Then we resampled using the distribution of cosine similarities calculated \\nin the original data. To estimate the distribution we used the same procedure of random sampling \\n(Subsection 4.1) but we assume that the buckets have an infinite size (Appendix B, Step 1), and \\nstop the algorithm after 100,000 pairs of references have been sampled. Figure 4 shows how our \\nmethod leveraging WikiWho and Jaccard similarities outperforms the  alternative based on Cosine \\nsimilarity between reference strings through all possible thresholds.   \\n \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n21 \\n \\nFigure 4 - ROC Curves to compare our method and a simple method based on cosine similarity. The \\nlight blue line shows the ROC curve for our method based on Jaccard similarity over WikiWho token IDs, \\nand the orange line the ROC curve for a method based on cosine similarity of strings. Each data point is \\ncalculated for each possible threshold in the sample data. \\n \\n5. Dataset composition and analysis \\nOur dataset contains the references of 6,073,708 non-redirect24 articles in the English Wikipedia. \\nIt comprises 55,503,998 references with 164,530,374 actions. The actions are divided into 33.73% \\ncreations, 31.3% modifications, 23.15% deletions, and 11.81% reinsertions. We find that 77.21% \\nof the articles (4,690,046) have at least one reference (median = 4, Œº = 11.83, max = 12,797). But \\nout of those articles, 78.42% do not yet have any DID-Rs (3.68 million, i.e. 60.54% of total articles; \\nsee Figure 5). The rest of the articles (1,012,289) have at least one DID-R, and 50,615 (5%) articles \\ncontain more than 50%  DID-Rs. More than 88% of the DIDs currently used to track the references \\ncorrespond to ISBNs and DOIs (Figure 6). \\n \\n \\n24 We excluded Wikipedia pages that are redirects. Redirects are Wikipedia pages that automatically send visitors to \\nanother \\npage \\nand \\ndo \\nnot \\nhave \\ntheir \\nown \\ncontent. \\nExample: \\nhttps://en.wikipedia.org/w/index.php?title=Symbiont&redirect=no  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n22 \\n \\nFigure 5 - Distribution of articles according to the presence of references that include a document \\nidentifier (DID References, DID-Rs). \\n \\n \\nFigure 6 - Distribution of DIDs by the type of identifier associated with the references. The graph \\nincludes all the DIDs found in all versions of the references. \\n \\nAs of June 2019, only 7.00% (3,943,984) of all articles include one of the identifiers we were \\ntracking. Figure 7 compares the distributions of all reference counts (left) with the DID-R counts \\n(right) per Wikipedia article, both of them suggesting power law distributions. However, the curve \\nfor all references is more flattened than the one for the DID-Rs.  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n23 \\n \\nFigure 7 - Distribution of reference counts in articles. The X-axis aggregates the number of references \\nper Wikipedia article into 500 bins (left) and 40 bins (rights). The Y-axis shows the number of articles \\nusing a logarithmic scale. \\n \\nAbout 10% of all DID References are DID-Lagged References, i.e. they did not have DIDs in their \\nearly Wikipedia article revisions (Table 1). By now - and in the future - this number will likely be \\nhigher, as DIDs can still be added to the references that were classified as No-DID References in \\nour 2019 dataset. We also observe that 12.1% of actions on the DID References occurred during \\nthe initial revisions in which the references did not yet have a DID; so this information would not \\nbe considered in any approach that relies only on DIDs for identifying and monitoring references.  \\n \\nIn the following section, we will now take a closer look at the data in order to find some answers \\nto our research questions. We will first look at the temporal evolution of different types of \\nreferences (based on the presence of DIDs), and second on the editors who are creating and editing \\nthe references.  \\n \\n5.1 Wikipedia References over Time  \\nThe first reference in an article of the English Wikipedia edition was introduced in December 2005. \\nSince then, more and more references have been added yearly (Figure 8). There was an initial steep \\nincrement of new references per year until 2010, in which more than 4 million references (which \\ncorresponds to 7.4% of all references) were created. After that, the increment of yearly created \\nreferences continued more moderately, and it seems to have settled in 2017 and 2018: about 5.58 \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n24 \\nmillion (10.05%) and 5.64 million (10.15%) of all references were added in the respective years. \\nTo find out if this is a permanent trend, future investigations are needed. \\n \\nAfter references have been created, some of them have never changed in any way, while others  \\nhave been either deleted or modified at least once. According to our data, modifications are the \\nmost common action (~51.5 million) that happen to references after their creation. The number of \\nmodifications per year was not growing monotonically as we have seen for creations, e.g. there is \\na peak of modifications between 2016 and 2018: 6.41 million in 2016, 8.10 in 2017, and back to \\n6.71 in 2018. We suspect that the increase of modifications 2016-2018 is due to WikiCite25 project \\nand sequence of events that started in 2016. \\n \\n \\nFigure 8 - Distribution of actions over time. Each of the four plots depicts the dynamics of one of the \\nactions: creations, modifications, deletions, reinsertions. On the top subplot of each action, bars represent \\nthe number of actions (Y-axis) performed over all references per year (X-axis). For example, around 2.3 \\nmillion references were created in 2007. On the bottom subplot of each action, the solid lines represent the \\nproportion of actions (Y-axis) that occurred yearly (X-axis) for all references. The dashed lines represent \\nthe proportion of actions that occurred yearly (X-axis) for only the DID References (DID-Rs). For \\nexample, around 8.9% of all deletions have been done in 2008, whereas for DID-Rs around 9.9% of \\ndeletions have been done in 2008.  \\n \\nWith the exception of 2005-2006 (years with small reference counts), the number of deletions has \\nshown a decreasing trend until 2014. This was most likely due to clean-up efforts of initial reference \\nadditions, plus high volatility, e.g., because of disagreements, also shown in the high reinsertion \\n \\n25 https://meta.wikimedia.org/wiki/WikiCite_2016  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n25 \\ncounts until 2010, which are by default a reaction to previous deletions.26 Starting at its high count \\nin 2008, the number of reinsertions was unevenly dropping from 2.33 (11.98%) million actions in \\n2008 till 1.39 (7.14%) million actions in 2018. Deletions have dropped since 2015, entailing a \\nconstant reference growth.  \\n  \\nOne might expect the same distribution of actions across years for DID References, i.e., that they \\nwould be treated by editors in the same way as general references. Yet, we can see some differences \\nbetween general references (Figure 8, solid lines) and DID-Rs (dashed-lines). The most distinct \\npatterns are noticeable in the creations and modifications of references (the dashed and solid lines, \\nFigure 8):   \\n‚óè Until 2009 the amount of  creations of DID-Rs was aligned with creations of all references \\n(overlap of the dashed and continued line). However, between 2010 and 2014 fewer (than \\nexpected) DID-Rs were created, and after 2015 the trend was reversed. For instance, in \\n2018, around 11.06% of new DID-Rs (versus 10.15% of general references) have been \\nadded to Wikipedia articles.  \\n‚óè There is no clear trend in the modifications of DID-Rs (the second plot from the left, Figure \\n8), as the plot shows multiple peaks and troughs across the years. We observe fewer \\nmodifications of DID-Rs in 2007-2009, 2013, 2017, and 2018; and more modifications in \\n2012 and 2014-2016. The highest number of modifications have been reached in 2016 (1.02 \\nmillion actions or 14.79%) and 2017 (0.9 million actions or 13.03%).  \\n‚óè The relatively small differences in deletions of some years (2008, 2010-2012, 2014, and \\n2015 in Figure 8) do not necessarily mean that their presence ended in those years (since \\nthey can be reinserted). \\n \\nWe found that DID-Rs have a higher survival rate: they are deleted (without further reinsertions) \\nat a lower rate than the rest of the references. As of June 2019, around 31.8% (17.02 million) \\nreferences have been deleted (without further reinsertions) between 2005 and 2019, 0.97 million \\nof them are DID-Rs (i.e. 25.7% of all DID-Rs). Figure 9 presents the cumulative percentage of \\ndeleted references for each year, we observe that the percent of deleted references grew from ~20% \\nin 2007 to ~32% in 2019 (and from ~11% to ~26% for DID-Rs correspondingly). This speaks to a \\nhigher value of these references to the editor community, possibly because of their perceived \\nquality.  \\n \\n26 The years 2006-2010 in the English Wikipedia have been pointed out as a highly volatile period before (Fl√∂ck et al. \\n2017) \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n26 \\n \\nFigure 9 - Percentage of deleted references. The orange line shows the percentage of DID-Rs  that were \\ndeleted (w/o further reinsertions) before any given year in the X-axis;  the blue line shows the percentage \\nfor all references. For example, if we would analyse Wikipedia references as of 1 January 2010, we would \\nobserve that 27.8% of all references and 18.2% of DID References have been deleted without being \\nreinserted before June 2019.  \\n \\nWe observed in Figure 8 (second subplot from the left) that there are clear differences in the overall \\nnumber of modifications, and the number of modifications of DID references. Some of these \\nmodifications are of particular interest because they are the ones in which DIDs are added to \\nalready existing references (by definition, references that were classified as DLag in Table 1). \\nTherefore, we have closely investigated these modifications (Figure 10). We observed that the \\nhighest peaks of newly added DIDs occurred during: (1) May and June 2008 with 22,126 DIDs \\nadded during two months, (2) May 2014 with 18131 DIDs added, and (3) February 2019 with \\n12,486 DIDs added. Based on information until June 2019, these three peaks correspond, \\nrespectively, to (1) 19-26%, (2) 17%, and (3) 56% of references that at the time should have had a \\nDID (see Appendix E for statistics of other peaks). Putting it the other way around, 44-83% of \\nreferences remained without DID even after pronounced waves of DIDs additions. \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n27 \\n \\nFigure 10 - Monthly total of modifications that added a DID to existing references. The X-axis displays \\nthe year and y-axis the amount of modifications. Only modifications in which a DID was added to a \\nreference are considered. \\n \\nThe latter percentages will be even higher in the future (after June 2019) as more DIDs will be \\nadded to references that existed at those peaks. Hence, we also analyzed how long it takes for the \\nreference to be attributed with DIDs. Figure 11 presents the distribution of time span between \\nreference creation and DID introduction for references created in three different years. In 2006, it \\ntook between 500 and 1000 days for most of the references. In contrast in 2018, it took less than \\n10 days for most of the references to get a DID. \\n \\n \\nFigure 11 - Distribution of the time spans between the creation of the references and the \\nintroduction of their DID for the years 2006, 2012, and 2018. The X-axis shows the time span in days \\nbetween reference creation and the introduction of their DIDs (only including the references created in \\neach of the years in the titles of the plot). The Y-axis shows the frequency for each of the time spans. See \\nAppendix D for distributions of all other years. \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n28 \\n \\nAs we already mentioned, DID References correspond to 7% of all the references in our 2019 \\ndataset - but the question is whether this would have been different at earlier points in time. Would \\nwe have collected the dataset in other years, the percentages would have been slightly different \\n(solid line, Figure 12). For example, if one would ask the same question at the beginning of 2007, \\nthere would have been around 6.6% of DID References. After a recovery in 2010, the number of \\nDID References has stabilized around 7% with a small increase in the last 4 years.  \\nHypothetically, one could collect the histories of references using only DIDs (see Appendix C). In \\nthat case, one would observe ~4.4% of DID References in 2007 (dashed line, Figure 12) where the \\ntrue number should have been at least 6.6% references; the alternative method would have missed \\n37.5% (~2.2% out of ~6.6%) of references that got their corresponding DID after the hypothetical \\ndata collection. These differences are discussed in more detail in Section 6.   \\n \\n \\nFigure 12 - Percentage of DID-Rs at different time points. The solid line defines the percentage of \\nDID-Rs(Y-axis) at certain time points (X-axis) obtained using our approach, the dashed line defines the \\npercentage of DID-Rs (Y-axis) at certain time points (X-axis) obtained using DIDs (with which we \\nmatched references between revisions; see Appendix C). \\n \\n5.2 The Editors of Wikipedia References \\nIn the context of altmetrics, the focus is often placed on which scholarly works receive mentions \\nor interactions from social media or other alternative platforms, while relatively little is known \\nabout who is behind these mentions and interactions. In social media platforms such as Wikipedia, \\nit is relevant to understand the actors who participate in the inclusion of scholarly publications as \\nthis has a direct impact on visibility. In contrast to traditional publications where the decision of \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n29 \\nwhich material should be cited is attributed to the authors of each publication, in a collaborative \\nenvironment, the decision is not straightforward but may have to be negotiated over different article \\nrevisions. In this subsection, we investigate (1) whether contributions come from registered editors, \\nbots, or non-registered sessions (IP addresses) (see Table 3), and (2) explore the behaviour of these \\nactors within Wikipedia. In particular, we are interested in whether those who edit Wikipedia \\nreferences differ from the overall Wikipedia editor community, and we inquire if there exist g sub-\\ncommunities of editors that specialize in different types of editing activities (creation, deletion, \\nmodification, reinsertion). \\n \\nTable 3 - Types of Wikipedia Editors. The first column lists the types of editors, the number of \\nreference-editing actors of each type and their actions that we encounter in our dataset. The second \\ncolumn elaborates on each. \\nTypes of Editors  \\nDescription \\nRegistered Editors  \\nActors:           1,910,667 \\nActions:     121,681,174 \\nThese correspond to individual users who have registered their profile \\non Wikipedia and edited at least one reference. \\nBots  \\nActors:                 1,172 \\nActions:      19,386,851 \\nBots were identified from bot lists of Wikimedia plus an additional list \\nof bots‚Äô names that we created. These sources were combined into a final \\nlist consisting of 10,262 unique account names (see Appendix F for the \\nsources). Since Wikipedia has strict rules and mechanisms to combat \\nspam and any automatic-like activities, all bots have to be registered \\naccounts. \\nNon-registered Editors \\nActors:                  N/A \\nActions:     23,459,838 \\n \\nEdits coming from non-registered IP addresses cannot be attributed to \\nspecific anonymous editors. Several persons can share the same IP \\naddress (e.g., university addresses or libraries), and one editor can \\nconnect via several IPs.  \\n \\n \\nWe found 1,910,66727 editors, 1,172 bots, and 23,459,838 edits by 4,286,160 IP addresses that \\nworked with Wikipedia references (Table 3). Figure 13 presents the distributions of actions per \\nuser type. Registered editors are responsible for the majority of actions, i.e., more than 122 million \\n(74% of all actions in our dataset). Registered editors focused on the creation of new references \\n(40% of their actions) and modification of existing ones (28.2% of their actions). Bots, in \\ncomparison, with a total of 19.4 million (13.7%) of all actions, were focused on modifications (that \\ncorresponds to 71% of their actions). Non-registered editors are responsible for only 14.3% of the \\nactions in our dataset. And although registered editors made most of the deletions (around 24.5 \\n \\n27 For comparison, English Wikipedia has a total of 35.7 million registered editors as of July 2019. \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n30 \\nmillion, left plot in Figure 13), non-registered editors appear to specialize on them (right plot): non-\\nregistered sessions have proportionally more deletions (53.3% of all their actions) than either \\nregistered editors (20.2%) or bots (5.6%), not unlikely due to large amounts of vandalism, \\nespecially blanket deletions of large chunks of text. The non-registered editors seem to comprise a \\nvery diverse and occasional set of editors, as 89.8% of IPs have less than 10 actions at all. This \\nfigure could be higher when we consider that some IPs might be associated with several editors \\n(e.g., school IPs); we assume that it is unlikely that one individual would use a large number of \\nIPs. Given the comparably low figures of actions for non-registered editors but mostly the \\ndifficulties of attributing actions to specific actors, we will exclude them from the rest of the \\nanalysis in this section. \\n \\n \\nFigure 13 - Distribution of actions performed by type of editors. The left plot shows the total actions \\n(Y-Axis) per type of account (X-Axis), and type of action (legend). The right plot shows the percentage \\n(Y-axis) of the type of actions (legend) within the account type (X-axis). The X-axis also presents the total \\nnumber of actions (n_actions) and editors or IP addresses (n_editors or n_IP) for each account type. \\n \\nMost registered editors have performed only a few actions on references in Wikipedia articles, \\nwhereas the top-contributors have contributed millions of action (Figure 14). We also studied the \\nnumber of different articles in which each editor has performed actions on references. We see a \\nsimilar trend as with the number of actions, the user at the top has edited references in 226,334 \\narticles. This seems to suggest that some editors are specifically focusing on reference editing \\nbeyond a specific topical area of interest.  \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n31 \\n \\nFigure 14 - Distribution of actions on Wikipedia references per registered editor. \\n \\nUsing a manually curated list of Wikipedia bots (10,262 unique bot account names, see Table 33), \\nwe found that 1,172 bots (0.1% of editors) have taken part in the edition of references. On a per \\nuser basis, bots performed more actions on references than registered users (Mann-Whitney U test, \\np<0.001; Figure 15).  \\n \\n \\nFigure 15 - CDF of actions performed by bots and registered editors. The X-axis shows the number of \\nactions (x), and the Y-axis the probability of a user having less than x actions. For example, 85% of bots \\nhave less than 1000 actions, whereas 85% of registered editors have less than 30 actions. \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n32 \\n \\nBots and registered editors display very different behaviour that is evident by directly looking at \\nthe types and quantity of actions (Figure 13 and Figure 15). Within the group of registered editors, \\nwe were interested in identifying subgroups of users who behave similarly (and distinct from other \\nsub-groups), as measured by the types of actions that they usually perform. \\n \\nWe use the K-means clustering algorithm to find such groups. Each registered editor is represented \\nby 4 features, one per type of action, that contain the distribution (in percentages) of actions of that \\neditor. We applied the algorithm on a sample of 10,000 random editors. To determine the optimal \\nnumber of clusters the following analyses were performed: (1) silhouette coefficients (Rousseeuw, \\n1987), and (2) Clustering tree algorithm (Zappia & Oshlack, 2018) with Sugiyama layout \\n(Sugiyama et al., 1981) for tree depiction. \\n \\nThe silhouette measures how close each editor in one cluster is to editors in the neighbouring \\nclusters. For example, classifying editors using 6 clusters (k=6) resulted in a mean silhouette score \\n0.69 (Figure 16): (1) editors from clusters 0, 1 and 2 (with mean silhouette score between 0.9 and \\n0.95) are well separated from other clusters as all the editors have a positive silhouette and most of \\nthem a very high one (>0.85), and (2) clusters 3, 4 and 5 have outliers because some of the editors \\nhave a negative silhouette (indicating that they are very close to other clusters.  \\n \\n(a) \\n(b) \\nFigure 16 - K-means clustering results (k=6) for clustering registered editors by types of actions. a) \\ncentroids of clusters, b) the silhouette coefficients of clusters \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n33 \\n \\nThe editors were classified using different values of k (from 1 to 11), and then a tree was created \\n(Figure 17). By traversing the tree from top to bottom, we can observe the composition of each \\ncluster at a given k with respect to the cluster of the previous level. For example, cluster 6 at k=7 \\ncontains elements of clusters 0, 2, and 5 from k=6. Therefore it is likely a non-well-defined cluster \\n(unstable). This suggests that stopping the division of clusters at k=6 would provide us with  a more \\nstable configuration of clusters. This observation is confirmed by the average silhouette scores.  \\n \\n \\nFigure 17 - ClusTree of reference editors. Each level of the tree (from top to bottom) corresponds to the \\nk used, and each node on that level corresponds to a cluster of that k. The edges (arrows) of the tree \\nrepresent the editors that ‚Äúmove‚Äù from one cluster in level k to another cluster in level k+1. The legend of \\nthe graph displays 4 scales, from top to bottom: (1) the transparency level of arrows (in_prop) shows the \\nproportion editors from one group that end up in another group. (2) The arrow colour (count) shows the \\nnumber of editors that ‚Äúmove‚Äù from one cluster to another, (3) the node size is proportional to the number \\nof editors in the clusters, and (4) the nodes colour intensity depicts the stability index (Kiselev et al. 2017). \\n \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n34 \\nWith the 6 clusters, we clearly observe different behavioural patterns that characterize the \\nregistered editors. Table 4 summarizes this characterization. \\n \\nTable 4. Classification of registered editors according to the type of activity. The first column presents \\nthe cluster id and the percentage of registered editors in parenthesis. The second column describes the \\ngroup of registered editors in terms of the actions they perform (Figure 16a). \\nCluster (%) \\nType of activity  \\n0 (39.56%) \\nOnly create new references \\n1 (20.55%) \\nOnly delete references \\n2 (10.3%) \\nModify references in 90% of the cases, create new ones in 6% cases \\n3 (11.17%) \\nMostly delete and create references (42% of cases for each action), modify in 10% \\ncases and do a few reinsertions \\n4 (4.06%) \\nMostly (70%) reinsert deleted references and do a few deletions, creations and \\nmodifications \\n5 (14.36%) \\nMostly create (55%) and modify (35%), and do a few deletions and reinsertions  \\n \\n \\nAdditionally, we wanted to know whether editors of references are different from the general \\nWikipedia editor community (Wikipedians). We have therefore compared the 10,000 most \\nproductive reference editors in our dataset with the most productive Wikipedians according to \\nWikimedia Foundation28. The ranking of the most productive Wikipedians is based on the total \\nnumber of revisions they have created, whereas we have used five different rankings of reference \\neditors based on the following criteria: (1) total count of actions, (2) modifications, (3) creations, \\n(4) deletions and (5) reinsertions. To see if highly productive reference editors correspond to highly \\nproductive Wikipedians, we utilized rank-biased overlap (RBO) by Webber et al. (2010). Apart \\nfrom accounting for the position in the rank of each editor, RBO properly deals with two \\ncharacteristics of our rank comparison task: indefiniteness (the rank range of 10,000 is arbitrary) \\nand top-weightedness (the variation among the top-active editors is more relevant than the one \\namong the rest) of the editors. Regarding the latter, the weight is determined by the research using \\nthe parameter p (ranged from 0 to 1); the lower it is, the more importance is placed on the top \\nresults.  \\n \\n \\n28 https://en.wikipedia.org/wiki/Wikipedia:List_of_Wikipedians_by_number_of_edits  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n35 \\nWe calculated different values of p (Figure 18) as a way to explore, and found that the RBO scores \\nare very low for values below 0.95 which place a lot of importance to the order; we only start seeing \\nhigher scores after values above 0.995 (RBO Scores ~0.016). The low scores cannot be attributed \\nto the lack of common editors between the rank lists (of 10 000 editors), as scores go relatively \\nhigh with extremely high values of p past 0.99995.  \\n \\n \\nFigure 18 - RBO scores between ranked lists of productive Wikipedians and the most active editors \\nof a certain action. \\n \\nTo clarify this effect, we also present a more intuitive index, Jaccard similarity (Table 5), which \\nsimply takes the magnitude of the intersection of elements (editors) between two lists divided by \\nthe total number of different editors. For example, 52% of editors are in both, the top-active \\nWikipedians and the top-active registered editors of all actions in Wikipedia references. Not only \\nis the latter group very different from the normal Wikipedians, but also the lack of overlap is more \\nnotable for groups specialized in certain types of actions. For example, the cluster 1 (Table 4) \\ndedicated to only deletions has a Jaccard index of 0.42 (for the top 10,000) and an even lower RBO \\nscore (0.363 with p of 0.9999995); even more distinctive is the group of editors doing reinsertions \\n(last row of Table 5), which well describes cluster 3, i.e. editors with mostly reinsertions.  \\n \\n \\n \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n36 \\nTable 5 - Similarity of action groups with productive Wikipedians. The first column (actions) displays \\nthe criteria used for our ranking. The second column represents the RBO scores for two values of the \\nparameter p of RBO. The rest of the columns show the Jaccard similarity of first top-x editors of both \\nrankings. \\nActions \\nRBO scores for p=[0.95, \\n0.9999995] \\nJaccard similarity of top \\n10 \\n100 \\n500 \\n1000 \\n5000 \\n10000 \\nTotal \\n[0.001, 0.635] \\n0.05 \\n0.25 \\n0.32 \\n0.38 \\n0.48 \\n0.52 \\nModifications \\n[0.002, 0.428] \\n0.11 \\n0.18 \\n0.25 \\n0.29 \\n0.42 \\n0.46 \\nCreations \\n[0.003, 0.449] \\n0.00 \\n0.12 \\n0.20 \\n0.26 \\n0.37 \\n0.41 \\nDeletions \\n[0.001, 0.363] \\n0.11 \\n0.17 \\n0.25 \\n0.31 \\n0.41 \\n0.42 \\nReinsertions \\n[0.001, 0.297] \\n0.05 \\n0.09 \\n0.16 \\n0.20 \\n0.34 \\n0.37 \\n \\n \\n6. Discussion  \\nWith this work we have presented a high-quality dataset of Wikipedia references (prepared by \\nutilizing a new method also introduced and evaluated here) and have used this dataset for initial \\ninvestigations into the edit histories of Wikipedia references and the role of different types of \\nWikipedia editors. The dataset can be used and repurposed by the scientific community in the \\nfuture.  \\n \\n6.1 Quality and applications of the dataset \\nTo the best of our knowledge, we have created the most comprehensive dataset of English \\nWikipedia references to date. To achieve this, we mine all these revisions, and did not limit the \\nprocess to only references that contain document identifiers (DIDs). Moreover, the dataset \\npreserves the traceability of each reference across the revisions, including its creations, \\nmodifications to its content, and full deletions and reinsertions of the text corresponding to the \\nreference. Our evaluation indicates that our method has an accuracy (based on F1 score) above \\n96%29 against a gold standard based on judgements by crowdworkers that we also contribute as \\npart of this work. At the same time, since we manually inspected the false positive cases of our \\nsystem, we are certain that the quality of the gold standard is very high and could be used to evaluate \\n \\n29 The reported F1 score corresponds to the method that was calculated using stratified samples, but since the \\ndistribution of similarity of the references is biased towards 0 (completely different) and 1 (completely similar) the \\naccuracy of the our dataset is higher \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n37 \\nfuture methods, as we did with the baseline method based on cosine similarity (Subsection 4.3). \\nResearchers could compare our method to other bibliometric matching algorithms, such as ones \\nprovided by  the  Centre  for  Science  and Technology Studies (CWTS), the Institute for Research \\nInformation and Quality Assurance (iFQ), or Web of Science (WoS)30 (Olensky et al., 2016). \\nHowever, we argue that those methods would not perform well as they depend on bibliographical \\nfields, which are often missing in Wikipedia references, not to mention the existence of abundant \\nerrors, especially in early revisions.   \\n \\nDespite our efforts to ensure the high quality of our dataset, we had to make certain compromises \\nthat lead to the following limitations: (1) While our method covers references indicated as inline \\ncitations via ref tags across their edit histories, we do not include any other forms of references that \\ncoexist in Wikipedia, e.g., parenthetical references31 or wikilinks to full references using \\ntemplates32 outside of ref tags. This decision was partly because the format of these references is \\nnot uniform and we cannot guarantee that their extraction would be accurate, but it is also in line \\nwith Wikipedia‚Äôs recommendation for how references should be added to articles. The latter also \\nimplies that we can assume some quality control for inline citations based on Wikipedia‚Äôs \\nstandards. Altmetric.com is now also only considering ref tags for accessing references and argue \\nthat other forms like sources mentioned as additional reading can be easily manipulated. (2) The \\ndataset was created based on the English Wikipedia as of June 2019. We do not know anything \\nabout edits that happened after this point in time. In general, this causes that the estimates of DID \\ncoverage are optimistic because we do not know if more DIDs were (or will be) added after that \\ndate. (3) We worked with a selection of common types of document identifiers that we summarized \\nas DIDs. For our analyses that include information about whether a reference included a DID at \\nsome point in time, this only pertains to the following types of identifiers: DOI, PubMedID, PMC, \\nISBN, ISSN, ArXiv ID. The list of identifiers is the same used by the Wikimedia Foundation \\nproject (Halfaker et al., 2019), as it might capture most academic citations. We use the presence of \\nidentifiers as a weak indicator for quality of the referenced publications, and not as a clear \\ncharacteristic to distinguish between different types of publications, e.g. scientific vs. non-\\nscientific. (4) For the clustering in Subsection 5.2, we did not take into consideration anonymous \\neditions to Wikipedia because we only have access to the IPs of the editors, and it would be wrong \\nto assume a one-to-one relation between an IP and an editor.  \\n \\n \\n30 http://apps.webofknowledge.com  \\n31 https://en.wikipedia.org/wiki/Wikipedia:Parenthetical_referencing  \\n32 For example, shortened footnote template (e.g., {{sfn}}), Harvard style templates (e.g., {{harvnb}}), or freehand \\nanchors (e.g., [[ #anchor_id]])  \\nhttps://en.wikipedia.org/wiki/Wikipedia:Citing_sources/Further_considerations#Wikilinks_to_full_references  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n38 \\nIn spite of the limitations, we hope that by providing our dataset to the research community, we \\ncontribute to future work in the area of altmetrics. For example, this can be used to compare \\ndifferent data collection approaches (e.g. used by altmetrics aggregators), or to evaluate previous \\ndatasets used in altmetrics publications against one that include the historical evolution of those \\nexistent references. It also opens the opportunity to investigate  additional research questions \\nrelated to coverage of specific types of publications over time, background information for \\nevolutions of highly-cited publications, topical distributions of references, and the dynamics of \\neditors that surround the references. We already start this work by providing insights into the \\nevolution of references based on edit types (actions), DID coverage, and editor characterization.  \\n \\n6.2 Evolution of Wikipedia references \\nFor our first research question (RQ1) we were investigating in more detail how Wikipedia \\nreferences evolve over time. Our data clearly highlights that references in Wikipedia have to be \\nviewed as a continuum that evolves in different dimensions. These insights should not be \\nunderestimated when working with Wikipedia data in the area of altmetrics, as they imply that the \\npoint of data collection will be crucial for any observations and that ways are needed to also account \\nfor vanishing or changing references: citation counts for publications based on Wikipedia data will \\nthus not only increase, but may as well decrease over time. Between 19.4% and 31.8% of total \\nreferences (between 10.8% and 25.7% of DID-Rs) were deleted every year (from 2007 to 2019) \\nand never reinserted again. These full deletions could cause erroneous assumptions drawn from \\nstatistics or even correlation analyses, and imply an instability of Wikipedia mentions as a \\nmeasurement instrument. In classical bibliometric instruments, comparable issues are negligible \\nsince changes of the reference list in papers are almost impossible, and retractions of paper \\n(together with their referencing lists) are very rare events (Shema et al., 2019). But for altmetrics, \\nthe phenomena of citation data volatility needs to be discussed in the community.  \\n \\nBesides the quantification of different types of actions as presented in Section 5, we were able to \\nobserve additional tendencies of reference evolution. Regarding RQ1, we find evidence that there \\nis a continuous effort to increase the quality of Wikipedia references, expressed in the following \\nways: First of all, the number of references added to Wikipedia is constantly rising. Another \\nevidence, regarding the effort to increase the quality of references, is the sharp increase of \\nmodifications in the last three years. \\n \\nInterestingly, we found that in some of the years there are peaks of modifications only targeting \\nDID-Rs. We believe that these peaks represent particular efforts to add missing DIDs as the peaks \\nare often followed by low values (troughs) - probably because there is a decrease in the amount of \\nmissing DID-R that can be detected by the normal editor community. This salient pattern shows \\nthat DID-Rs are treated very differently by the Wikipedia community.  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n39 \\n \\nAssuming that the presence of DIDs is an indicator of credibility, the proportion of new references \\nthat already include a DID (DBorn) has been consistently higher in the later years. The Wikipedia \\neditor community seems to also perceive the reference with DID (DID-R) as more credible, as we \\nconclude from the fact that DID-Rs are deleted with lower rates than all references.  \\n \\n6.3 The role of identifiers and potential effects for altmetrics \\nNot only because of the discovered differences in the ways Wikipedia editors treat DID-Rs, but \\nalso because of the general importance of document identifiers (e.g. for identifying and tracking \\npublications that were cited by Wikipedia articles), we placed an additional focus on the evolution \\nof document identifiers as elements within Wikipedia references (RQ2). We argue that changes \\nperformed on DID-Rs have a higher relevance for those altmetrics tools that rely on identifiers to \\ntrace citations.  \\n \\nFull deletions of references might be the most intuitive case of disruption for measuring impact \\nbased on Wikipedia references, and it affects references with or without references in the same \\nway. Modifications of references do not always have a direct implication for the altmetrics field: \\nThe only modifications that are relevant are those that change the reference in such a way that (1) \\nthey make it point to a new resource (i.e. equivalent to remove and add a reference), or (2) they \\nmake the reference either detectable (by adding a DID) or invisible (by removing the DID). The \\npractical implications of these two relevant scenarios for the generation of altmetric data depend \\non the altmetrics mining method. But we assume that currently some of the altmetrics aggregators \\ntake advantage of the presence of DIDs for identifying and counting references from Wikipedia. \\nTherefore, we have looked at the specific modifications that introduced a DID to an existing \\nreference in more detail.  \\n \\nSpecifically, we analyzed the DID-Lagged References that did not include a reference upon their \\nfirst introduction, but received it through later edits. Those references would potentially have been \\nignored during their initial lifespans before getting their DID. We were able to show that they \\ncorrespond to a considerable fraction of DID-Rs (10% corresponding to 12.1% actions before the \\nintroduction of the DID). We found important periods regarding the evolution of the DID-Lagged \\nReferences (Figure 12). Before 2010, a method that would have only relied on DIDs would have \\nmissed up to 37.5% (2007) of references for which we know that they should have had a DID (as \\nwe see that their DIDs were added by June 2019). The situation quickly improved between 2009 \\nand 2010 (11.3%), and then continued doing so until our data collection. As mentioned in the \\nlimitations, our estimates are optimistic as we cannot consider references where identifiers will be \\nadded after June 2019. Our findings show that mining methods that rely on DIDs are vulnerable to \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n40 \\ncoverage errors (Sen et al., 2019), that can misrepresent the importance of academic works in the \\naltmetrics community. \\n \\n6.4 Towards understanding who edits Wikipedia references \\nIn our last research question, we investigated the editor community that creates and maintains \\nWikipedia references (RQ3). Our findings can help to better understand who contributes to the \\nbody of references in Wikipedia. These contributors play a crucial role within Wikipedia as they \\njudge the relevance of references and shape what Wikipedia readers may consume, also influencing \\nwhether an article is perceived as relevant based on the presence of references. This influence goes \\nbeyond Wikipedia, e.g. in the area of altmetrics, where citations from Wikipedia are contributing \\nto different types of indicators that aim to measure publications‚Äô impact.  \\n \\nWe found that most of the references (87.6%) are created by registered editors, whereas bots were \\nonly responsible for 1.6% of new references. The concern that the presence of bots that add \\nreferences (Nielsen, 2008) was dominating the reference creation is not currently a generalized \\nissue. For comparison, this has been the case in Twitter, where Robinson-Garcia et al. (2017) found \\nthat bots (and thoughtless bot-like retweets of user accounts) were responsible for most of the \\nactivity containing scholarly articles. These findings support the idea that Wikipedia references are \\ncurated by humans and thus involve deliberate selection of sources and materials; an essential \\nfeature to justify their use in altmetrics research.  \\n \\nWhile the learning that Wikipedia references seem to be largely maintained by registered editors, \\nthis does not yet tell us whether they are representative of the core Wikipedia editor community or \\nwhether some editors seem to explicitly specialize on reference editing.  According to our similarity \\nmetrics (RBO and Jaccard), registered editors that participate in the evolution of Wikipedia \\nreferences are considerably different from the rest of the Wikipedia editors. Furthermore, we were \\nable to identify clusters of editors with very clear boundaries; two of these clusters are fully \\nspecialized in creations and deletions and together add up to ~61% of the editors. We also found \\nsingle editors that edited references in many different Wikipedia articles (e.g. one editor has edited \\nreferences in more than 226,000), and thus appeared to be highly specialized on reference editing \\nindependent of topical domains.  \\n \\nThese observations deserve additional attention in the future, as they should remind us of  our \\nillustrative, introductory example (Figure 1). Despite Wikipedia being a community effort, \\nindividual persons can have substantial influence over certain areas. A single editor has the \\npotential to largely affect the representation of a specific reference by adding (or removing) it \\nmultiple times. Non-registered editors could also be responsible for these types of exploitation of \\nWikipedia, at least we found that many deletions have been done by non-registered editors.  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n41 \\nAlternative explanations for this observation could be that (1) registered editors might log out from \\ntheir accounts to perform deletions of references (as a way of keeping themselves anonymous and \\nfree from possible repercussions of known peers), or (2) it might represent possible larger scale \\nvandalism (i.e. attempts to delete big sections of Wikipedia articles) that escaped the filters of \\nvandalic revisions included by the WikiWho service that we used. Further exploration is necessary \\nto disclose the magnitude of the potential issue, but we highlight the value of our dataset to analyse \\nthis issue. \\n \\n7. Conclusions \\nIn this paper, we have introduced an overview of the evolution of Wikipedia references, analyzed \\nthe historical coverage of reference-mining methods that are based on DIDs, and offered a \\ncharacterization of the Wikipedia editors. In the scope of our research questions, we conclude that \\nthe quality of Wikipedia references has been slowly but persistently increasing. Although our \\nfindings do highlight limitations, we believe that the historical registry of Wikipedia contains \\ninformation that can be leveraged to create more robust methods of mining and assigning \\nimportance to references in Wikipedia. We recommend that such methods use this record to reduce \\nmanipulations and biases that blur the visibility of references, to increase the overall coverage of \\nreferences (by looking at all revision), and to assign impact based on historical activity and the \\ncommunity of (e.g. reputable) editors that surround the references. \\n \\nThese recommendations only lighten up a different path for the creation of altmetrics based on \\nWikipedia, and there is certainly more to be done. The high-quality dataset that accompanies this \\npaper offers the opportunity to extend the research in this direction, for example by (1) analyzing \\nthe longevity and activity of references distinguishing between academic and non-academic (see \\nSingh et al (2020) for a classification approach), (2) exploring the dynamics of references according \\nto different knowledge fields, (3) further investigating the editors by mining (with natural language \\nprocessing techniques) their profile pages and extract demographics, (4) modelling the co-editors \\nnetwork to find important actors and communities, and (5) predicting which references are still \\nmissing a document identifiers since our dataset already provide this information for existing \\nreferences.  \\n \\nAcknowledgements \\nThis research was supported by the Deutsche Forschungsgemeinschaft, DFG, project number \\n314727790. We would like to thank all the *metrics project members and also:  Prof. Dr. Isabella \\nPeters and Prof. Dr. Claudia Wagner for their supervision and feedback, student assistants Tara \\nMorovatdar and Alexandra Stankevich for their help with the data curation and Kenan Erdogan for \\nthe insights about WikiWho service.  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n42 \\nReferences \\nBould, M. D., Hladkowicz, E. S., Pigford, A.-A. E., Ufholz, L.-A., Postonogova, T., Shin, E., & \\nBoet, S. (2014). References that anyone can edit: Review of Wikipedia citations in peer reviewed \\nhealth science literature. The BMJ, 348, g1585. https://doi.org/10.1136/bmj.g1585  \\nChen, C.-C., & Roth, C. (2012). {{Citation needed}}: The dynamics of referencing in Wikipedia. \\nProceedings of the Eighth Annual International Symposium on Wikis and Open Collaboration, 1‚Äì\\n4. https://doi.org/10.1145/2462932.2462943  \\nFl√∂ck, F., & Acosta, M. (2014). WikiWho: Precise and efficient attribution of authorship of \\nrevisioned content. Proceedings of the 23rd International Conference on World Wide Web, 843‚Äì\\n854. https://doi.org/10.1145/2566486.2568026  \\nFl√∂ck, F., Erdogan, K., & Acosta, M. (2017) \"TokTrack: A complete token provenance and change \\ntracking dataset for the English Wikipedia.\" Eleventh International AAAI Conference on Web and \\nSocial Media. http://arxiv.org/abs/1703.08244  \\nGrathwohl C. (2011). Wikipedia comes of age. Chronicle of Higher Education, 57. Retrieved from \\nhttps://www.chronicle.com/article/Wikipedia-Comes-of-Age/125899  \\nHalfaker, A., Mansurov, B., Redi, M., & Taraborelli, D. (2019, December 17). Citations with \\nidentifiers in Wikipedia. https://doi.org/10.6084/m9.figshare.1299540  \\nHaustein, S. (2016). Grand challenges in altmetrics: Heterogeneity, data quality and dependencies. \\nScientometrics, 108(1), 413‚Äì423. https://doi.org/10.1007/s11192-016-1910-9  \\nHolmberg, K. J. (2015). Altmetrics for Information Professionals: Past, Present and Future. \\nChandos Publishing. \\nHuvila, I. (2010). Where does the information come from? Information source use patterns in \\nWikipedia. Information Research, 15(3). http://www.informationr.net/ir/15-3/paper433.html  \\nImran, M., Akhtar, A., Said, A., Iqra, S., Hassan, S.-U., & Aljohani, N. R. (2018). Exploiting Social \\nNetworks of Twitter in Altmetrics Big Data. STI 2018 Conference Proceedings, 1339‚Äì1344. \\nhttp://hdl.handle.net/1887/65219  \\nKiselev, V. Y., Kirschner, K., Schaub, M. T., Andrews, T., Yiu, A., Chandra, T., Natarajan, K. N., \\nReik, W., Barahona, M., Green, A. R., & Hemberg, M. (2017). SC3: consensus clustering of single-\\ncell RNA-seq data. Nature Methods, 14(5), 483‚Äì486. https://doi.org/10.1038/nmeth.4236  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n43 \\nKittur, A., Suh, B., Pendleton, B. A., & Chi, E. H. (2007). He says, she says: Conflict and \\ncoordination in Wikipedia. Proceedings of the SIGCHI Conference on Human Factors in \\nComputing Systems, 453‚Äì462. https://doi.org/10.1145/1240624.1240698  \\nKousha, K., & Thelwall, M. (2017). Are wikipedia citations important evidence of the impact of \\nscholarly articles and books? Journal of the Association for Information Science and Technology, \\n68(3), 762‚Äì779. https://doi.org/10.1002/asi.23694 \\nLewoniewski, W., Wƒôcel, K., & Abramowicz, W. (2017). Analysis of References Across \\nWikipedia Languages. In R. Dama≈°eviƒçius & V. Mika≈°ytƒó (Eds.), Information and Software \\nTechnologies (pp. 561‚Äì573). Springer International Publishing. https://doi.org/10.1007/978-3-319-\\n67642-5_47  \\nLin, J., & Fenner, M. (2013). Altmetrics in Evolution: Defining and Redefining the Ontology of \\nArticle-Level \\nMetrics. \\nInformation \\nStandards \\nQuarterly, \\n25(2), \\n20. \\nhttps://doi.org/10.3789/isqv25no2.2013.04  \\nLin, J., & Fenner, M. (2014). An analysis of Wikipedia references across PLOS publications. \\nExpanding Impacts and Metrics, An ACM Web Science Conference 2014 Workshop, 23‚Äì26. \\nhttps://doi.org/10.6084/m9.figshare.1048991.v3 \\nMesgari, M., Okoli, C., Mehdi, M., Nielsen, F. √Ö., & Lanam√§ki, A. (2015). ‚ÄúThe sum of all human \\nknowledge‚Äù: A systematic review of scholarly research on the content of Wikipedia. Journal of the \\nAssociation \\nfor \\nInformation \\nScience \\nand \\nTechnology, \\n66(2), \\n219‚Äì245. \\nhttps://doi.org/10.1002/asi.23172  \\nMuriƒá, G., Abeliuk, A., Lerman, K., & Ferrara, E. (2019). Collaboration Drives Individual \\nProductivity. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW), 74:1‚Äì74:24. \\nhttps://doi.org/10.1145/3359176  \\nNielsen, F. √Ö. (2008). Clustering of scientific citations in Wikipedia. Wikimania 2008. \\nhttp://arxiv.org/abs/0805.1154  \\nNielsen, \\nF. \\n√Ö. \\n(2007). \\nScientific \\ncitations \\nin \\nWikipedia. \\nFirst \\nMonday, \\n12(8). \\nhttps://doi.org/10.5210/fm.v12i8.1997  \\nOkoli, C., Mehdi, M., Mesgari, M., Nielsen, F. √Ö., & Lanam√§ki, A. (2014). Wikipedia in the eyes \\nof its beholders: A systematic review of scholarly research on Wikipedia readers and readership. \\nJournal of the Association for Information Science and Technology, 65(12), 2381‚Äì2403. \\nhttps://doi.org/10.1002/asi.23162  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n44 \\nOlensky, M., Schmidt, M., & van Eck, N. J. (2016). Evaluation of the citation matching algorithms \\nof CWTS and iFQ in comparison to the Web of science. Journal of the Association for Information \\nScience and Technology, 67(10), 2550‚Äì2564. https://doi.org/10.1002/asi.23590  \\nOrtega, J. L. (2018). Reliability and accuracy of altmetric providers: A comparison among \\nAltmetric.com, PlumX and Crossref Event Data. Scientometrics, 116(3), 2123‚Äì2138. \\nhttps://doi.org/10.1007/s11192-018-2838-z  \\nPanciera, K., Halfaker, A., & Terveen, L. (2009). Wikipedians are born, not made: A study of \\npower editors on Wikipedia. Proceedings of the ACM 2009 International Conference on \\nSupporting Group Work - GROUP ‚Äô09, 51. https://doi.org/10.1145/1531674.1531682  \\nPiccardi, T., Redi, M., Colavizza, G., & West, R. (2020). Quantifying Engagement with Citations \\non Wikipedia. Proceedings of The Web Conference 2020 (WWW ‚Äô20), April 20‚Äì24, 2020. \\nhttps://doi.org/10.1145/3366423.3380300  \\nPooladian, A., Borrego, √Å. (2017). Methodological issues in measuring citations in Wikipedia: a \\ncase \\nstudy \\nin \\nLibrary \\nand \\nInformation \\nScience. \\nScientometrics \\n113, \\n455‚Äì464. \\nhttps://doi.org/10.1007/s11192-017-2474-z \\nPriem, J., Taraborelli, D., Groth, P., & Neylon, C. (2010, October 26). Altmetrics: A manifesto. \\nhttp://altmetrics.org/manifesto/  \\nRedi, \\nM. \\n(2018, \\nMay). \\nResearch: \\nCharacterizing \\nWikipedia \\nCitation \\nUsage. \\nhttps://meta.wikimedia.org/wiki/Research:Characterizing_Wikipedia_Citation_Usage  \\nRedi, M., & Taraborelli, D. (2018, July 16). Accessibility and topics of citations with identifiers in \\nWikipedia. https://doi.org/10.6084/m9.figshare.6819710.v1  \\nRobinson-Garcia, N., Costas, R., Isett, K., Melkers, J., & Hicks, D. (2017). The unbearable \\nemptiness \\nof \\ntweeting‚ÄîAbout \\njournal \\narticles. \\nPLOS \\nONE, \\n12(8), \\ne0183551. \\nhttps://doi.org/10.1371/journal.pone.0183551  \\nRousseeuw, P. J. (1987). Silhouettes: A graphical aid to the interpretation and validation of cluster \\nanalysis. \\nJournal \\nof \\nComputational \\nand \\nApplied \\nMathematics, \\n20, \\n53‚Äì65. \\nhttps://doi.org/10.1016/0377-0427(87)90125-7  \\nSen, I., Floeck, F., Weller, K., Weiss, B., & Wagner, C. (2019). A Total Error Framework for \\nDigital Traces of Humans. ArXiv:1907.08228 [Cs]. http://arxiv.org/abs/1907.08228  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n45 \\nShema, H., Hahn, O., Mazarakis, A., & Peters, I. (2019). Retractions from altmetric and \\nbibliometric \\nperspectives. \\nInformation \\n- \\nWissenschaft \\n& \\nPraxis, \\n70(2‚Äì3), \\n98‚Äì110. \\nhttps://doi.org/10.1515/iwp-2019-2006  \\nShuai, X., Jiang, Z., Liu, X., & Bollen, J. (2013). A comparative study of academic and Wikipedia \\nranking. 25. https://doi.org/10.1145/2467696.2467746  \\nSingh, H., West, R., & Colavizza, G. (2020). Wikipedia Citations: A comprehensive dataset of \\ncitations with identifiers extracted from English Wikipedia. ArXiv:2007.07022 [Cs]. \\nhttp://arxiv.org/abs/2007.07022  \\nSugimoto, C. R., Work, S., Larivi√®re, V., & Haustein, S. (2017). Scholarly use of social media and \\naltmetrics: A review of the literature. Journal of the Association for Information Science and \\nTechnology, 68(9), 2037‚Äì2062. https://doi.org/10.1002/asi.23833  \\nSugiyama, K., Tagawa, S., & Toda, M. (1981). Methods for Visual Understanding of Hierarchical \\nSystem Structures. IEEE Transactions on Systems, Man, and Cybernetics, 11(2), 109‚Äì125. \\nhttps://doi.org/10.1109/TSMC.1981.4308636  \\nTeplitskiy, M., Lu, G., & Duede, E. (2017). Amplifying the impact of open access: Wikipedia and \\nthe diffusion of science. Journal of the Association for Information Science and Technology, 68(9), \\n2116‚Äì2127. https://doi.org/10.1002/asi.23687  \\nThelwall, M. (2016). Does Astronomy research become too dated for the public? Wikipedia \\ncitations to Astronomy and Astrophysics journal articles 1996-2014. El Profesional de La \\nInformaci√≥n, 25(6), 893‚Äì900. \\nWebber, W., Moffat, A., & Zobel, J. (2010). A similarity measure for indefinite rankings. ACM \\nTransactions \\non \\nInformation \\nSystems \\n(TOIS), \\n28(4), \\n20:1‚Äì20:38. \\nhttps://doi.org/10.1145/1852102.1852106  \\nZagovora, O., Ulloa, R., Weller, K., & Fl√∂ck, F. (2020). Individual Edit Histories of All References \\nin the English Wikipedia [Data set]. Zenodo. http://doi.org/10.5281/zenodo.3964990  \\nZahedi, Z., & Costas, R. (2018). General discussion of data quality challenges in social media \\nmetrics: Extensive comparison of four major altmetric data aggregators. PLoS ONE, 13(5). \\nhttps://doi.org/10.1371/journal.pone.0197326  \\nZaldivar, M. S. S., B. Tomlinson, R. LaPlante, J. Ross, L. Irani, A. (2018). Responsible Research \\nwith Crowds: Pay Crowdworkers at Least Minimum Wage. Communications of the ACM, 61(3), \\n39‚Äì41. https://doi.org/10.1145/3180492  \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n46 \\nZappia, L., & Oshlack, A. (2018). Clustering trees: a visualization for evaluating clusterings at \\nmultiple resolutions. GigaScience, 7(7). https://doi.org/10.1093/gigascience/giy083  \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n47 \\nSupplementary materials \\n \\nAppendix A - Crowdworkers‚Äô Task on FigureEight \\n \\nTo validate our method, we created a gold standard dataset labelled by annotators. See Section 4 \\nfor more details. Figures A1-A4 depict instructions that were presented to crowdworkers on \\nFigureEight platform. Figure A5 shows an example question from the task.  \\n \\nWith this task, we pay crowdworkers at least German minimum wage (9.19 Euro per hour as of \\nApril 2019). We estimated that for one Page with 6 Questions one crowdworker would need \\napproximately 2 minutes. Thus, one worker can finish approximately 30 Pages in an hour and earn \\n10.50 US$ with the payment of 35¬¢ per Page.   \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n48 \\n \\nFigure A1 - Instruction for crowdworkers from FigureEight Task (Part 1) \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n49 \\n \\nFigure A2 - Instruction for crowdworkers from FigureEight Task (Part 2). \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n50 \\n \\nFigure A3 - Instruction for crowdworkers from FigureEight Task (Part 3). \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n51 \\n \\nFigure A4 - Instruction for crowdworkers from FigureEight Task (Part 4). \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n52 \\n \\nFigure A5 - Example question of the task on FigureEight. \\n \\n \\n \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n53 \\nAppendix B - Stratified sampling of references \\n \\n \\nFigure B1 - Stratified sampling of 1000 pairs of references. The figure displays the \\npseudocode used for the sampling of references used for the evaluation of our method.  \\n \\n \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n54 \\nAppendix C - Dataset of references with document identifiers. \\n \\nBesides our main dataset, that contains all changes of the references despite the presence or \\nabsence of document identifiers (DIDs), we also created the dataset where references between \\nrevisions were assumed to be Equivalent if they share the DID. The dataset included 7% of all the \\nreferences. In this dataset, a reference would be created only when a DID is introduced. The dataset \\ncontains the following actions: 3,943,984 creations, 10,425,320 modifications, 2,685,187 \\ndeletions, and 1,566,989 reinsertions.  \\n \\nAfter matching these reference sequences with our full dataset we found that 10% of DID-\\nReferences (Figure C1) did not have a DID in their first revision. Thus, references‚Äô creation date \\nand time were wrong in 10% cases. All the actions (creation, modifications, deletions, reinsertions) \\nthat happened with this 10% of DID-References before their DID introduction would some up to \\n12.1% of actions of DID-References.  \\n \\n \\n \\nFigure C1 - References change sequence examples. \\n \\nInitially, that dataset is supposed to solve the validation problem while document identifiers are \\nthe unique attributes of referenced objects. Performance indicators for actions are presented in \\nTable C1, F1-score exceeds 0.97, 0.88 and 0.92 for insertions, deletions, and modifications \\naccordingly. Nevertheless, one has to keep in mind that these indicators were calculated based on \\n7% of references. Thus, metrics might be biased towards references with (1) shorter strings, i.e. \\nthose with fewer details and attributes, (2) non-scientific objects. \\n \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n55 \\nTable C1 - Micro performance metrics for actions, where references with document identifiers \\nwere used as the gold standard dataset.  \\n \\n Reinsertion \\n(n=1,566,989) \\nDeletion  \\n(n=2,685,187) \\nModification \\n(n=10,425,320) \\nF1 \\n0.97 \\n0.88 \\n0.92 \\nPrecision \\n0.97 \\n0.88 \\n0.93 \\nRecall \\n0.97 \\n0.87 \\n0.91 \\n \\n \\n \\n \\n \\n \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n56 \\nAppendix D - DIDs introduction time span \\n \\n \\nFigure D1 -  DIDs introduction time span for 2005 - 2019 years of reference creation. X-\\naxis: the time difference between reference creation and DID introduction. \\n \\n \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n57 \\nAppendix E - The biggest peaks of added DIDs \\n  \\nTable E1 - Represents months with the highest number of modifications when DIDs were added \\nto existing references. The second column represents the number of references where DIDs were \\nadded. The third column represents which percentage of omitted DID-Rs remain omitted DID-Rs \\nthat month due to modifications that happened. For example, after modifications in May 2008, \\n81% of omitted DID-Rs left undiscoverable (meaning that they should have had DID but it had \\nnot been added yet).    \\nDate \\nCount \\nRemaining omitted \\nDID-Rs, % \\n2008-05 \\n10037  \\n81 \\n2008-06 \\n12091 \\n74 \\n2014-05 \\n18131  \\n83 \\n2014-08 \\n8101  \\n91 \\n2016-01 \\n8451  \\n89 \\n2018-02 \\n8662  \\n83 \\n2019-02  \\n12486  \\n44 \\n2019-03 \\n9797  \\n11 \\n \\n \\nZagovora et al., 2020 \\nThis work is licensed under a Creative Commons License: Attribution-NonCommercial-NoDerivatives 4.0 International \\n58 \\nAppendix F -  List of bot names sources \\n \\nNot \\nall \\nbot \\naccounts \\nhave \\na \\nbot \\nflag \\n(so \\nthey \\ncan \\nbe \\nlisted \\nin \\nthe \\nlist \\nhttps://en.wikipedia.org/wiki/Special:ListUsers/bot ). So we created the list that would include all \\nformer and current bot names. This final list is a union of all the following sources (as of \\n01.08.2019):  \\n1. https://stats.wikimedia.org/EN/TablesWikipediaEN.htm#bots  \\n2. https://en.wikipedia.org/wiki/Wikipedia:List_of_bots_by_number_of_edits  \\n3. https://en.wikipedia.org/wiki/Wikipedia:Bots/Status/inactive_bots_1  \\n4. https://en.wikipedia.org/wiki/Wikipedia:Bots/Status/inactive_bots_2  \\n5. https://en.wikipedia.org/wiki/Wikipedia:List_of_Wikipedians_by_number_of_edits/Unfl\\nagged_bots  \\n6. https://en.wikipedia.org/w/api.php?action=query&list=allusers&augroup=bot  \\n7. https://en.wikipedia.org/wiki/Category:Approved_Wikipedia_bot_requests_for_approval  \\n8. https://en.wikipedia.org/w/api.php?action=query&list=categorymembers&cmtitle=Categ\\nory:Approved_Wikipedia_bot_requests_for_approval&cmlimit=5000  \\n9. https://en.wikipedia.org/wiki/Wikipedia:Bots/Requests_for_approval/Approved \\n10. https://en.wikipedia.org/wiki/Wikipedia:Bots/Status  \\n11. https://en.wikipedia.org/w/index.php?title=Wikipedia:List_of_bots_by_number_of_edits/\\nlatest&oldid=185748540      \\n12. https://en.wikipedia.org/w/index.php?title=Wikipedia:List_of_bots_by_number_of_edits\\n&oldid=359820313  \\n13. https://en.wikipedia.org/w/index.php?title=Wikipedia:List_of_bots_by_number_of_edits\\n&oldid=271877315  \\n14. https://en.wikipedia.org/wiki/Wikipedia:Bots/Requests_for_approval  \\n15. https://en.wikipedia.org/wiki/Wikipedia:Bots/Requests_for_approval/Approved/Archive_\\n14  \\n16. https://en.wikipedia.org/wiki/Wikipedia:Bots/Requests_for_approval/Approved/Archive_\\n13  \\n17. https://en.wikipedia.org/wiki/Wikipedia:Bots/Requests_for_approval/Approved/Archive_\\n12  \\n18. https://en.wikipedia.org/wiki/Wikipedia:Bots/Requests_for_approval/Approved/Archive_\\n11  \\n \\n',\n",
       " 'final_answer': 'Data Science is an interdisciplinary field that leverages a variety of scientific procedures, algorithms, tools, and machine learning techniques to extract valuable insights and create meaningful patterns from data. It integrates knowledge and methods from several foundational areas, including statistics, computer science, machine learning, deep learning, data analysis, and data visualization [Internal Docs].\\n\\nHere\\'s a detailed explanation incorporating insights from the provided sources:\\n\\n1.  **Core Definition and Purpose (Internal Documents):**\\n    At its heart, Data Science is about making sense of vast amounts of information. It is defined as \"an interdisciplinary discipline that encompasses a variety of scientific procedures, algorithms, tools, and machine learning algorithms... and creates patterns from which valuable insights may be extracted.\" Its primary goal is to \"make connections and solve challenges in the future.\" A key distinction is made between data analytics and data science: while **data analytics** focuses on \"removing current meaning from past context,\" **data science** is predominantly \"concerned with predictive modeling.\" This forward-looking aspect underscores its role in informing future decisions and strategies. The rise in its popularity is attributed to the increasing importance of data, often referred to as \"the new oil,\" which, when properly analyzed and utilized, can be immensely beneficial.\\n\\n2.  **Scientific Foundation (Wikipedia: Scientific Method):**\\n    The \"scientific procedures\" aspect of Data Science is deeply rooted in the scientific method. This involves a systematic approach to knowledge acquisition, much like in traditional sciences. As described in Wikipedia, the scientific method encompasses:\\n    *   **Careful observation** coupled with rigorous skepticism to prevent cognitive biases.\\n    *   **Creating a testable hypothesis** through inductive reasoning.\\n    *   **Testing this hypothesis** through experiments and statistical analysis.\\n    *   **Adjusting or discarding the hypothesis** based on empirical results.\\n    Data science adopts this empirical framework to ensure that its insights are robust, evidence-based, and subject to verification and refinement, rather than mere speculation.\\n\\n3.  **Methodologies and Practical Application (ArXiv):**\\n    The ArXiv paper, while focusing on software development screencasts, exemplifies the practical application of Data Science methodologies. It showcases how data science techniques are used to \"find, understand, and extend development\" by analyzing various forms of data. Specifically, it highlights:\\n    *   **Similarity Analysis:** Techniques like **Cosine similarity** and **Jaccard coefficient** are employed to measure the similarity between video frames and between text transcripts (spoken words in screencasts and API documents). This is a common data science task for clustering, classification, and recommendation systems.\\n    *   **Natural Language Processing (NLP):** The paper discusses extracting \"popular development topics\" using \"LDA topics\" (Latent Dirichlet Allocation, a topic modeling technique) from screencast transcripts. This involves processing unstructured text data to identify underlying themes, a core area of NLP within Data Science.\\n    *   **Tool Use and Evaluation:** The use of Python toolkits like `pyLDAvis` and `NLTK` for text processing and visualization, along with evaluating performance metrics such as **precision, recall, and F1-score**, demonstrates the practical toolkit and rigorous evaluation standards characteristic of data science projects.\\n    In essence, the ArXiv paper illustrates Data Science in action: applying statistical and computational methods to complex data (video frames, audio transcripts) to extract meaningful information, discover patterns (recurring tasks, popular topics), and inform practical applications (extending screencasts with relevant API documentation).\\n\\n4.  **Advanced Applications and Future Focus (YouTube):**\\n    The YouTube transcript, although not defining Data Science directly, points to cutting-edge areas where data science plays a critical enabling role. It discusses \"agentic AI systems\" that rely on \"feedback loops, memory, and and tool use,\" emphasizing \"temporal reasoning and autonomous tasking.\" Data Science provides the algorithms, models, and data-driven insights necessary to build, optimize, and understand such sophisticated AI systems. Data scientists are crucial in preparing and analyzing the vast datasets needed to train Large Language Models (LLMs) and design the feedback mechanisms that allow agentic AI to learn and adapt autonomously. This shows Data Science\\'s contribution to the evolution of intelligent systems that can make connections and solve complex challenges in dynamic, real-world environments.\\n\\nIn summary, Data Science is a dynamic and evolving field that combines rigorous scientific inquiry with advanced computational and statistical techniques to transform raw data into actionable knowledge, predict future trends, and drive innovation across various domains, from improving software development resources to powering the next generation of AI systems. Its significance stems from treating data as a vital resource, \"the new oil,\" for addressing the complex challenges of the 21st century.'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
