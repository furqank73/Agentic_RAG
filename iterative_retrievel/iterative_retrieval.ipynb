{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00a9926",
   "metadata": {},
   "source": [
    "# What is Iterative Retrieval in Agentic RAG?\n",
    "Combined both Iterative And Self reflection\n",
    "\n",
    "Definition:\n",
    "Iterative Retrieval is a dynamic strategy where an AI agent doesn't settle for the first batch of retrieved documents. Instead, it evaluates the adequacy of the initial context, and if necessary, it:\n",
    "\n",
    "- Refines the query,\n",
    "- Retrieves again,\n",
    "- Repeats the process until itâ€™s confident enough to answer the original question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0731eb7d",
   "metadata": {},
   "source": [
    "# Why Use It?\n",
    "In standard RAG:\n",
    "\n",
    "- A single retrieval step is done, and the LLM uses it to answer.\n",
    "- If the documents were incomplete or irrelevant, the answer may fail.\n",
    "\n",
    "In Iterative RAG:\n",
    "\n",
    "- The agent reflects on the retrieved content and the answer it produced.\n",
    "- If itâ€™s unsure, it can refine its search (like a human researcher would)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "410f26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import TextLoader,WebBaseLoader\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders.youtube import YoutubeLoader\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langgraph.graph import StateGraph, END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401d6e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# --- IMPORTANT ---\n",
    "# Make sure you configure your API key *before* running\n",
    "genai.configure(api_key=\"your_api_key\") \n",
    "\n",
    "class GeminiLLM:\n",
    "    def __init__(self, model: str):\n",
    "        # 1. Initialize the GenerativeModel object\n",
    "        try:\n",
    "            self.model = genai.GenerativeModel(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing model: {e}\")\n",
    "            print(\"Please ensure your API key is set and the model name is correct.\")\n",
    "            raise\n",
    "\n",
    "    def invoke(self, prompt: str):\n",
    "        # 2. Use the .generate_content() method\n",
    "        response = self.model.generate_content(prompt)\n",
    "        \n",
    "        # 3. Access the text using response.text\n",
    "        \n",
    "        # Mimic LangChain's .content\n",
    "        class Result:\n",
    "            def __init__(self, content):\n",
    "                self.content = content\n",
    "        \n",
    "        # 4. Pass the text content to your Result class\n",
    "        return Result(content=response.text)\n",
    "\n",
    "# --- Usage ---\n",
    "\n",
    "# Note: \"gemini-2.5\" might not be a valid model name.\n",
    "# Using \"gemini-1.5-flash\" as a common, valid example.\n",
    "try:\n",
    "    # Try this model name if 'gemini-1.5-flash' still fails after upgrading\n",
    "    llm = GeminiLLM(model=\"gemini-2.5-flash\")\n",
    "    prompt = \"hello\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    print(answer)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during invocation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d693e04",
   "metadata": {},
   "source": [
    "# load the text then split then embed then store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6223c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = TextLoader(\"data_science_interview_cleaned.txt\",encoding=\"utf-8\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "doc_splits = text_splitter.split_documents(docs)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(doc_splits, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b731a5fd",
   "metadata": {},
   "source": [
    "### Define Agent State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4482a8d",
   "metadata": {},
   "source": [
    "| Field              | Type             | Default      | Meaning                                                                                                                                  |\n",
    "| ------------------ | ---------------- | ------------ | ---------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `question`         | `str`            | *(required)* | The **original user query**. This is what the user initially asked.                                                                      |\n",
    "| `refined_question` | `str`            | `\"\"`         | The **improved or reformulated question**, after the system refines it (e.g., clarifying or expanding the query).                        |\n",
    "| `retrieved_docs`   | `List[Document]` | `[]`         | A **list of documents** retrieved from external sources (e.g., vector store, database, web). Each `Document` contains text and metadata. |\n",
    "| `answer`           | `str`            | `\"\"`         | The **generated answer** after combining or synthesizing information from retrieved documents.                                           |\n",
    "| `verified`         | `bool`           | `False`      | Indicates whether the answer has been **verified** (for accuracy or completeness) in the iterative loop.                                 |\n",
    "| `attempts`         | `int`            | `0`          | Counts how many **iterations or attempts** have been made to refine and verify the answer.                                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93edf4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterativeRAGState(BaseModel): # pydantic model \n",
    "    question: str\n",
    "    refined_question:str=\"\"\n",
    "    retrieved_docs:List[Document]=[]\n",
    "    answer:str=\"\"\n",
    "    verified:bool=False\n",
    "    attempts:int=0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07674ce",
   "metadata": {},
   "source": [
    "### Retrieve Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Retrieve Node\n",
    "def retrieve_docs(state: IterativeRAGState) -> IterativeRAGState:\n",
    "    query = state.refined_question or state.question # Use refined_question if it exists (non-empty), otherwise use the original question.\n",
    "    docs = retriever.invoke(query)\n",
    "    return state.model_copy(update={\"retrieved_docs\": docs})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a62920c",
   "metadata": {},
   "source": [
    "### Reflect And Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a8ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reflect And Verify\n",
    "def generate_answer(state: IterativeRAGState) -> IterativeRAGState:\n",
    "    \n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in state.retrieved_docs) # Extracts their text (page_content) and joins them with two line breaks (\\n\\n) for readability.\n",
    "    prompt = f\"\"\"Use the following context to answer the question:\n",
    "\n",
    "Context:\n",
    "{context} # The result is one big context block â€” all the relevant text that the LLM will use to answer the question.\n",
    "\n",
    "Question:\n",
    "{state.question}\n",
    "\"\"\"\n",
    "    response = llm.invoke(prompt.strip()).content.strip() # .content extracts the text reply from that object. .strip() cleans up any leading/trailing whitespace.\n",
    "    return state.model_copy(update={\"answer\": response, \"attempts\": state.attempts + 1}) #Updates two fields:answer â†’ stores the new LLM-generated answer.attempts â†’ increments by 1 (tracks how many times the system tried to generate or refine an answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185fa33c",
   "metadata": {},
   "source": [
    "## Reflect on answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reflect on answer\n",
    "def reflect_on_answer(state: IterativeRAGState) -> IterativeRAGState:\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "Evaluate whether the answer below is factually sufficient and complete.\n",
    "\n",
    "Question: {state.question}\n",
    "Answer: {state.answer}\n",
    "\n",
    "Respond 'YES' if it's complete, otherwise 'NO' with feedback.\n",
    "\"\"\"\n",
    "    feedback = llm.invoke(prompt).content.lower()\n",
    "    verified = \"yes\" in feedback # Respond \"YES\" if the answer is complete, otherwise \"NO\" with feedback.\n",
    "    return state.model_copy(update={\"verified\": verified})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac0a792",
   "metadata": {},
   "source": [
    "## Refine query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c9a45cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Refine query\n",
    "def refine_query(state: IterativeRAGState) -> IterativeRAGState:\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "The answer appears incomplete. Suggest a better version of the query that would help retrieve more relevant context.\n",
    "\n",
    "Original Question: {state.question}\n",
    "Current Answer: {state.answer}\n",
    "\"\"\"\n",
    "    new_query = llm.invoke(prompt).content.strip()\n",
    "    return state.model_copy(update={\"refined_question\": new_query})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371031fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(IterativeRAGState)\n",
    "\n",
    "builder.add_node(\"retrieve\", retrieve_docs)\n",
    "builder.add_node(\"answer\", generate_answer)\n",
    "builder.add_node(\"reflect\", reflect_on_answer)\n",
    "builder.add_node(\"refine\", refine_query)\n",
    "\n",
    "builder.set_entry_point(\"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"answer\")\n",
    "builder.add_edge(\"answer\", \"reflect\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"s.verified (answer is verified as good)\tâ†’ Go to END (stop)\n",
    "  ðŸš« s.attempts >= 2 (too many tries)\tâ†’ Also go to END (stop loop)\n",
    "  âŒ Otherwise\tâ†’ Go to \"refine\" (refine query and try again)\"\"\"\n",
    "\n",
    "builder.add_conditional_edges(   \n",
    "    \"reflect\", \n",
    "    lambda s: END if s.verified or s.attempts >= 2 else \"refine\"\n",
    ")\n",
    "\n",
    "builder.add_edge(\"refine\", \"retrieve\")\n",
    "builder.add_edge(\"answer\", END)\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4d0d05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final answer: \n",
      " Based on the provided context, the question \"data alysis science better\" seems to be asking about the relationship or comparison between data analysis and data science.\n",
      "\n",
      "The text does not present data analysis as being \"better\" than data science, but rather depicts **data analysis as a critical component and set of activities within the broader field of data science.**\n",
      "\n",
      "Here's how the context positions them:\n",
      "\n",
      "*   **Data Science:** Described as a growing, multidisciplinary field focused on using cutting-edge technologies to tackle real-world challenges and derive value from data (the \"new oil\"). A skilled Data Scientist combines technical skills (parsing data, creating models) with business sense.\n",
      "*   **Data Analysis:** Mentioned as a specific task or process. It's part of \"data processing\" (\"Exploring, mining, and analyzing data are all tasks that data processing does\"). It involves using \"statistical and mathematical analysis\" to \"provide a summary of the data's insights.\" The context also highlights how \"data analysis instructs us\" on important decisions, such as marketing campaigns, emphasizing the value of accurate data insights derived from analysis.\n",
      "\n",
      "In summary, data science is the overarching discipline that leverages various techniques, including data analysis, to solve problems and create value.\n",
      "verified:\n",
      " True\n",
      "attempts:\n",
      " 1\n"
     ]
    }
   ],
   "source": [
    "query = \"data alysis science better\" # wrong spellings to test the agenticrag pipeline\n",
    "\n",
    "initial_state= IterativeRAGState(question=query)\n",
    "final=graph.invoke(initial_state)\n",
    "\n",
    "print(\"final answer: \\n\",final[\"answer\"])\n",
    "print(\"verified:\\n\",final[\"verified\"])\n",
    "print(\"attempts:\\n\",final[\"attempts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "deb3a45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final answer: \n",
      " The provided context does not state that one is \"better\" than the other. Instead, it highlights their differences in focus and methodology:\n",
      "\n",
      "*   **Data Science** is concerned with **predictive modeling**, altering data to derive useful insights, fostering innovation by answering future-oriented questions, and solving complex issues using a wide array of mathematical and scientific tools. It's a broad field requiring mastery of disciplines like math, statistics, programming, and machine learning.\n",
      "*   **Data Analytics** is concerned with **verifying current hypotheses and facts**, answering questions for efficient business decision-making, and removing current meaning from past context.\n",
      "\n",
      "Essentially, data science looks more towards the future and innovation through prediction, while data analytics focuses on understanding the present and past to inform current decisions. They serve different, yet complementary, purposes within an organization.\n",
      "verified:\n",
      " True\n",
      "attempts:\n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "query = \"data analysis data science which is better\"\n",
    "\n",
    "initial_state= IterativeRAGState(question=query)\n",
    "final=graph.invoke(initial_state)\n",
    "\n",
    "print(\"final answer: \\n\",final[\"answer\"])\n",
    "print(\"verified:\\n\",final[\"verified\"])\n",
    "print(\"attempts:\\n\",final[\"attempts\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
